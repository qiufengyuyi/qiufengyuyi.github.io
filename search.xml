<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[达观文本分类大赛17名思路总结]]></title>
    <url>%2F2019%2F03%2F29%2F%E8%BE%BE%E8%A7%82%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%A4%A7%E8%B5%9B17%E5%90%8D%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[最近solo了一个比赛，达观的文本分类比赛，大概搞了20来天，最后b榜成绩17名，比a榜下降了一名。虽然最后没有进前10，不过这次比赛相比之前参加kaggle的文本分类比赛，可以说是有了长足的进步。这里把这次比赛的思路总结一下，后续有时间的话会把代码开源，不过对于已成家的上班族来说，还是不要抱太大期望。。。 先简述一下赛题吧，任务很简单，是一个文本多分类比赛，这个比赛主要有两个难点： 1、文本长度分布不均，短的很短，长的很长，最长的貌似都上万个词了，最短的才几十个词。虽然训练数据只有十万多，但是对于缺少GPU、内存的同学来说，做这个比赛还是比较痛苦的，而且用深度学习时，对文本的截断和padding也是一个影响性能的因素。 2、文本是脱敏的，也就说不知道提供的数据里面的具体文本内容。这个也是现在国内文本比赛的趋势，脱敏的数据意味着很慢做一些传统NLP的数据预处理，比如停止词过滤、具体词的分析，同时也没办法使用已有的预训练好的词向量。 主办方提供了词分割的文本和字分割的文本，由于时间和硬件成本，我只使用了词分割的文本，其实如果将字文本也用上的话，进行模型融合，应该还可以让最后成绩提升，后面有时间再试了。 预处理首先是先看看所有分类标签的占比，有没有样本分类不均衡的现象。不知道是不是主办方处理过了，这次并没有严重的不均衡，一共19个类别，基本上每个类别样本量差距不大。另外，要想办法过滤一下停止词和标点符号。针对脱敏数据，我统计了每个词的idf值，将那些所有文章都出现的词筛选出来，大概筛出来10个左右的词。这些词即使不是标点符号，肯定也是一些对文章主题无用的词，可以过滤点。另外，经过分析，发现训练集样本中有文本重复的记录，应该是数据的噪声，有些重复样本的标签居然是不同的！这种噪声数据，我就全部过滤掉了，而标签相同的重复样本我就保留一条，其余的也删除。 （PS：删除过滤词之后，发现有的样本词全部删光了，这种文本估计也是噪声，重要的是测试集也有这种样本，总不能删除吧，只能使用-1这个UNK标记来填充，索性这种样本数量也不多，对总体的模型影响不是太大。） 词向量主要是自己从头开始训练了word2vec和glove两种词向量，维数是300维，最后两个词向量都使用了，做了concat拼接，当然这个维度直接使用会过拟合的，所以后面接了dropout层，使用0.5的概率。 模型本次比赛主要还是用的深度模型，最后会和xgboost结合，做stacking。 我用的方法其实也没有很复杂的地方，下面简单列出，同时会列出一些trick： 1、深度模型的话，我使用了双向LSTM+self-attention,这里要提一点，我的LSTM并没有使用传统的tf的rnncell，因为实在是太慢了！由于我使用的硬件平台关系，我的job最长时间不能超过12小时，否则我的当前进程会被强制下线。所以，一开始被这个训练成本搞得差点弃赛。 后来各种搜索高性能的rnn实现，发现tensorflow里面早有了cudnn的高性能实现，于是试了tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnnops的CudnnLSTM，效率瞬间飞起！本来我的一个epoch要10000多秒，最后降到了1500秒。但是要注意的是，这个CudnnLSTM不支持对padding的masking，即没有实现dynamic_rnn,所以对于一些padding的序列，它的梯度是会把padding的位置传递进去的。可能这也是我深度模型没有上太高分的一个原因吧。后面使用attention可以说对模型的提升程度也很高，其他的降维方法如max pooling，average pooling等都试过，但是还是attention效果最好，这个方法能够帮助模型对一些重要词赋予较高的权重，这个权重是通过模型动态学习到的。 到这里，我的分数提升路径大概是：bicudnnLSTM+maxpooling:0.777,bicudnnLSTM+attention:0.779，总算是赶上了baseline的水平了。 备注一下：基于性能和模型训练成本的平衡，最后我文本序列长度定为了2500。分析过文本长度的分布，大部分文本长度都在5000以下，但是1000-5000的文本数量也不少。 2、使用了伪标签。因为本次比赛的训练集为10万+，但是对于长本来说，这个量级还是比较少的，训练集不够，而且训练集和测试集的分布也可能会有一定的差异。因此借鉴了kaggle上的这个技巧，将当前预测最好效果的测试集+标签添加到训练集中，作为训练集进行训练，通过这种方式，我的线上成绩达到了0.78+。具体的使用方法就引用kaggle上的一个图例，将图中的评估标准换成比赛的F1_score就可以了。 3、使用了传统的NLP特征。这一步是我深度模型提升最大的。主要是对tfidf向量化后的term-doc矩阵，训练了lsi和lda两种特征，最后将两个特征的稀疏矩阵拼接起来放入我的深度模型中，大概如图所示： 使用这个模型，使用10cv+伪标签训练，最后模型的线上分数能达到0.79+ 4、我训练前期使用的是adam的优化方法，后期使用的momentum gradientDescent手调，由于时间因素，并没有调到很理想的性能，所以我尝试将10cv中，每个cv中所有训练数据（不包括测试集数据）在softmax层前得到的特征保存下来，并输入到xgboost进行训练： 这样，我一个参数组合的深度模型，最后能得到10个xgboost的训练结果，最后我调了两组参数，也就是得到了20个xgboost的训练结果，用于后面的stacking。 5、stacking是我之前比赛没有尝试过的，这次是第一次尝试，使用的是oof方法。我的基模型用的是步骤四得到的20个xgboost的结果，注意这里结果是19个分类的概率，即stacking后，应该是得到一个390个特征的数据。oof方法这个在网上很多都有，就不详细说明了，简单说就是用基模型验证集的结果拼成训练集，然后进行二次训练，而测试集的特征则是所有基模型的测试结果的平均。通过这种方法，我的线上分数升到了0.794+。（注意，stacking我并没有使用伪标签数据，一旦用了很容易过拟合，这个要注意。） 6、后续就是各种参数的调优和lgb、svm的尝试。基本上没有什么多说的。最后最好的A榜分数大概在0.7943，但是悲催的是我没有选择用于b榜评测的提交结果，幸好最近两次提交中有一次结果还可以，大概在0.7939左右。不过跟我的最好成绩也差不多，跟前面的差距也挺大的。。。 总得来说，这次比赛，将之前我参加比赛的未实现的想法都实现了，但是还是有一些遗憾的，比如没有使用字的文本，另外后来有想到将每个文本拆开来，单独训练出不同模型，最后再融合，这样可以解决文本过长的问题，不过时间不够就没有尝试。还有最后想做error analysis，发现对脱敏的数据基本无从下手，只是用了textrank提了关键词，发现一些互相分错混淆的类别，确实在关键词集上式相似的，估计是主题很接近的类别，这种类别要区分开来估计要更复杂的网络。希望在观看前10答辩时，能有一些启发。 最后，吐槽一下，一个人比赛实在是太累。。。由于工作关系，也不好与他人组队。后面选择比赛的时候，还是只能选一些数据相对较小的比，不然真的吃不消。不过做了那么多判别分类式的比赛，希望后面做一些生成式的比赛，比如机器阅读、自动摘要、QA等。最近有AIC和字节跳动的比赛，可能会选一个参加一下吧。欢迎有兴趣的同学私聊。]]></content>
      <categories>
        <category>比赛总结</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>text classificaton</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（基础算法复习）之最长回文子串的动态规划解法]]></title>
    <url>%2F2019%2F03%2F29%2F%EF%BC%88%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%EF%BC%89%E4%B9%8B%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%A7%A3%E6%B3%95%2F</url>
    <content type="text"><![CDATA[再开一个新坑，以后会不定期总结一下对于一些基本算法题的解法思路和实现。作为一个AI领域的从业者，不仅对于AI本身的技术要有掌握，对于计算机科学中的基本算法思想也要扎实的基础，这样才能做一个合格的算法工程，本人在这方面的能力还不够成熟，因此开这个坑，希望通过这种方式，真正去理解算法的本质。 本文要总结的题目是找最长回文子串，对应leetcode中的题目如下： Longest Palindromic Substring Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: Input: “babad” Output: “bab” Note: “aba” is also a valid answer. Example 2: Input: “cbbd” Output: “bb” 本文主要集中在如何一步一步使用动态规划的方法来解决这个问题并进行优化的过程，有其他更好的解决该问题的方法不在本文中讨论。 最简单思想首先，这个题目最简单的思想就是暴力遍历所有可能结果，假设当前字符串序列为 $s_1…s_n$ 具体做法是遍历所有可能长度的子串，判断其是不是回文子串，然后记录其长度，最后返回那个长度最长的子串即可。 暴力搜索的问题暴力搜索的时间效率肯定是最低的，那么低在哪里呢？我们以$s_1…s_n$为例，列举暴力搜索的前几个步骤，很明显就能看出来问题： 子串长度为1时，遍历所有单个字符 $s_1,s_2,…,s_n $，此时所有单个字符都是回文。 子串长度为2时，遍历所有二元字符子$s_1s_2,s_2s_3,s_3s_4,…,s_{n-1}s_n $，然后判断每个元素是否为回文，判断依据是 $s_i?=s_{i+1}$ 子串长度为3时，遍历所有三元字符串 $s_1s_2s_3,s_2s_3s_4,…,s_{i-1}s_is_{i+1},s_{n-2}s_{n-1}s_n$ ，这里判断每个元素是否为回文。 子串长度为4时，遍历所有四元字符串 $s_1s_2s_3s_4,s_2s_3s_4s_5,…,s_{i-2}s_{i-1}s_is_{i+1},s_{n-3}s_{n-2}s_{n-1}s_n $，然后判断每个元素是否为回文。 重复上述步骤，直到遍历到 $s_1…s_n​$ 这个长度为n的字符串为止。 进一步分析，其实步骤3时，其实会用到1的结果，因为每个元素中间项 $s_i $通过1已经知道是回文了，那么只要判断 $s_{i-1}==s_{i+1}$ 是否成立就行了。然而步骤3是重复处理了。 同样的，步骤4同样用到了步骤2的结果，对于 $s_{i-2}s_{i-1}s_is_{i+1}$ ，如果知道 $s_{i-1}s_i $是回文，那么只要分析 $s_{i-2}?=s_{i+1} $就可以了。因此步骤4也重复处理。 引入动态规划通过上面的分析可以知道，暴力搜索时，并不是每一个元素都需要去专门遍历并判断是否为回文，有些元素可以通过历史的结果来帮助判断一个元素是否为回文。同时我们也可以总结一些规律： 假设 $s_i,…,s_j $是我们要求解的最长的回文子串(其中j&gt;=i)，那么它必然符合两个条件： a. $s_i==s_j $ b. $s_{i+1},…,s_{j-1} （j&gt;=i+2）$必然也是一个回文子串，否则上述假设不成立。 基于上述规律，我们可以将这个问题进行拆分，分成最基本的优化目标问题单元：即只要到一个回文子串，然后以该子串为中心，左右两边的字符若相等，则找到一个更长的新的回文子串。即从一元字符开始，以该字符为中心向两边扩展的方式，逐渐找到更长的回文子串，有点像穿衣服一样，一层一层往上套。这个就是动态规划的思想，当然我只是用口语化的方式描述了一下，理论化的动态规划肯定更加严谨公式化，但是我就不在这里讨论了，有兴趣的可以自行查阅。 那么如何实现上述的思想呢？前面说了，我要知道$s_i,…,s_j$是否为回文，那么主要是需要知道$s_{i+1},…,s_{j-1}$是否为回文。这里可以将其视作一个填表问题。我们设计一个bool型的n*n的二维矩阵A，如果$s_i,…,s_j$是一个回文串，那么 $A_{ij} $位置就填上true，否则为false。填表的顺序则是从长度为1的字符串开始，逐级向长度更长的字符串填，间而言之就是一种周期性的斜向上方向，具体如图： 其实上图也说明了一个事情，就是我们并不需要把所有表格都填上，而是只需要填满上三角的表格就能得到最终结果。因此，在代码实现的时候，我们可以初始化一个上三角形状的二维数组，这样可以节省一半的空间。（代码实现的时候，我还是初始化了整矩阵，为了与后续优化区分）。 我们要做的，就是当遇到回文串时，不仅在上述表格相应位置填上值，而且需要记录当前子串的起始位置和长度，用于后续的返回结果。 代码实现1下面给出我写的初步动态规划解法，只列出关键逻辑，完整代码后续会放到github上，到时会贴出来。代码不是很优美，求大佬轻喷。 12int s_size = s.size();vector&lt;vector&lt;bool&gt; &gt; result(s_size, vector&lt;bool&gt; (s_size,false)); 初始化一个n*n的bool型矩阵，n为字符串长度。 12345678910111213for(int i=0;i&lt;s_size;i++)&#123; result[i][i] = true; if(i &lt; s_size-1) &#123; if (s[i]==s[i+1]) &#123; result[i][i+1] = true; cur_max = 2; start = i; &#125; &#125;&#125; 这里是先填充对角线，以及 $(i,i+1)$ 上斜线的位置，对角线对应一元字符，这个很好理解，都是回文，而上斜线的位置也很好处理，只要判断$s_i?=s_{i+1} $就可以了。同时要记录当前最长回文的起始位置start以及长度cur_max。 1234567891011121314151617for (int distance = 2;distance &lt; s_size;distance++) &#123; // cout&lt;&lt;distance; for (int i = 0;i+distance&lt;s_size;i++) &#123; result[i][i+distance] = (result[i+1][i+distance-1]&amp;&amp;(s[i]==s[i+distance])); if(result[i][i+distance]) &#123; if(distance+1&gt;cur_max) &#123; cur_max = distance+1; start = i; end = i+distance; &#125; &#125; &#125; &#125; 这里最外层的循环表示字符串的长度遍历，里层遍历则是对字符串起始位置的遍历。循环内部的逻辑在上面章节已经介绍过，即根据$s_{i+1},…,s_{j-1}$是否为回文以及$s_i?=s_j$来判断$s_i,…,s_j$是否为回文。同时记录回文子串的起始位置和长度。 进一步优化上述代码在leetcode运行后的时间和空间成本如下： 可以看到无论是时间成本还是空间成本都是很高的，优化的空间很大呀！ 回头看我们的上述解法过程，我们将一个矩阵中一个斜线上的所有位置填表称为一个epoch操作。每轮epoch其实都隐含得使用了前一个epoch的信息，更加具体的说是前一个epoch的某一个位置上的值。我们是否需要一整个二维矩阵表格来填某个epoch的信息呢？答案是不需要！具体原因看下图图示： 具体来说，每次填表格，其实只需要其右下角的表格的值就可以了，而初始的对角线和上斜线位置的判断也是很简单就能实现也不用记录，因此实际上我们不用一个二维矩阵，只需要几个临时变量记录右下角的表格值就可以了。 另外，我们需要变更填表的顺序，之前我们的顺序都是每个epoch斜上方向45度填。这里我们改换成如下的填表顺序： 其中箭头上标的数字代表循环的epoch轮数。即每次我向左斜向上方填表，填到边界时，跳到下一轮的其实位置，接着填。 优化代码实现代码的话其实注意几个点就行，一个是每一轮遍历的顺序是往左斜向上的，另一个就是每一轮要记录当前轮起始的位置，因为下一轮的起始位置是与前一轮的起始位置和当前轮数相关的，如图： 主要代码逻辑如下： 123bool pre_flag = false;int epoch_start_i = 0;int epoch_start_j = 0; 这里直接初始化一个临时变量存放右下角的值，然后初始化两个位置变量来存每一个epoch的起始位置。 对于对角线和上斜线的处理就不贴出来了，这里贴一下其他位置的填法： 12345678910111213141516171819202122232425262728293031//根据右下角的表格填当前表格pre_flag = (pre_flag &amp;&amp; s[i]==s[j]);//如果是回文，且长度最长，则记录起始位置if(pre_flag &amp;&amp; j-i+1&gt;cur_max)&#123; cur_max = j-i+1; start = i;&#125;//到边界位置，需要换到下一轮的起始位置if(i-1&lt;0 || j+1 &gt;=s_size)&#123; //本轮起始位置为对角线上 if (epoch_start_i == epoch_start_j) &#123; i =epoch_start_i; j = epoch_start_j+1; &#125;else &#123; //本轮起始位置在上斜线上 i =epoch_start_i+1; j = epoch_start_j; &#125; continue; &#125;else&#123; //向左斜上方向处理 i--; j++; continue;&#125; 最后，上述代码在leetcode上运行的花费如下： 可以看到，无论是时间复杂度，还是空间复杂度，都有了很大的优化。]]></content>
      <categories>
        <category>基础算法</category>
      </categories>
      <tags>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow模型线上部署]]></title>
    <url>%2F2019%2F03%2F24%2Ftensorflow%E6%A8%A1%E5%9E%8B%E7%BA%BF%E4%B8%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[这次写一个比较短的工程部署总结，说是tensorflow线上部署，并没有使用目前推荐的tensorflow serving。一则是该功能只是整个项目中的一个很小的功能点，如果单独为该功能部署tensorflow serving，成本和时间上会比较超标；二则由于公司内部网络环境限制，无法完整顺利获取tensorflow serving需要的依赖包（在没有网络环境下去装tensorflow serving环境的同学都懂的）；三则，项目并没有对模型在线学习的需求，只需要模型离线训练后，部署到线上即可。基于上述三点条件，选择了使用tensorflow python开发并训练模型，最后将模型进行序列化。然后在java端（项目主开发语言）调用模型进行在线预测。 前置条件tensorflow 1.3 所需的tensorflow java jar包：libtensorflow-1.4.1.jar 所需的tensorflow jni 库文件：libtensorflow_jni.so libtensorflow_framework.so 模型序列化一开始，由于未考虑模型如何移植到java平台的原因，模型训练和保存的时候，还是按照传统的方式，即通过tf.train.Saver()方法得到saver对象，然后调用saver.save方法保存模型，得到模型的checkpoint,meta,data,index四个文件。 上述文件中保存了模型的中间参数，模型当前训练的状态等信息，在某种意义上是动态的，只能通过tensorflow本身的python接口来调用该保存好的模型，并不能跨平台直接使用。其实模型的本质还是一堆权重数据和具体的权重计算流程，因此需要某种机制能固定住模型的权重数据和计算流程，即freeze模型。 1234567891011saver_predict = tf.train.import_meta_graph(model_config.ckpt_path + model_name)with tf.Session() as sess: if os.path.exists(model_config.ckpt_path): print("Restoring Variables from Checkpoint") saver_predict.restore(sess, tf.train.latest_checkpoint(model_config.ckpt_path)) else: print("Can't find the checkpoint.going to stop") exit() output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node] frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names) tf.train.write_graph(frozen_graph_def, model_config.ckpt_path, target_model_path, as_text=False) 上述代码的作用是对使用import_meta_graph读取训练好的模型,然后获取当前计算图中的所有节点，并将这些节点中的所有权重参数转化为常量，最后将这些常量保存到一个pb文件中，pb文件即protobuf，是 Google 推出的一种二进制数据交换格式。能够用于跨平台间的数据交换。上述代码实际使用时，会有问题，在java侧使用java的api调用模型时，会出现如下错误： 1Invalid argument: Input 0 of node XXXXXXXXXXX/BatchNorm/cond/AssignMovingAvg_1/Switch was passed float from XXXXXXXXXXX/BatchNorm/moving_variance:0 incompatible with expected float_ref. 经过一番google，找到了这个github的issue，貌似并没有得到很好的解决，推测问题原因是在freeze模型的权重参数时，对一些tensor的data type处理有问题。问题链接如下，有兴趣的同学可以看看，我最后换了另外一种方式来做。 []: https://github.com/davidsandberg/facenet/issues/161 最终，我使用的是另外一种方法，即在模型训练完时，直接对模型的权重参数序列化，保存为pb文件。代码如下： 1234# 将模型序列化保存`builder = tf.saved_model.builder.SavedModelBuilder(model_config.pb_path.format(epoch))builder.add_meta_graph_and_variables(sess, ["XXX"])builder.save() 调用SavedModelBuilder得到builder对象，然后将session中的所有图结构和权重参数存入到builder中，最后保存为pb文件。 Java调用模型在线预测这里主要使用的是tensorflow的Java版本api，说是Java版本，其实功能很局限，并没有模型训练方面的功能，所幸它有读取模型和数据，然后在线预测的功能，因此可以适用于当前场景。在部署时，有几点需要注意一下： 1.最基础的事情，当然是记得将tensorflow jar包加入到build-path中。 2.在linux上进行部署时，需要将两个so文件部署到项目工程的java build path中，因为我们的工程中的path包含/usr/lib/，因此可以将两个so文件放到这个路径下。两个so文件主要是用于tensorflow 上层的api与底层操作系统native library进行通信的接口。 下面主要介绍一下如何使用java来调用模型预测。 首先列出用到 两个主要的操作对象： 12SavedModelBundle tensorflowModelBundleSession tensorflowSession SavedModelBundle为java侧与pb文件接口的对象，能够读取pb文件。而Session对应的是tensorflow中的会话对象，java中，tensorflow的预测操作也是需要在一个会话中进行的。 12tensorflowModelBundle = SavedModelBundle.load(tensorflowModelPath, "XXXX");tensorflowSession = tensorflowModelBundle.session(); 然后就是构造输入模型的数据了。同python中的情况类似，java侧的模型接收的数据类型必须为tensor类型，因此需要将数据转化为tensor。因此要用到Tensor对象的create方法来生成Tensor，假设当前我们处理好后的数值型输入数据为一个二维数组testvec： 1Tensor input = Tensor.create(testvec); 当然如果有其他输入的话，也要都转化为Tensor，简单说就是原来模型中feed_dict中的所有输入都要转化为Tensor对象。 然后就是调用session，输入需要的数据，然后调用具体的计算节点输出结果： 1Tensor output = tensorflowSession.runner().feed("input",input).feed(XX,XX).fetch("computation node_name").run().get(0); 这行代码有几个注意点： 1、feed方法返回的仍然是Runner对象，这个机制使得我们可以链式调用feed方法，将所有需要喂入模型的数据装载。 2、Runner对象的fetch方法是定位到具体的计算图中的计算节点（tensor），这个与python中调用模型预测的方法类似，需要在构造计算图的时候，对模型输出样本预测概率（或者logits）的tensor指定名称。 3、最后的get()方法则是获取返回的结果，这里我输入了参数0，是因为run()方法默认返回的是一个List，因为有可能有的需求会调用多个计算节点，因此会返回多个tensor，但是此时我只需要得到一个tensor结果，因此获取List中的第一个元素。 上述方法返回的是一个Tensor对象，为了输出结果，需要将它转化为原始的二维数据格式： 1float[][] resultValues = (float[][]) out.copyTo(new float[1][1]); 调用Tensor的copyTo方法，能够将Tensor转化为指定数据格式的数组。之所以是二维数组，是因为我们的输入数据是二维数组，虽然一次一般是预测一个样本，但为了开发的普适性，统一处理为二维数组，数组存储的就是该样本的预测概率。 最后有一点需要注意一下，在使用完模型后，需要将所有创建的Tensor关闭，销毁资源，当然这个是开发的一个好习惯，能够避免资源的泄露和低效利用。 12out.close();input.close();]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记系列（一）SGM for multi-label classification]]></title>
    <url>%2F2019%2F03%2F24%2F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89SGM-for-multi-label-classification%2F</url>
    <content type="text"><![CDATA[阅读论文，不能读完就过去了，要思考和记录论文的创新点和有用的思想。这个系列就是以尽可能的简单，尽可能少的文字去将一些核心的东西提取出来，方便自己以后查阅。 论文题目：SGM:Sequence generation model for Multi-label classification 论文target：文本分类，多标签分类，即一个文本样本分类标签会有多个。 论文intuition：多标签分类问题中，不同标签之间往往存在着一定的相关关系，使得每个标签并不是独立的。而传统的多标签分类中，通常是在最后一层对每个分类标签使用sigmoid_crossentropy_loss来计算损失，然后将损失求和，同时计算每个分类标签的概率，忽略了标签之间的关联性。比如kaggle上的toxic-classification比赛中，不同类别toxic标签之间的关联性是很大的，比如性别歧视和种族歧视等。 论文主要关注点：1、使用seq2seq的方式来解决多标签分类问题。该网络架构本身就很新颖。通过这种方式，能够在一定程度上建模标签之间的关联信息。结构如下： 其实，网络结构主体与经典的带attention的seq2seq是非常相似的。encoder并没有变化，主要改进点在decoder上。由于并没有target文本，我们需要建模的是不同标签之间的关系，因此将所有标签类别作为一个序列，假设有n中标签，那么decoder就有n个时刻。其中，每个时刻t的解码器（假设是一个rnn），一共接受三个不同来源的信息：前一个时刻t-1的rnn hidden state；encoder的attention上下文信息（attention的输入为encoder的隐层信息和decoder当前时刻t的隐层信息）；分类标签的global embedding信息。 备注： 论文在此处有一个不太对应的地方，在上图中，画出的是每个时刻t，decoder的rnn接收的是当前时刻的 $c_t$ ，但是按照attention本身的使用方法，以及论文中的公式： s_t = LSTM(s_{t-1},[g(y_{t-1});c_{t-1}])应当是先将前一时刻的attention信息和前一时刻的标签global embedding先拼接，然后在输入到rnn中。这与图上的画法不符，只能认为是作者的图画错了。另外，在作者给出的源码中，并没有将c信息与global embedding拼接操作，而是直接将global embedding输入到了rnn中。这个在github上有人提出了issue，但至今没有回应，不知道是什么原因。 2、masked softmax以及sorted label。这个是本文另一个重要的点。decoder中，每个时刻计算得到$s_t$后，如何得到标签呢？需要进行两步操作，第一步原图中并没有详细画出，我就补了一条线，即会在后面接一个类似于全连接层的计算，其接收的输入为当前decoder的$s_t$以及当前时刻计算得到的attention context $c_t$ : o_t=W_of(W_ss_t+W_cc_t)其中 $W_o,W_s,W_c$均为需要学习的权重参数矩阵。而f表示一个non-linear函数。 第二步为masked softmax，即上图中的MS。 y_t=softmax(o_t+I_t)使用mask的方式就是对上一步的输出 $o_t$添加一个mask vector $I_t$ ，这个向量的值可是有一定说法的，如果当前时刻输出的标签结果在前面t-1时刻中有出现过，则赋予 $I_t$ 一个极小值，否则则赋予零向量。至于这个极小值，官方源码中是赋予了-9999999999。 为什么要使用mask vector？论文说明是为了防止重复预测相同的标签，有时候预测错了标签会因此导致错误一直传递下去，而模型也不希望预测出重复的标签。 至于这样mask有用吗？虽然论文中做了相关的w/o实验，但是由于实验数据本身标签的量级不大，所以个人感觉并不能看出这个做法的有效性。这个还有待后续的验证。 另外还有一个注意点就是sorted label，即训练时，将每个样本的标签按照频率来排序，即出现较多的标签排在序列的较早时刻。这么做的目的在于，我们训练的loss还是使用交叉熵loss，每个时刻上的标签y都需要给定，因此其顺序也要给定。将频率较高的标签放在开头来训练，能够让模型提早学习对数据整体而言最有用的信息，如果一个标签出现次数很少，模型在一开始花了大力气学习它的性价比就不高，容易拖模型的后腿。 3、global embedding。这个可以说是该论文中第二重要的一个点。它的最大作用就是能让每个时刻都能编码记录前面t-1时刻的标签预测进程。传统的做法是直接使用上一时刻预测出来的结果 $y_{t-1}$但是这个结果是通过取所有标签概率分布中最大的那个结果，不一定就是正确的，如果错误，那么就会使这个错误传递。因此论文借鉴了LSTM中的关于门机制的思想，通过门的机制，将前面t-1时刻的所有预测出来的y信息都编码整合。具体做法： a. 为每个标签初始化一个embedding$e_i$ b. 根据decoder计算得到的标签的概率分布，以概率为权重，计算所有标签的加权和$\bar{e}$ ： \bar{e}=\sum_{i}^{l}{P(y_i)e_i}c.构建门H: H=\sigma(W_1e+W_2\bar{e})可以看到这个门是由embedding和加权和embedding共同控制的。 再次吐槽一下，这里论文并没有sigmoid函数的计算，个人认为应当是漏掉了，因为门机制本身就是控制0-1之间的通路的作用，且源码中，是有这一步的操作的。 d.使用门，计算得到最后的global embedding： g(y_{t-1})=(1-H)\odot e+H\odot\bar{e}注意，这里是向量element-wise乘。 通过上述方式，根据加权和$\bar{e}$ 以及门的控制，能够学习所有标签label的综合信息，防止模型在错误的道路上”一往无前“。 后记虽然， 这篇论文写得有一些问题，但是其中有一些思想是可以借鉴的。 1、使用seq2seq来建模多标签之间的相关关系。 2、使用门机制的思想，来综合考虑所有标签的信息，防止对某个标签预测错误的短视。]]></content>
      <categories>
        <category>论文总结</category>
      </categories>
      <tags>
        <tag>multi-label text classification</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN循环神经网络—梯度爆炸和消失的简单解析]]></title>
    <url>%2F2019%2F03%2F23%2FRNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AE%80%E5%8D%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[通过cs224n关于循环神经网络一章的内容，以及自己对其他相关论文和博客的研读后，发现对循环神经网络中BPTT（即backpropagation through time）中，关于产生梯度爆炸和梯度消失的数学推导不是太理解，因此自己翻阅了线性代数中关于雅克比矩阵的相关内容，经过一番推导，终于搞懂其中的原委。故总结一下，若有错误，请各位大牛指正。 雅克比矩阵先百科一下什么是雅克比矩阵。引用维基百科上的定义，the matrix) of all first-order partial derivatives of a vector-valued function。首先，他是一个矩阵，其次矩阵的元素是一个一阶函数的偏导数，最后这个一阶函数的偏导数的对象是一个向量函数。 举个例子，大家应该就能明白这个定义意思： 假设有如下函数排列： 可以将$y_1…y_n$组成一个向量Y，其shape为$n_1$,同理$x_1…x_n$也组成一个向量X，其shape也为,可以$n_1$看到该平方函数就是一个对向量X的一个向量函数，而其雅克比矩阵可以写成如下形式：（公式比较难看，望包涵） 可以看到除了对角线上的所有矩阵元素值都是0，而对角线上的矩阵元素值就是对应y与x的一阶偏导数值。 RNN中的梯度消失和梯度爆炸问题RNN的BPTT，很多博客包括中都详细推过了，这里我不做重复说明，我只聚焦到其中的一点，即梯度消失和爆炸的问题探讨。 首先，我使用一下cs224n中，对该问题的所有notation。 该公式表示了RNN的隐藏层的计算过程，其中，h表示隐藏层的输出，W（hh）表示隐藏层到隐藏层的权重矩阵。（是个方阵）在梯度反向传播过程中，需要计算损失函数对W权重矩阵的梯度，可以得到如下公式： t表示时刻，E表示损失函数，该公式表示将所有时刻t的权重偏导数求和，得到这个序列的最终权重偏导，继续使用链式推导，可以得到： 其中，损失函数E对y的偏导数、y对ht的偏导数，以及h对W的偏导数都能很容易的求得，除了中间的ht对hk的偏导数。我们下面就重点关注这一项的求解。该式子最终可写成： 可得知，我们最终需要求的是某时刻的隐藏层值h对上一时刻隐藏层值h的偏导数在所有时刻上的和。 将这个式子再用链式法则拆开一下，令 得到： 其实，从上面的式子可以得知，等式右边的两个乘子项分别是两个不同的雅克比矩阵，因为$h_{t-1}$本身是一个向量函数，而$z_t$是对$h_{t-1}$的向量函数，因此分别将两个乘子式化为雅克比矩阵： 这个矩阵怎么求？回想一下我在上一章介绍雅克比矩阵时举的例子，其实可以类比到这个矩阵，其中$h_t$是一个$n_1$维的向量，而$z_t$也是一个$n_1$维的向量，写成函数排列可得： 简单的说，就是该雅克比矩阵中，某一行或某一列上，只有一个偏导数是非0的元素，即该矩阵除了对角线元素，其他元素都为0，因此可写成： 该矩阵是一个对角矩阵，用diag来表示，其中对角相当于一个向量，每个元素是h对z的偏导数，h与z的关系映射函数是我们网络中的激活函数，这里用 $f’(z_t)$表示h对z的偏导，则上述可写成：$diag(f’(z_t))$ 现在关注一下另一个雅克比矩阵： 同样写成函数排列形式： 看到这个函数排列式，是不是对上面的雅克比矩阵求解比较清晰了呢？是的，该雅克比矩阵的每个元素都是对应W权重矩阵的对应元素，即 最终，可得到： 可以得知，当序列长度越长，对一个序列进行BPTT，每次时刻多有对 的连乘操作，即相当于W权重矩阵的连乘，刚开始的几层可能影响不大，因为连乘较少，但是当越往前传播，W连乘的级数是急剧上升的，若W矩阵初始化不当，小于1或大于1，都将会使得梯度计算朝着接近于0或者无限大的趋势发展。而我们的激活函数sigmoid或tanh，在梯度很大或很小时的曲线都是很平滑的，很容易导致越往后训练，梯度几乎不变。因此产生了梯度消失和梯度爆炸问题。 reference： 1、cs224n公开课 2、Jacobian matrix and determinant]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
</search>
