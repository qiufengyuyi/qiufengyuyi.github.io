<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[tensorflow模型线上部署]]></title>
    <url>%2F2019%2F03%2F24%2Ftensorflow%E6%A8%A1%E5%9E%8B%E7%BA%BF%E4%B8%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[这次写一个比较短的工程部署总结，说是tensorflow线上部署，并没有使用目前推荐的tensorflow serving。一则是该功能只是整个项目中的一个很小的功能点，如果单独为该功能部署tensorflow serving，成本和时间上会比较超标；二则由于公司内部网络环境限制，无法完整顺利获取tensorflow serving需要的依赖包（在没有网络环境下去装tensorflow serving环境的同学都懂的）；三则，项目并没有对模型在线学习的需求，只需要模型离线训练后，部署到线上即可。基于上述三点条件，选择了使用tensorflow python开发并训练模型，最后将模型进行序列化。然后在java端（项目主开发语言）调用模型进行在线预测。 前置条件tensorflow 1.3 所需的tensorflow java jar包：libtensorflow-1.4.1.jar 所需的tensorflow jni 库文件：libtensorflow_jni.so libtensorflow_framework.so 模型序列化一开始，由于未考虑模型如何移植到java平台的原因，模型训练和保存的时候，还是按照传统的方式，即通过tf.train.Saver()方法得到saver对象，然后调用saver.save方法保存模型，得到模型的checkpoint,meta,data,index四个文件。 上述文件中保存了模型的中间参数，模型当前训练的状态等信息，在某种意义上是动态的，只能通过tensorflow本身的python接口来调用该保存好的模型，并不能跨平台直接使用。其实模型的本质还是一堆权重数据和具体的权重计算流程，因此需要某种机制能固定住模型的权重数据和计算流程，即freeze模型。 1234567891011saver_predict = tf.train.import_meta_graph(model_config.ckpt_path + model_name)with tf.Session() as sess: if os.path.exists(model_config.ckpt_path): print("Restoring Variables from Checkpoint") saver_predict.restore(sess, tf.train.latest_checkpoint(model_config.ckpt_path)) else: print("Can't find the checkpoint.going to stop") exit() output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node] frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names) tf.train.write_graph(frozen_graph_def, model_config.ckpt_path, target_model_path, as_text=False) 上述代码的作用是对使用import_meta_graph读取训练好的模型,然后获取当前计算图中的所有节点，并将这些节点中的所有权重参数转化为常量，最后将这些常量保存到一个pb文件中，pb文件即protobuf，是 Google 推出的一种二进制数据交换格式。能够用于跨平台间的数据交换。上述代码实际使用时，会有问题，在java侧使用java的api调用模型时，会出现如下错误： 1Invalid argument: Input 0 of node XXXXXXXXXXX/BatchNorm/cond/AssignMovingAvg_1/Switch was passed float from XXXXXXXXXXX/BatchNorm/moving_variance:0 incompatible with expected float_ref. 经过一番google，找到了这个github的issue，貌似并没有得到很好的解决，推测问题原因是在freeze模型的权重参数时，对一些tensor的data type处理有问题。问题链接如下，有兴趣的同学可以看看，我最后换了另外一种方式来做。 []: https://github.com/davidsandberg/facenet/issues/161 最终，我使用的是另外一种方法，即在模型训练完时，直接对模型的权重参数序列化，保存为pb文件。代码如下： 1234# 将模型序列化保存`builder = tf.saved_model.builder.SavedModelBuilder(model_config.pb_path.format(epoch))builder.add_meta_graph_and_variables(sess, ["XXX"])builder.save() 调用SavedModelBuilder得到builder对象，然后将session中的所有图结构和权重参数存入到builder中，最后保存为pb文件。 Java调用模型在线预测这里主要使用的是tensorflow的Java版本api，说是Java版本，其实功能很局限，并没有模型训练方面的功能，所幸它有读取模型和数据，然后在线预测的功能，因此可以适用于当前场景。在部署时，有几点需要注意一下： 1.最基础的事情，当然是记得将tensorflow jar包加入到build-path中。 2.在linux上进行部署时，需要将两个so文件部署到项目工程的java build path中，因为我们的工程中的path包含/usr/lib/，因此可以将两个so文件放到这个路径下。两个so文件主要是用于tensorflow 上层的api与底层操作系统native library进行通信的接口。 下面主要介绍一下如何使用java来调用模型预测。 首先列出用到 两个主要的操作对象： 12SavedModelBundle tensorflowModelBundleSession tensorflowSession SavedModelBundle为java侧与pb文件接口的对象，能够读取pb文件。而Session对应的是tensorflow中的会话对象，java中，tensorflow的预测操作也是需要在一个会话中进行的。 12tensorflowModelBundle = SavedModelBundle.load(tensorflowModelPath, "XXXX");tensorflowSession = tensorflowModelBundle.session(); 然后就是构造输入模型的数据了。同python中的情况类似，java侧的模型接收的数据类型必须为tensor类型，因此需要将数据转化为tensor。因此要用到Tensor对象的create方法来生成Tensor，假设当前我们处理好后的数值型输入数据为一个二维数组testvec： 1Tensor input = Tensor.create(testvec); 当然如果有其他输入的话，也要都转化为Tensor，简单说就是原来模型中feed_dict中的所有输入都要转化为Tensor对象。 然后就是调用session，输入需要的数据，然后调用具体的计算节点输出结果： 1Tensor output = tensorflowSession.runner().feed("input",input).feed(XX,XX).fetch("computation node_name").run().get(0); 这行代码有几个注意点： 1、feed方法返回的仍然是Runner对象，这个机制使得我们可以链式调用feed方法，将所有需要喂入模型的数据装载。 2、Runner对象的fetch方法是定位到具体的计算图中的计算节点（tensor），这个与python中调用模型预测的方法类似，需要在构造计算图的时候，对模型输出样本预测概率（或者logits）的tensor指定名称。 3、最后的get()方法则是获取返回的结果，这里我输入了参数0，是因为run()方法默认返回的是一个List，因为有可能有的需求会调用多个计算节点，因此会返回多个tensor，但是此时我只需要得到一个tensor结果，因此获取List中的第一个元素。 上述方法返回的是一个Tensor对象，为了输出结果，需要将它转化为原始的二维数据格式： 1float[][] resultValues = (float[][]) out.copyTo(new float[1][1]); 调用Tensor的copyTo方法，能够将Tensor转化为指定数据格式的数组。之所以是二维数组，是因为我们的输入数据是二维数组，虽然一次一般是预测一个样本，但为了开发的普适性，统一处理为二维数组，数组存储的就是该样本的预测概率。 最后有一点需要注意一下，在使用完模型后，需要将所有创建的Tensor关闭，销毁资源，当然这个是开发的一个好习惯，能够避免资源的泄露和低效利用。 12out.close();input.close();]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记系列（一）SGM for multi-label classification]]></title>
    <url>%2F2019%2F03%2F24%2F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89SGM-for-multi-label-classification%2F</url>
    <content type="text"><![CDATA[阅读论文，不能读完就过去了，要思考和记录论文的创新点和有用的思想。这个系列就是以尽可能的简单，尽可能少的文字去将一些核心的东西提取出来，方便自己以后查阅。 论文题目：SGM:Sequence generation model for Multi-label classification 论文target：文本分类，多标签分类，即一个文本样本分类标签会有多个。 论文intuition：多标签分类问题中，不同标签之间往往存在着一定的相关关系，使得每个标签并不是独立的。而传统的多标签分类中，通常是在最后一层对每个分类标签使用sigmoid_crossentropy_loss来计算损失，然后将损失求和，同时计算每个分类标签的概率，忽略了标签之间的关联性。比如kaggle上的toxic-classification比赛中，不同类别toxic标签之间的关联性是很大的，比如性别歧视和种族歧视等。 论文主要关注点：1、使用seq2seq的方式来解决多标签分类问题。该网络架构本身就很新颖。通过这种方式，能够在一定程度上建模标签之间的关联信息。结构如下： 其实，网络结构主体与经典的带attention的seq2seq是非常相似的。encoder并没有变化，主要改进点在decoder上。由于并没有target文本，我们需要建模的是不同标签之间的关系，因此将所有标签类别作为一个序列，假设有n中标签，那么decoder就有n个时刻。其中，每个时刻t的解码器（假设是一个rnn），一共接受三个不同来源的信息：前一个时刻t-1的rnn hidden state；encoder的attention上下文信息（attention的输入为encoder的隐层信息和decoder当前时刻t的隐层信息）；分类标签的global embedding信息。 备注： 论文在此处有一个不太对应的地方，在上图中，画出的是每个时刻t，decoder的rnn接收的是当前时刻的 $c_t$ ，但是按照attention本身的使用方法，以及论文中的公式： s_t = LSTM(s_{t-1},[g(y_{t-1});c_{t-1}])应当是先将前一时刻的attention信息和前一时刻的标签global embedding先拼接，然后在输入到rnn中。这与图上的画法不符，只能认为是作者的图画错了。另外，在作者给出的源码中，并没有将c信息与global embedding拼接操作，而是直接将global embedding输入到了rnn中。这个在github上有人提出了issue，但至今没有回应，不知道是什么原因。 2、masked softmax以及sorted label。这个是本文另一个重要的点。decoder中，每个时刻计算得到$s_t$后，如何得到标签呢？需要进行两步操作，第一步原图中并没有详细画出，我就补了一条线，即会在后面接一个类似于全连接层的计算，其接收的输入为当前decoder的$s_t$以及当前时刻计算得到的attention context $c_t$ : o_t=W_of(W_ss_t+W_cc_t)其中 $W_o,W_s,W_c$均为需要学习的权重参数矩阵。而f表示一个non-linear函数。 第二步为masked softmax，即上图中的MS。 y_t=softmax(o_t+I_t)使用mask的方式就是对上一步的输出 $o_t$添加一个mask vector $I_t$ ，这个向量的值可是有一定说法的，如果当前时刻输出的标签结果在前面t-1时刻中有出现过，则赋予 $I_t$ 一个极小值，否则则赋予零向量。至于这个极小值，官方源码中是赋予了-9999999999。 为什么要使用mask vector？论文说明是为了防止重复预测相同的标签，有时候预测错了标签会因此导致错误一直传递下去，而模型也不希望预测出重复的标签。 至于这样mask有用吗？虽然论文中做了相关的w/o实验，但是由于实验数据本身标签的量级不大，所以个人感觉并不能看出这个做法的有效性。这个还有待后续的验证。 另外还有一个注意点就是sorted label，即训练时，将每个样本的标签按照频率来排序，即出现较多的标签排在序列的较早时刻。这么做的目的在于，我们训练的loss还是使用交叉熵loss，每个时刻上的标签y都需要给定，因此其顺序也要给定。将频率较高的标签放在开头来训练，能够让模型提早学习对数据整体而言最有用的信息，如果一个标签出现次数很少，模型在一开始花了大力气学习它的性价比就不高，容易拖模型的后腿。 3、global embedding。这个可以说是该论文中第二重要的一个点。它的最大作用就是能让每个时刻都能编码记录前面t-1时刻的标签预测进程。传统的做法是直接使用上一时刻预测出来的结果 $y_{t-1}$但是这个结果是通过取所有标签概率分布中最大的那个结果，不一定就是正确的，如果错误，那么就会使这个错误传递。因此论文借鉴了LSTM中的关于门机制的思想，通过门的机制，将前面t-1时刻的所有预测出来的y信息都编码整合。具体做法： a. 为每个标签初始化一个embedding$e_i$ b. 根据decoder计算得到的标签的概率分布，以概率为权重，计算所有标签的加权和$\bar{e}$ ： \bar{e}=\sum_{i}^{l}{P(y_i)e_i}c.构建门H: H=\sigma(W_1e+W_2\bar{e})可以看到这个门是由embedding和加权和embedding共同控制的。 再次吐槽一下，这里论文并没有sigmoid函数的计算，个人认为应当是漏掉了，因为门机制本身就是控制0-1之间的通路的作用，且源码中，是有这一步的操作的。 d.使用门，计算得到最后的global embedding： g(y_{t-1})=(1-H)\odot e+H\odot\bar{e}注意，这里是向量element-wise乘。 通过上述方式，根据加权和$\bar{e}$ 以及门的控制，能够学习所有标签label的综合信息，防止模型在错误的道路上”一往无前“。 后记虽然， 这篇论文写得有一些问题，但是其中有一些思想是可以借鉴的。 1、使用seq2seq来建模多标签之间的相关关系。 2、使用门机制的思想，来综合考虑所有标签的信息，防止对某个标签预测错误的短视。]]></content>
      <categories>
        <category>论文总结</category>
      </categories>
      <tags>
        <tag>multi-label text classification</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN循环神经网络—梯度爆炸和消失的简单解析]]></title>
    <url>%2F2019%2F03%2F23%2FRNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AE%80%E5%8D%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[通过cs224n关于循环神经网络一章的内容，以及自己对其他相关论文和博客的研读后，发现对循环神经网络中BPTT（即backpropagation through time）中，关于产生梯度爆炸和梯度消失的数学推导不是太理解，因此自己翻阅了线性代数中关于雅克比矩阵的相关内容，经过一番推导，终于搞懂其中的原委。故总结一下，若有错误，请各位大牛指正。 雅克比矩阵先百科一下什么是雅克比矩阵。引用维基百科上的定义，the matrix) of all first-order partial derivatives of a vector-valued function。首先，他是一个矩阵，其次矩阵的元素是一个一阶函数的偏导数，最后这个一阶函数的偏导数的对象是一个向量函数。 举个例子，大家应该就能明白这个定义意思： 假设有如下函数排列： 可以将$y_1…y_n$组成一个向量Y，其shape为$n_1$,同理$x_1…x_n$也组成一个向量X，其shape也为,可以$n_1$看到该平方函数就是一个对向量X的一个向量函数，而其雅克比矩阵可以写成如下形式：（公式比较难看，望包涵） 可以看到除了对角线上的所有矩阵元素值都是0，而对角线上的矩阵元素值就是对应y与x的一阶偏导数值。 RNN中的梯度消失和梯度爆炸问题RNN的BPTT，很多博客包括中都详细推过了，这里我不做重复说明，我只聚焦到其中的一点，即梯度消失和爆炸的问题探讨。 首先，我使用一下cs224n中，对该问题的所有notation。 该公式表示了RNN的隐藏层的计算过程，其中，h表示隐藏层的输出，W（hh）表示隐藏层到隐藏层的权重矩阵。（是个方阵）在梯度反向传播过程中，需要计算损失函数对W权重矩阵的梯度，可以得到如下公式： t表示时刻，E表示损失函数，该公式表示将所有时刻t的权重偏导数求和，得到这个序列的最终权重偏导，继续使用链式推导，可以得到： 其中，损失函数E对y的偏导数、y对ht的偏导数，以及h对W的偏导数都能很容易的求得，除了中间的ht对hk的偏导数。我们下面就重点关注这一项的求解。该式子最终可写成： 可得知，我们最终需要求的是某时刻的隐藏层值h对上一时刻隐藏层值h的偏导数在所有时刻上的和。 将这个式子再用链式法则拆开一下，令 得到： 其实，从上面的式子可以得知，等式右边的两个乘子项分别是两个不同的雅克比矩阵，因为$h_{t-1}$本身是一个向量函数，而$z_t$是对$h_{t-1}$的向量函数，因此分别将两个乘子式化为雅克比矩阵： 这个矩阵怎么求？回想一下我在上一章介绍雅克比矩阵时举的例子，其实可以类比到这个矩阵，其中$h_t$是一个$n_1$维的向量，而$z_t$也是一个$n_1$维的向量，写成函数排列可得： 简单的说，就是该雅克比矩阵中，某一行或某一列上，只有一个偏导数是非0的元素，即该矩阵除了对角线元素，其他元素都为0，因此可写成： 该矩阵是一个对角矩阵，用diag来表示，其中对角相当于一个向量，每个元素是h对z的偏导数，h与z的关系映射函数是我们网络中的激活函数，这里用 $f’(z_t)$表示h对z的偏导，则上述可写成：$diag(f’(z_t))$ 现在关注一下另一个雅克比矩阵： 同样写成函数排列形式： 看到这个函数排列式，是不是对上面的雅克比矩阵求解比较清晰了呢？是的，该雅克比矩阵的每个元素都是对应W权重矩阵的对应元素，即 最终，可得到： 可以得知，当序列长度越长，对一个序列进行BPTT，每次时刻多有对 的连乘操作，即相当于W权重矩阵的连乘，刚开始的几层可能影响不大，因为连乘较少，但是当越往前传播，W连乘的级数是急剧上升的，若W矩阵初始化不当，小于1或大于1，都将会使得梯度计算朝着接近于0或者无限大的趋势发展。而我们的激活函数sigmoid或tanh，在梯度很大或很小时的曲线都是很平滑的，很容易导致越往后训练，梯度几乎不变。因此产生了梯度消失和梯度爆炸问题。 reference： 1、cs224n公开课 2、Jacobian matrix and determinant]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
</search>
