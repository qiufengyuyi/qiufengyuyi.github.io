<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[隐马尔科夫模型学习总结之二]]></title>
    <url>%2F2019%2F04%2F02%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8B%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[前章节主要是简单总结了马尔科夫模型的一阶情况，包括其原理、可以解决的问题。最后引出了隐马尔科夫模型。本章节就对隐马尔科夫模型进行一个总结。 隐马尔科夫模型与马尔科夫模型的区别在于”隐“字。”隐“代表了什么呢？在现实生活中，马尔科夫模型的研究对象往往是不可观测到的（上一章节已举出了例子），但同时存在很多对象我们是可以观测的，且受那些抽象对象的影响，此时我们可以引入隐变量表示无法观测的对象，我们可借由对可观测对象进行建模，来达到间接研究隐变量的目的，这就是隐马尔科夫模型能够做到的事情。 仍然以天气为例（虽然天气是可以观测到的，但为了简化描述，先用这个例子吧），我们不需要直接去对天气建模，而是将它作为隐变量，同时我们观测一些状态属性，如每天的温度、湿度、冷饮消耗量，这些属性的状态或多或少都依赖于天气的状态。 假设当前T时刻内的观测序列为 $\left\{ y_{1},y_{2},…y_{T} \right\} $,每个时刻的观测状态来源于观测状态列表 $V=\left\{ v_{1},v_{2},…,v_{k} \right\} $,其中 $y_{t}\in V$ 。隐变量状态序列仍用 $Q = \left\{ q_{1},q_{2},…,q_{T} \right\} $，每个时刻的隐变量状态来源于隐状态列表 $S=\left\{ s_{1},s_{2},…,s_{n} \right\}$ ，其中 $q_{t}\in S$ 。 在隐马尔科夫模型（以下简称HMM）的问题中，存在两个条件独立。 在给定t时刻的隐状态的条件下，t时刻的观测状态与其他时刻的隐状态是独立的。即当前时刻的观测状态只与当前时刻的隐状态有关 $P(y_{t}=v_{i}|q_{t}=s_{j}) =P(y_{t}=v_{i}|y_{1},…,y_{T},q_{1},…,q_{T}) $ 在给定t时刻的隐状态的条件下，前一个t-1时刻的隐状态与后一个t+1时刻的隐状态时条件独立的，即一阶马尔科夫性质。$P(q_{t}|q_{t-1},q_{t-2},…,q_{1})=P(q_{t}|q_{t-1})$ 用图表展示如下： 根据上述性质，我们仍然有状态转移矩阵A表示隐状态之间的转移概率情况。但是在该情况下，我们并不能通过直接观测来构造这个A矩阵。除此之外，我们新增了一个矩阵B用于表示隐状态生成观测状态的概率， $P(y_{t}=v_{i}|q_{t}=s_{j}) =P(y_{t}=v_{i}|y_{1},…,y_{T},q_{1},…,q_{T})=B_{ji}​$ ，该矩阵又叫发射矩阵。 另外，与马尔科夫模型类似，HMM的初始隐状态也是需要学习的参数之一，通常用 $\pi $表示，因此HMM的参数集可以用 $\lambda=\left\{ A,B,\pi \right\}$ 表示。 相对于马尔科夫模型解决的两大问题，HMM模型能够解决三种问题： 预测T时刻内的观测序列的概率 预测T时刻内最大概率出现的隐状态 学习HMM模型的参数 $\lambda ​$ 下面分别总结这三个问题的解法。 预测一个观测序列的概率这个的问题的前提条件是，已知模型的参数集 $\lambda$ ,给定一个观测状态序列，求出该序列的概率是多少。 该问题的解法比较简单，就是计算给定状态概率转移矩阵A、发射矩阵B，求给定观测序列与所有隐状态的联合概率之和： 上述公式可以用之前提到的HMM的两个性质来化简，举例： 上式等号右边的第一个乘式使用的是一阶马尔科夫性质化简，第二个乘式可以看出是与等号左边的式子的递归形式，因此该式子最终可化简为： 可以看到上式最后其实是状态转移矩阵和发射矩阵、初始状态的连乘形式，扩展到我们一开始的问题公式，该公式最后可化简为： 其中： 直接计算上式虽然可行，但是上式相当于要做 $K^{T} $个项的计算，当时间序列很长的时候，是难以计算的。因此引入了前向算法、后向算法、以及前向后向结合的算法来计算该式子。 前向算法 该算法用到了动态规划的思想。首先，我们引入一个中间变量 $\alpha_{i}(t) $,它表示时刻t的隐状态 $q_{t}=i $的情况下， $y_{1}…y_{t} $与 $q_{t}$ 的联合分布。 我们把 $\alpha_{i}(t) ​$的表达式进行展开和化简： t=1时刻的 $\alpha_{i}(t) $很好求，使用贝叶斯公式就可以求出来，而t=2时刻的如何求呢？我们之所以要引入这个临时变量 $\alpha_{i}(t) $，是为了能够让整个序列概率的求和公式进行简化，最好是以递归的形式进行简化，因此t=2时刻的临时变量必须要与t=1时刻的临时变量建立一个递归关系，故我们在计算 $\alpha_{j}(2)$ 时，引入变量 $q_{1}$ ，但是为了不改变原来概率的计算，通过积分（在离散情况下，相当于求和）把这个引入变量消去。最后化简为： 至此，可以建立递归的算法，最后： 因为 所以得到最后的计算式子。上面的算法，在每个时刻只需要计算k个项的求和，最后一共需k*T个项的求和，比传统方法快多了。 后向算法 后向算法与前向算法的思想是相同的，不同之处在于前向算法是从前往后迭代，而后向算法是从后往前迭代,同时建立的是基于t时刻的隐状态的条件概率，定义个临时变量 $\beta_{i}(t) $,它表示时刻t的隐状态 $q_{t}=i $的情况下， $y_{t+1}…y_{T}$ 基于 $q_{t}$ 的条件概率。如图： 类似前向算法，我们后向算法是从t=T,到t=1进行迭代，同时注意到当t=T时，由于 $q_{T} ​$已经确定，故 $\beta_{i}(T)=1​$ 。 计算T-1时，同样，为了建立与T时刻的迭代关系，故引入 $q_{T}$ ,同时为了不改变原概率计算，对其进行积分，消除该变量的影响。 由于 $y_{T}$ 只与 $q_{T}$ 有关，因此上式继续化简为： 一直计算前一个的递归，直到t=1 要得到最终的 $P(Y|Q)$ ,还需要通过贝叶斯公式，考虑t=1时刻的 $P(y_{1}|q_{1})*P(q_{1})$ ，因此最后得到的计算式为： 前向后向结合 除了以一个方向来进行迭代算法外，还可以从两头向中间的方式进行前向和后向的结合，就不多赘述了。原理与上述类似。 至此，HMM的第一个问题就可以解决了。在讲下一章训练HMM的模型参数问题前，我要讲三个推导，与前面所讲的前向后向算法有关，且后面的模型训练时会用到这三个推导。 推导1： 由于在给定 $q_{t}=i$ 的条件下，t时刻前面的Y和t时刻后面的Y应当是条件独立的，因此上式可化为： 可以看到与之前的前向和后向算法联系起来了。 推导2： 该式子可以得到，给定观测序列和模型参数的条件下，t时刻的隐状态的概率为对前向后向算法结合后的一种归一化。 推导3： 与推导2类似，但是添加了一个项，求 $P(q_{t}=i,q_{t+1}=j,Y|\lambda)$ ,即给定模型参数的条件下，求某两个时刻隐状态为某组合值与观测序列的联合概率： 将上式第一个乘子式根据t和t+1时刻进行划分，利用前向后向算法的思想进行化简： 由于 $y_{t+1} ​$是依赖于 $q_{t+1}​$ 的，因此需要将 $y_{t+1}​$ 单独提出来，如下： 上述推导在HMM模型训练时，计算状态转移矩阵和发射矩阵时都会使用到该推导。 预告：下一章讲解的HMM模型训练，会使用EM算法，迭代得去训练得到最后的模型参数集 $\lambda=\left\{ A,B,\pi \right\} $ reference: cs229 徐亦达老师的notes]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔科夫模型学习总结之一]]></title>
    <url>%2F2019%2F04%2F02%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8B%E4%B8%80%2F</url>
    <content type="text"><![CDATA[最近学习了徐亦达老师关于隐马尔科夫模型的教程和cs229中关于HMM的notes，把所有分散的知识点进行总结，加深自己学习的印象。（会分几个章节写，因为内容太多了。） 首先说明两个备注： 目前很多博客文章中对HMM模型的介绍以及举例其实有限定，即模型中的观测变量的概率分布是离散型的，比如扔色子、预测天气等比较形象的实例中，观测状态都是以某几个取值分类来决定的。除了上述离散型的概率分布外，观测变量的概率分布也有连续型的，比如预测股票的价格中，每天的股价其实是连续的值。在现实生活中，连续型的数据分布应当是更加常见的，但碍于篇幅，这次我讨论离散型的数据分布，当学习了卡尔曼滤波以及高斯分布等相关知识后，再补充连续型的数据分布的HMM。 本文只讨论一阶马尔科夫的情况，即当前状态只与前一个状态有关。还有更复杂的二阶以及其他变式HMM模型，本文不做讨论。 HMM模型适用于时间序列特征的数据，在语音识别、中文分词、词性标注等任务中都有比较好的表现，且其模型结构和原理相较于CRF以及其他复杂模型来说较容易理解，因此在深入自然语言处理研究前，掌握该模型的原理和推导是很有必要的，下面将详细对HMM的模型进行解析和推导。 马尔科夫模型首先总结一下马尔科夫模型的知识点。何为马尔科夫模型？对于时间序列数据来说，每个时刻t都有其独有的状态，本文只讨论该状态为离散的情况。即给定一组状态列表$S=\left\{ s_{1},s_{2},…s_{|n|} \right\}$,我们可以再时间序列上观察到这个状态列表中的元素的序列组合。以天气预报问题为例，假设当前 $S=\left\{ sun,cloud,rain \right\} ,n=3,$我们观察到近5天里的一个天气序列： \left\{ z_{1}=s_{sun},z_{2}=s_{cloud},z_{3}=s_{cloud},z_{4}=s_{sun},z_{5}=s_{cloud} \right\}在没有任何假设的情况下，每一天的天气状态可能与任何一个变量都有关系，这样我们就不能用统计语言去为现实问题建模了。为了简化问题，我们引入了一阶马尔科夫假设，即时刻t的状态概率只与前一时刻t-1的状态有关，即 P(z_{t}|z_{t-1},z_{t-2},...,z_{1})=P(z_{t}|z_{t-1})满足该假设性质的概率模型，可以称之为一阶马尔科夫模型，或者叫一阶马尔科夫链。若某时刻t的状态与前一个时刻以及前两个时刻的状态有关，则为二阶马尔科夫链。以此类推。 同时上述公式无法表示第0时刻的状态概率，因此还需要引入一个初始状态 $z_{0} $表示第0时刻的状态。在HMM模型中，初始状态通常用 $\pi$ 表示。 根据对样本数据的状态转移统计，我们可以得到一个状态转移矩阵 $A\in\Re^{n*n} $，表示某个状态转移到另一个状态的概率，其中矩阵元素 $A_{ij}$ 表示任意时刻t状态i转移到状态j的概率。 由该矩阵可知，该矩阵每一行的概率和均为1，即 $\sum_{j}^{n}{A_{ij}}=1$ （该结论在后面推导时会有用。） 马尔科夫模型的两个问题结合上述我们队马尔科夫模型的描述，其实我们可以利用上述性质，解决时间序列特征的数据集的两个问题。 已知模型的状态转移矩阵，和初始状态 $\pi $，假定在时刻t内，一个状态序列 $z_{set}$ 出现的概率是多少？ 如何估计我们模型中的参数，即状态转移矩阵和初始状态，来最大化一个观察序列$z_{set}$的最大似然估计？ 问题一：计算某个状态序列的概率 可以通过链式法则来计算 $P(z_{set}) $ 根据我们讲过的一阶马尔科夫假设，当前时刻t的状态只与前一个时刻的状态有关，即 $P(z_{t}|z_{t-1},z_{t-2},…,z_{1})=P(z_{t}|z_{t-1}) $可以将上式进行化简，得到： 即一个状态序列的概率就是将每个时刻对应前一时刻状态的状态概率进行连乘得到。 问题二：极大似然参数估计 本问题实质上是求出模型的状态转移矩阵，使得观察序列 $z_{set} $似然估计最大。一般我们会对似然估计函数求对数，方便求极值。 首先这个似然函数是有约束条件的 每个概率值都应该是非负的，即 $A_{ij}\geq0 ​$ 矩阵A的每一行的概率和应当是1（用到前面的结论。） $\sum_{j}^{n}{A_{ij}}=1$ 面对这种带不等式和等式约束条件的优化问题，自然我们想到用拉格朗日乘子法进行优化。 注意到由于我们对A矩阵的每一行均有一个等式约束条件，因此一共有n个约束条件，所以拉格朗日乘子也有n个，用 $\alpha_{i}​$ 表示。 对参数 $A_{ij} $求偏导数，令偏导数为0。求得参数的值： 上式中， $1\left\{ z_{t-1}=s_{i}\wedge z_{t}=s_{j} \right\} ​$表示t-1时刻的状态为$s_i​$与t时刻的状态为$s_j​$同时发生时对该项取1，其他项取0。 得到状态转移矩阵元素值为： 由于表达式中还有拉格朗日乘子，故还需要对乘子进行求偏导数，得到乘子的表达式。 将之前得到的 $A_{ij} $的表达式代入上式中，即可得到 $\alpha_{i}$ 的表达式。 求得乘子后，再把乘子反代入到 $A_{ij} $的表达式中，就可以得到状态转移矩阵的元素。 根据求得的矩阵元素表达式可以看出，其实我们是根据我们样本数据中状态之间的真实转移频率来近似模型的状态转移概率的，这个是属于频率学派的思想，即对事物本身进行真理的探讨，直接对事物本身进行建模。与之相对的还有贝叶斯学派，他主张事物的不确定性，用概率分布去接近事物的本质。关于这两个学派的区别，感觉还是有点模糊，准备在将来单独开篇学习总结一下。 通过上述的描述，可以看到马尔科夫模型对时间序列数据能够有一个强力的抽象，方便我们使用统计方法对现实世界的数据进行建模分析。但是该模型具有一定的局限性，首先区别于天气，某些事物的状态我们是不能直接观测到，比如股市的熊市和牛市状态、语音识别中具体的语言语素、文本序列中的词性等等。此时直接用马尔科夫模型就不适用了，因此推出了隐马尔科夫模型，通过引入隐变量来表示无法观测的状态变量，来解决该问题。 关于HMM模型的具体总结，见下一章节。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>Markov assumption</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拍拍贷文本相似度计算大赛总结]]></title>
    <url>%2F2019%2F04%2F01%2F%E6%8B%8D%E6%8B%8D%E8%B4%B7%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E5%A4%A7%E8%B5%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[18年6月下旬到7月上旬期间搞了个拍拍贷的NLP比赛，做的是文本相似度计算，本来是准备做蚂蚁金服的文本相似度比赛，不过它需要的开发环境比较麻烦，后来看拍拍贷的这个比赛只需要上传结果，而且词向量都帮你训练好了，于是就参加了拍拍贷的。最终成绩勉强进复赛，比不上排在前面的大佬。说是总结，其实没有什么很好的经验，毕竟名次很低，不过一来通过这次比赛，我熟悉了pytorch这个框架，趟了一些坑，二来也是对这个NLP任务有了一定的经验，所以在这边记录一下，如果对其他人有帮助，那就最好，如果我的总结有不对的地方，还请指正。 任务描述首先，本次NLP任务个人感觉应该是偏paragraph idenfication的，即两个文本表示虽然表示形式不同，但是表达内容是相同的。其实文本相似度计算是一个很大的分支，除了刚刚提到的之外，还有文本蕴含（Textual Entailment），即AB两种表达，通过A表达能否推断出B表达的真实性，举个例子就是：A：支付宝不能在超市付款，B：支付宝不能线下付款。这个B真实的例子，也有B是不真实或者B不确定是不是真实的情况。另外还有一种为relatedness，这个感觉与paragraph idenfication类似，但是更加广义，通常它是对两个文本评相似度的程度，并不是非黑即白。 关于文本相似度的相关知识我推荐一个文章，是我很佩服的一个大佬夕小瑶Elsa，感觉很有实际的借鉴意义：如何匹配两段文本的语义？ Traditional model这次比赛参加晚了，所以传统特征+xgboost虽然搞了几天，但是没有仔细搞，而且这个比赛的数据是脱敏的，所以一些文本预处理、语法、句法相关处理就没办法做了，所以主要是借鉴了以前参加quora的大神的一些特征提取方法和思路。大致分为以下几点： 1、从n-gram角度计算两个句子之间的各种相似度计算，包括jaccard，SimHash+海明距离等等，其中n-gram对词和字两个方向都进行计算，我取了1-4个gram。jaccard相似度计算很简单，公式如下： 其实就是计算两个句子之间的重复程度，A,B是两个集合，在这个任务中，可以将AB看做是两个用n-gram组成的词、词组集合。实际使用的时候，还会增加J分别被A和B长度归一化后的重复比率作为新特征。 而SimHash是一种将一段文本hash成固定长度，然后用于计算文本间相似度的方法，其原理简单说就是对文本中的每个特征，也就是词或者ngram词组，用传统hash方法分别生成一个唯一的签名如固定8位长度的二进制码10010000，其中1表示该特征的权重在该位上为正，0则为负，权重一般是某个词的次数，也可以是tfidf值，比如10010000对应词a权重为w，对应的表示为w-w-ww-w-w-w-w,然后将所有特征的权重按照签名后的表示按位相加，最后得到一个和向量x，该向量维数与hash的签名二进制码长度一致，假设某一维上的数值大于0，则置为1，否则置为0，最后得到的向量表示可以代表一段文本。如图： 其实很好理解，就是对一篇文档中的每个基本元素用一个唯一的随机hash字串表示，然后将这些字串通过字串的权重进行加权求和，就得到一段文本的表示。 之后使用海明距离计算，即两个码字的对应比特取值不同的比特数。这个是很简单的距离计算了。 2、一些NLP方面的传统特征，如wmd，欧式距离、余弦相似度、最长字串匹配、曼哈顿距离、皮尔逊相关系数，还有各种统计学上的相关性计算。这里其实重点在于如果将一个句子向量化。我用了三种方法：直接将句子的所有词向量做平均，得到句子的向量表示；通过计算每个词的tfidf值，然后将词向量做加权平均，权重为词对应的tfidf值；训练doc2vec,得到句子的向量表示。不过由于是脱敏数据，doc2vec是需要段落信息的，所以这个任务下训练doc2vec可能效果不明显。 3、fuzzywuzzy特征，这是一个开源的工具包，里面定义了对文本进行模糊匹配的一些api。 4、图特征，这类特征我研究时间最少，但是个人感觉应该是很重要的，因为句子的相似度计算，可以看成是两个句子作为图的节点，在领域上的接近程度。我主要是使用了pagerank算法，构建了句子的有向图，每个句子都有一个权重，表示这个句子的重要性程度，一般重要性程度越高，表示有越多的节点和它是相似的。最后将节点权重作为一个句子的一个特征。另外一个我没有做的就是，句子相似度的传递性，感觉这也是可以通过图来分析，即AB相似，BC相似，是否可以推出AC相似。而AB不相似，BC相似，是否可以推出AC不相似，等等。 5词共现矩阵。之前讲词向量和glove时，介绍过词共现矩阵也可以用于NLP任务。因此这边就使用了，主要是通过两个角度，一个是原始的词共现矩阵，每个句子都可以用词共现矩阵中的向量来表示；另外，由于原始词共现矩阵是一个稀疏矩阵，所以一般都会对该矩阵做SVD降维，然后再处理。 6、这个是我来不及做的，就是通过topic model，对文本建立主题模型或者隐语义分析。这个后续我会专门开辟一个坑来讲LDA。因为LDA个人感觉在传统方法中还是很好的，但是它的原理有点复杂，牵扯到各种统计分布、采样方法、EM等知识，所以对于不擅长统计概率学的我来说还是有点挑战性的。 综上，最后通过一些特征筛选方法，如皮尔逊相关系数、以及ensemble方法中的特征重要性计算，最后筛选出50多个特征，用xgboost训练，大概能得到0.25左右的分数。 这里吐槽一下，这次比赛基本没有用cv或者stacking，因为电脑硬件实在跟不上。跑一次简单的留一法，都要1-2个小时，唉，没有服务器搞比赛真是伤不起。 Deep model接下来就是时间花费最多的deep model。这次做比赛，我有一个目的就是借此机会学习pytorch这个框架。因为相对于tensorflow，感觉它更加方便，最重要的就是可以直接debug！！！而且python-style的风格实在是用的很爽。安利一波。不过，用的时候也有各种坑，好在它的社区好不错，上面回答问题的同学都很nice。 这次我主要是参考了COLING 2018的 Best Reproduction Paper，《Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering》，这篇文章可以说是将最近几年的一些state-of-art的model都罗列了出来，并做了实现和分析。因此我实现的模型就在这篇论文里了。 先简单介绍一下deep model在paraphrase identification中的大致分类。一般有两种实现方式。 1、一种是基于sentence encoding方式，即重点在于使用一些方法将句子进行编码，争取在编码后最大获取句子的各种信息，然后对两个待比较的句子编码做一些距离计算，如欧式距离、余弦相似度、曼哈顿距离，最后接几层全连接层，得到分类的输出，有的模型就是将两个句子编码直接接全连接层。这种方式虽然简单粗暴，但是效果还是不错的，我就是使用了这种方式，最好的有0.21的分数。这种方式的最大弊端在于训练速度实在是太慢了，因为它的参数量太多了，基本上一次训练都要2-3个小时。 2、另一种是基于交互的方式，即会对两个句子做一些词对齐的机制，比如用attention机制或者其他方法，然后将句子之间的交互信息拼接起来。这种方法的优势在于它的参数量实在是不多，对于我这种硬件不行的参赛者可以说是很友好了，基本上训练一次，只需要40-50分钟。效果也不错，大概在0.23左右。我觉得这种方式应该是潜力最大的，但是我后来实在是没时间调参了。希望有大佬能够给出一些参数建议，也让我体验一下单模型0.17+的快感。 基于交互的model 基于交互的model，重点在于如何将两个句子进行“交互”。我主要参考了DecAtt模型，即decomposable attention model，论文见：https://arxiv.org/abs/1606.01933 它的交互分为三个步骤，如图所示： attend：主要是对两个句子进行对齐处理。具体做法是先对两个句子分别使用一个全连接层+relu做线性和非线性变换，然后将变换后的两个向量做矩阵乘法（将一个向量转置后乘），得到一个n*m的矩阵（其中n和m分别为两个句子的长度）。这个矩阵就是一个未归一化的attention权重，然后分别对两个句子做权重的加权平均得到两个对齐后的句子表示。假设原句对为，得到的对齐表示为 compare：将上面得到的两个对齐句子a,b表示分别与bi,ai做比较，具体比较方式为接一个全连接层，最后得到句子的比较表示va,vb aggerate:对va和vb分别沿这时间维度求和，然后将两个句子表示做拼接，送到后面的分类层。 可以看到这个模型使用了attention机制，在参数量很小的情况下，就能获取两个句子之间的交互关系信息。可以说是把attention机制的优势发挥出来了。 具体实现时，有一个坑要重点说明，就是pytorch对padding后在句子的处理比较麻烦。即在NLP任务中，不同样本数据文本长度会不一样，而做minibatch训练时，需要一次送入多个样本数据，因此需要将一个batch中的所有文本先对齐到同一长度，因此会先做pad操作，即补0.但是在计算attention时，尤其是做attention加权平均时，补0的那部分信息不能作为平均的分母，即归一化分母应当只算原本句子的那些词。经过多方查阅，我最后实现了以下的方法，来解决这个问题： 1234567891011121314151617181920def attention_softmax3d(self, raw_attentions,lengths): """ :param raw_attentions: shape:batch_size,maxlen1,maxlen2 :param lengths:句子原始长度 len2 :return: """ # 待加权句子pad后的长度 max_len = raw_attentions.size(-1) # 创建一个1*maxlen的数列tensor，范围从0到maxlen-1 idxes = torch.arange(0,max_len,out=torch.LongTensor(max_len)).unsqueeze(0).cuda() # 创建mask，batch_size,1,maxlen,其中句子原始长度len以内的地方都置为1，其他位置都置为0 mask = Variable((idxes&lt;lengths.unsqueeze(1)).float()).unsqueeze(1) max_logits,_ = raw_attentions.max(dim=2, keepdim=True) # 算归一化权重时，乘以mask，使得pad为0的位置都不会被计算在内 masked_logits_exp = torch.exp(raw_attentions-max_logits) * mask logtis_sum = torch.sum(masked_logits_exp,dim=-1,keepdim=True) # out = nn.functional.softmax(raw_attentions, dim=1) out = masked_logits_exp/logtis_sum return out 尝试改进点：主要是对几个全连接层的单元数调参，还有就是在句子input representation层上，在embedding_lookup之后，接一个双向rnn层（BIlstm）。添加rnn层，虽然在一定程度上提高了local分数，但是在线上分数却降低了，推测应该是过拟合了，由于加了lstm之后训练速度明显变慢，时间不够，所以没有继续调下去。 基于encoding的model 基于encoding的model，核心在于如何使用一些方法捕捉每个句子的核心信息特征，然后将这些特征加到句子表示中，最后通过一些距离、相似度计算，来得到分类依据。 一开始，我参考的模型是SSE，即Shortcut-stacked BiLSTM，原始论文：[1708.02312v2] Shortcut-Stacked Sentence Encoders for Multi-Domain Inference 它的结构如下： 核心主要有三个： 1、使用了三层BiLstm，使用了残差网络的思想，会将之前原始的句子表示以及前面lstm层的输出做拼接，作为下一层lstm的输入。这样做，能够最大化三层lstm的学习能力，防止网络在层数增加到一定程度时，无法提升性能。 2、对lstm的最终输出做了over time max pooling，我试了average pooling，效果还是没有max pooling好。 3、最后得到的两个句子的encoding输出v1,v2，然后计算v1、v2的各种距离相似度，个人感觉|v1-v2|和v1*v2很有用，首先他们对于两个句子的比较与被比较关系是相同的，即AB和BA的结果是一样的，这也符合常理，因为在本任务中，AB相似度本身应该是没有方向性的。后面陆续试了余弦相似度，欧式距离，效果均没有明显提升。 具体实现情况是，我的电脑根本跑不动三层lstm！！！。一个小时连一个epoch都跑不完。最后无奈只能减小lstm的单元数，原本是[512,1024,2048],最后被我改到[64,128,256]，才勉强可以在我的耐心极限内跑完，当然效果不好，才0.24左右。 后来我看了Infersent这篇model，感觉一层lstm应该也会有很好的编码效果，所以试了一下，直接把一层的lstm单元数设为2048，最后效果居然达到了0.21。 嗯，后来经过分析，发现在机器性能有限的情况下，与其堆叠rnn层数，不如调整rnn单元数和最后全连接层的层数和单元数的比例来得实际。事实上，当lstm的输出维度达到一定的程度时，如果后面的全连接层层数或单元数达不到相应的量级，就无法完全适配前面lstm编码得到的信息。当我的lstm单元数为512时，后面全连接层设为三层，单元数设为3000-4000时，模型效果能达到0.21。当然这是我总结出来的经验，不知道有没有理论依据，如果有大佬了解相关理论，欢迎交流，我对理论知识的探讨是很渴求的，本身对这方面就一直在学习。 当然，关于pytorch实现还有一个坑需要说明一下，也是关于句子padding的问题，tensorflow对于rnn的padding处理有专门的api:dynamic_rnn。但是pytorch貌似没有，但是它有专门两个api处理padding的句子：pack_padded_sequence，pad_packed_sequence 一般rnn处理变长的经过padding的句子序列过程如下： 1、使用pack_padded_sequence将句子序列变为PackedSequence对象，这个对象封装有方法，能够让rnn对象识别哪些是pad的位置。但是，它需要输入的一个batch中的句子序列是按照长度降序排列的，原因在于它内部的实现机制，具体就不讲了，有兴趣的同学自己可以看看源码，略复杂。因此这一步要先将原始的batch中的句子重排序，有一点一定要记住，就是要保存原始的句子在batch中的id的位置。后续在rnn处理后，需要重置句子的顺序。 1234567891011121314151617181920def sort_lengths(data, length_list): """ 将句子按照长度进行降序排序，已满足pack_padded_sequence的需要 :param data: 待排序的句子序列 :param length_list:句子原始长度列表 :return: """ #将一个batch中的句子长度列表按照句子长度降序排序 sorted, idx_sort = torch.sort(length_list,dim=0,descending=True) #保存原始的未经过排序的id列表 _, idx_unsort = torch.sort(idx_sort, dim=0) sent_input_list = [] new_lengths_list = [] #根据排序后的id列表，存放一个batch中的句子 for i,index in enumerate(list(idx_sort)): sent_input_list.append(data[:,index,:].unsqueeze(1)) new_lengths_list.append(length_list[index]) sent_inputs = torch.cat(sent_input_list,dim=1) return sent_inputs,idx_unsort,new_lengths_list 排完序后，用pack_padded_sequence生成PackedSequence对象 1234#排序后的batch句子，原始的batch中句子id列表，排序后的句子长度列表sort_seq,orig_order_idx,sort_lengths_list = sort_lengths(seqs,length_list)#将batch的句子pack成专用对象packed_seq = nn.utils.rnn.pack_padded_sequence(sort_seq,sort_lengths_list) 2、使用RNN单元接受PackedSequence对象，它会像dynamic_rnn一样自动处理pad的位置，不算其梯度。得到output后，由于后续有其他操作，仍然需要将变长的序列对齐，因此调用pad_packed_sequence将句子补齐，这是pack_padded_sequence的反操作。 12output,(h_t,c_t) = lstm_unit(packed_seq,(h_0,c_0))output,_ = nn.utils.rnn.pad_packed_sequence(output) 3、但是由于之前对一个batch的句子做了重排序，然而我们的label并没有重排序，如果直接就送入后面的操作，计算loss时，得到的结果肯定是不对的。因此需要将句子按照原来的顺序排列，这时就要用到第一步排序时的副产物：原始句子id list。 123456#reordering，根据原始的存放顺序，将句子在batch中的顺序归位。reorder_output_list = []orig_order_idx = list(orig_order_idx)for index in orig_order_idx: reorder_output_list.append(output[:,index,:].unsqueeze(1))reorder_output = torch.cat(reorder_output_list,dim=1) 总之，绕了一个圈子，把相当于dynamic_rnn的代码实现了一下。如果有大佬有更好的实现凡是，欢迎给出建议。 deep model+传统特征后续我将筛选出的传统特征，加入到了deep model中，效果是有提升的，大概有5%左右的提升。不过由于时间关系，也没有尝试其他融合方式。 总结至此，我要总结的也差不多了。这次比赛，重在参与，虽然只是勉强进了复赛，但是还是学到了很多东西，尤其是熟悉了pytorch这个框架。然而也有很多想法没有去实践。 比如数据增强，这个脱敏数据如何做数据增强？可以通过图的建立，将间接相似的句子对找出来，也可以将间接不相似的句子对找出来，添加到样本中。 比如deep model，做深做大，或者尝试其他更复杂的模型，因为有大佬单模型能达到0.16左右，这样看来，我跟大佬的差距还真是蛮大的。。。 再比如stacking，这个是我一直想做的，但是一来是机器硬件跟不上，二来是单人参加比赛，实在是没时间，精力不够，只能在下班时间埋头苦干。下次比赛，争取找点队友，或者找个靠谱的机器。 最后说一句，希望有大佬能够开源，至少能够讲讲自己的模型思路，让我们这些菜鸟能够多多学习。]]></content>
      <categories>
        <category>比赛总结</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>attention</tag>
        <tag>paraphrase identification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于提升python程序效率的一些思考]]></title>
    <url>%2F2019%2F04%2F01%2F%E5%85%B3%E4%BA%8E%E6%8F%90%E5%8D%87python%E7%A8%8B%E5%BA%8F%E6%95%88%E7%8E%87%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[本文难得水一篇，讲一讲最近在工作中遇到的一些关于python编程的效率问题，希望对其他同学有一些启示，有些东西是网上其他博客就有的，感兴趣的同学可以自行搜索，深入学习。 问题起因是这样的：当前有一个pandas的dataframe结构A，A存储了一个数据序列中的各个字段的值，A的每一行表示时刻t的序列数据的具体内容。当前，需要对A进行归一化整合，使之转化为一段hash后的字符串，中间当然省略了一些很复杂的处理逻辑（包括各种字符串处理，列表查询、字典查询、hash转换等，注意加粗的两个操作，后面会提到）。一个dataframe里面的record数量从100+到5000+不等，DB中存有几十万的dataframe。 传统的单纯使用循环操作，逻辑代码如下： 123for dataframe in dataframe_list: for index, row in dataframe .iterrows(): do something complex........ 可想而知，稍微懂开发的人肯定会说这个效率太慢了。事实上，确实很慢，万级别的数据我运行了整整24小时才跑完。这对于任何一个开发人员来说都是不能忍受的，所以要想办法提升效率。因此我做了以下一些尝试，将运行时间从24小时，逐步地缩减至了1小时。可能还有一些提升空间，如果有同学对pandas或者python高性能编程有一些好的见解，欢迎留言，一起探讨，这方面我还是一个菜鸟。 1、使用python的multiprocessing，主要是使用python的多进程并行，至于python的多线程，熟悉python的都知道，几乎没有传统多线程并行的优势，它的运行机制本质还是单线程的。主要是使用multiprocessing.pool的map方法，但是我的机器目前的cpu核数并不多，才四核，最后试了下，提升效果并不明显，而且最后数据整合花费的时间还是不少的，性价比比较低。 2、然后研究了pandas的map，apply，其中，map的对象是series，即我不能传整个dataframe进去。实际情况是我循环内的操作是要用dataframe里的每个字段的数据的，因此map不适用。apply的话，可以适用，同时由于新的pandas版本中支持了apply的function适用多个参数，因此用起来也挺方便，内层的循环可以去掉，换为： 1dataframe.apply(function,args=(arg1,arg2,....),axis=1) 但是，实体测试之后，发现，这种方法几乎没有显著的提升，对于1000条数据的性能，只提升了几秒。后来，google查问题，发现pandas的apply方法对于rowwise的遍历性能是很差的，相反它对column_wise的列遍历倒是效果很好。因此和传统的for循环相比，apply on row几乎没有太大优势。 3、后来又想到了numpy的向量化操作，如果能将上述逻辑转化为向量单位进行操作，会不会有提升？因此就查到了numpy的vectorize方法： Define a vectorized function which takes a nested sequence of objects or numpy arrays as inputs and returns an single or tuple of numpy array as output. The vectorized function evaluates pyfunc over successive tuples of the input arrays like the python map function, except it uses the broadcasting rules of numpy. 简单说就是能将一个function改造成numpy支持的向量化操作。当然，这个function的参数是有要求的，dataframe或者series肯定是不行的，因此我将本来传入的参数dataframe按列拆开来（幸好列数不多），将每列以ndarray的形式作为参数传入function，然后再用numpy的vectorize改造： 12345def do_function(array_1,array_2,.....): do something return somethingvec_fun = np.vectorize(do_function)result = vec_fun(array_1,array_2,.....) 经过实际测试，性能确实有提升，对于1000条数据的性能，提升了30-40秒左右。 4、上述提升仍然是不够的，注意到我外层还有一个循环，能够对这个循环进行优化呢？答案是可以的。python3中，对于列表递推式的实现有优化，内部使用了generator迭代器来实现，比普通的for循环的性能更好，因此外层的循环用列表递推式来优化： 1[vec_fun(dataframe) for dataframe in dataframe_list] 经过实地测试，性能确实也有提升，对于1000条数据的性能，提升了20秒左右。 5、到了这个时候，一开始我也不知道该怎么优化下去了。对于循环来说，我能做的已经做了，那么只能去循环内部查看哪些代码块可以进行性能优化。经过漫长的分析，终于发现有一个地方优化后，对于性能会有一个惊人的提升。之前一开始我提到了我的内部循环逻辑中，有列表查询和字典查询，这个就是制约我提升性能的最大敌人！由于功能需要，我需要维护很多字典和列表供字符串处理时进行一些查找操作。这些字典列表规模都很大，基本都有十万-百万级。如果用传统的if somethin in list来进行查找，它的时间效率应该是O(n),随着列表规模的增大而增大的。通过google搜索，发现Set数据结构能够优化查找的效率，它首先会对列表进行去重，然后使用类似于红黑树的数据结构，这样进行查找的时候，它的效率应该是O(logn)。因此将所有list用set重新构建后，发现性能有惊人的提升，对于1000条数据的性能，提升了200秒左右！ 经过上述优化后，对于万级别的数据，本来需要24小时才能跑完，目前只需要1个小时的时间就能全部完成，优化效果显著。 当然，优化之路没有终点，我也只是对代码进行了初步的优化，可能还有一些奇技淫巧能够让python程序得到提升，希望对这方面有研究的同学来探讨。]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention机制简单总结]]></title>
    <url>%2F2019%2F03%2F31%2FAttention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本来是想接CMU的NLP公开课的坑的，但是发现讲attention这块内容的公开课程质量着实不高，教授说话声音模糊。（说实话这门公开课的质量都不太好，对萌新不太友好，不过它比斯坦福的cs224n内容多一点，建议学完cs224n再来看这门课会好一点。）再加上最近做一个headline generation的比赛，要用到的attention的地方很多，因此把之前比赛中以及课程中的attention知识简单总结一下，会参考到著名的论文：Attention is all you need。感兴趣的可以去读这篇论文，真的是一篇有划时代意义的论文。 备注：由于attention自被广泛运用以来，涌现出了很多attention的变体，各种奇技淫巧层出不穷，本文篇幅有限，只会讲几个最基本的attention方法。 What is attention？先简单描述一下attention机制是什么。相信做NLP的同学对这个机制不会很陌生，它在Attention is all you need可以说是大放异彩，在machine translation任务中，帮助深度模型在性能上有了很大的提升，输出了当时最好的state-of-art model。当然该模型除了attention机制外，还用了很多有用的trick，以帮助提升模型性能。但是不能否认的时，这个模型的核心就是attention。 attention机制：又称为注意力机制，顾名思义，是一种能让模型对重要信息重点关注并充分学习吸收的技术，它不算是一个完整的模型，应当是一种技术，能够作用于任何序列模型中。 Why attention？照例讲一下为什么要引入attention机制。在之前总结过的seq2seq模型以及之前做NLP的比赛中，对于一段文本序列，我们通常要使用某种机制对该序列进行编码，通过降维等方式将其encode成一个固定长度的向量，用于输入到后面的全连接层。 一般我们会使用CNN或者RNN（包括GRU或者LSTM）等模型来对序列数据进行编码，然后采用各种pooling或者对RNN直接取最后一个t时刻的hidden state作为句子的向量输出。这里会有一个问题： 常规的编码方法，无法体现对一个句子序列中不同语素的关注程度，在自然语言中，一个句子中的不同部分是有不同含义和重要性的，比如上面的例子中：I hate this movie.如果做情感分析，明显对hate这个词语应当关注更多。当然是用CNN和RNN能够编码这种信息。但是如果序列长度很长的情况下，这种方法会有一定的瓶颈。拿CNN举例，具体如下图：图来自“变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统 CNN的核心就是卷积核能够变相学习n-gram的信息，如果是用hierarchical的卷积核，那么越上层的卷积核越能编码原始距离较远的词组的信息。但是这种编码能力也是有上限的，对于较长的文本，模型效果不会再提升太多。RNN也是同理。 基于参加达观文本分类的经历，对于这种长文本处理，使用RNN+attention的效果比使用单纯的RNN+pooling的效果要好不少。 How to use attention?首先讲一下attention最基本最抽象的流程。以seq2seq模型为例。 基本流程：对于一个句子序列S，其由单词序列[w1,w2,w3,…,wn]构成。 1、应用某种方法S的每个单词 $w_i $编码为一个单独向量 $v_i$ 。 2、解码时，使用学习到的注意力权重 $a_i $对1中得到的所有单词向量做加权线性组合$\sum_{i}^{}{a_iv_i}$. 3、在decoder进行下一个词的预测时，使用2中得到的线性组合。 具体构成如下： 我们的最终目标是要能够帮助decoder在生成词语时，有一个不同词语的权重的参考。在训练时，对于decoder我们是有训练目标的，此时将decoder中的信息定义为一个Query。而encoder中包含了所有可能出现的词语，我们将其作为一个字典，该字典的key为所有encoder的序列信息。n个单词相当于当前字典中有n条记录，而字典的value通常也是所有encoder的序列信息。 上面对应于第一步，然后是第二部计算注意力权重，由于我们要让模型自己去学习该对哪些语素重点关注，因此要用我们的学习目标Query来参与这个过程，因此对于Query的每个向量，通过一个函数 $a_i = F(Q_i,K) $，计算预测i时刻词时，需要学习的注意力权重，由于包含n个单词，因此， $a_i$ 应当是一个n维的向量，为了后续计算方便，需要将该向量进行softmax归一化，让向量的每一维元素都是一个概率值。 最后对Value vectors进行加权线性组合，得到带权重参考的“字典”输出： 权重计算函数 眼尖的同学肯定发现这个attention机制比较核心的地方就是如何对Query和key计算注意力权重。下面简单总结几个常用的方法： 1、多层感知机方法 a(q,k) = w_2^Ttanh(W_1[q;k])主要是先将query和key进行拼接，然后接一个激活函数为tanh的全连接层，然后再与一个网络定义的权重矩阵做乘积。 这种方法据说对于大规模的数据特别有效。 2、Bilinear方法 a(q,k)=q^TWk通过一个权重矩阵直接建立q和k的关系映射，比较直接，且计算速度较快。 3、Dot Product a(q,k)=q^Tk这个方法更直接，连权重矩阵都省了，直接建立q和k的关系映射，优点是计算速度更快了，且不需要参数，降低了模型的复杂度。但是需要q和k的维度要相同。 4、scaled-dot Product 上面的点积方法有一个问题，就是随着向量维度的增加，最后得到的权重也会增加，为了提升计算效率，防止数据上溢，对其进行scaling。 a(q,k)=\frac{q^Tk}{\sqrt{|k|}}我个人通常会使用2和3，4。因为硬件机器性能的限制，1的方法计算比较复杂，训练成本比较高。 self-attention关于attention有很多应用，在非seq2seq任务中，比如文本分类，或者其他分类问题，会通过self attention来使用attention。这个方法思想很简单，而且计算成本相对来说不高，强烈推荐。具体来说就是： Query和Key，value都是相同的，即输入的句子序列信息（可以是词向量lookup后的序列信息，也可以先用cnn或者rnn进行一次序列编码后得到的处理过的序列信息。）后面的步骤与上述的都是一样的： 1、首先建立句子序列中的每个词 $q_i $与句子其他词k的注意力权重 $a_i$ 2、然后将注意力权重向量进行softmax归一化，并与句子序列的所有时刻的信息（词向量或者rnn hidden state）进行线性加权。 这种方法中，句子中的每个词都能与句子中任意距离的其他词建立一个敏感的关系，可以说在一定程度上提升了之前所说的CNN和RNN对于长距离语义依赖建模能力的上限。下图同样来自：“变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统 Multi-head attention下面介绍另一种很有效的attention使用方法，叫multi-head attention。这个是在Attention is all you need这篇论文中被使用。图例如下： 公式化表示如下： multihead(Q,K,V)=concat(head_1,head_2,...,head_h)W^o\\ head_i=attention(QW^Q,KW^K,VW^V)解释一下，与原来的self-attention的核心原理其实是差不多的，但是由于self attention只从一个角度去学习关注点，可能会有点偏颇。所以，设计h种不同的 $(W_i^Q,W_i^K,W_i^V) $权重矩阵对，然后做基本的attention操作前，将query，key和value分别用上述权重对做线性变换，然后再计算得到h个不同角度的attention权重 $head_i$ ，将这些 $head_i$ 按列拼接后，再与一个新的权重矩阵 $W^o$ 做线性变换，得到最终的attention输出。 这里要重点说一个问题，就是关于multihead的每个权重矩阵的维度。做self attention时，若使用Bilineard方式，也有一个权重矩阵W，这个权重矩阵维度一般是与Q、K、V的维度是一样的（通常做法，也有不一样的，但是维度不会低）。但是做multihead时，如果对每个权重矩阵的维度还是设为原始维度，那么计算的成本将会蹭蹭得往上涨。如果硬件性能不太行，极有可能会报OOM问题。（不要问我怎么知道的，惨痛的经历。。。）所以通常的做法是：设三种权重矩阵 $(W_i^Q,W_i^K,W_i^V)$的维度分别为 $(d^q,d^k,d^v)$ ,原始维度为d,那么 d^q=d^k=d^v=d/honly attention without RNN?看到有些文章或者博客对attention is all you need中的transformer模型的解读，可能不太完整，似乎只要用attention，就能秒杀其他任何模型。有些同学可能觉得以前的RNN，CNN都不需要了。其实个人感觉，将RNN与attention结合是能拿到好的结果的。为什么transformer没有使用RNN也能对序列数据有很好的建模效果？因为它用了很多其他的trick，首先就是使用了positional embedding，即将词语在句子中的位置关系也做了embedding，它的目的就是通过与原始词向量和attention结合，构建词序上的关系信息，这样就省去了rnn的网络结构，使得训练成本大大降低。但是它的embedding的初始化是一种带正弦函数和余弦函数的数学先验很强的一个方法，可能对其他任务不是太适用，且调这个参数也是比较难的。因此对于个人学习或者研究来说，可以尝试，但是使用RNN还是一个比较稳定的方法，虽然它的训练还是很比较慢的。。。 后记本文只是简单讲了几个最基础的attention方法，还有很多有效复杂的方法未涉及，比如global attention，local attention，hard attention等等，还有之前做文本分类时了解过的HAN（hierarchical attention network，是先对一个句子里面的词做attention，然后对文章中的句子做attention，相当于做了一个二层结果的层次attention）有兴趣的同学可以直接看论文了解。 引用： cmu nlp公开课 “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[seq2seq之tensorflow源码解析]]></title>
    <url>%2F2019%2F03%2F31%2Fseq2seq%E4%B9%8Btensorflow%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[很直接的说，这篇文章就是对tensorflow的seq2seq大礼包的源码做了一定程度的解析。个人一直觉得作为一个算法工程师，要经常学习好的开源框架里面的工程代码，这样不仅能够在实现自定义模型时好下手，也能提升自己的工程能力。本文用到的tensorflow框架版本为1.4。目前比较新的版本中的相关代码主体改动有一些，不是很大，所以可以适配着看。 之前的文章中，我已经对seq2seq+attention模型有过讲解，当时以为自己弄懂了。但是最近通过一些比赛，实际去实现这个模型的时候，才发觉自己并没有真正吃透这个模型，因此下面重新对这个带attention机制的seq2seq模型进行回顾，然后再从tensorflow的seq2seq大礼包源码入手，讲解其中的步骤。为了更好描述，这里以最近参加的标题生成的比赛作为例子。（成绩不太好，就不写总结了） 假设当前的输入数据，已经对out-of-vocab词用代替，且对decoder的输入数据增加了标记，对输出数据末尾增加了标记，最后已经padding成等长数据。模型的主要结构为encoder-decoder。规定输入和输出共享一个词典和embedding层。 Encoder首先是encoder。主要作用是读取并学习document text中的信息。常使用RNN或者带门机制的RNN（GRU或者LSTM）。一般会先对输入进行embedding_lookup，获取对应的embedding_input后，将其输入到RNN的中进行编码。一般的encoder会直接输出最后 $t_n $时刻的状态 $h_{t_n} $，作为decoder的初始 $t_0​$ 状态。这里有个改进点，可以提升整个模型的性能： 之所以可以将encoder的$h_{t_n}$作为decoder的初始状态，在于经过RNN的学习与传递后，最后一个时刻的状态h已经整合了当前网络对输入文本的分析和总结，因此直接将其作为decoder的初始状态，相对于随机初始化操作，可以加快网络的训练速度，提升模型的性能。那么，我们可以通过其他方式，更好得获得encoder的学习成果，比如将所有t时刻的状态 $h_{t_i} $进行一个average pooling,max pooling，甚至进行一次self attention，最后得到一个综合的输出，作为decoder的初始状态，这样应该能比baseline方法更好。在实践过程中，也确实是这样，其中average pooling的效果最好，而self attention之所以效果较差的原因可能是模型中后面decoder的时候也使用了attention机制，因此这里使用self attention机制对模型效果提升没有很大的效果，相反，还会导致过拟合。且average pooling的效率最高，因此最后我选用了这种方式，来输出decoder的初始状态。 当前encoder模块有两个比较重要的输出会被后续的decoder使用： 1、RNN中的每个时刻的状态$h_{t_i}​$ 2、RNN中的每个时刻的输出 $o_{t_i} ​$ 其中，状态$h_{t_i}​$会作为decoder的初始状态被使用到，而输出 o$_{t_i}​$则会在attention机制计算时使用到。这就是该模型的核心之处。下面用图表示，会比较清楚。 Decoder decoder的主体也是使用的RNN，原始输入经过embedding_lookup(和encoder共享的)后，输出到RNN中，每个t时刻的状态为 $s_t$ ,其中 $s_0$ 的初始化已经在前面讲过，用encoder的相关信息来初始化。然后就是模型的attention机制的计算和使用。 这里以Bahdanau Attention计算方式为例。这个attention具体的内容就不讲了，有关attention的内容之前文章有描述，这里直接放上公式： e^t_{i}=v^Ttanh(W_oo_i+W_ss_t+b_{attn})\\ a^t = softmax(e^t)其中， $v,W_o,W_s,b_{attn} ​$均为网络权重参数。 $o_i​$ 为参与attention计算的memory，即encoder的output，而 $s_t​$ 为decoder在decoder t时刻的状态。 这里注意，我使用的是Global attention，相对的还有Local attention，由于Local attention计算方面还要考虑上下文windows，为了便于讲解，使用了简单的Global attention机制，且效果也挺好的。 这个公式表明，我每次要计算输出当前时刻的decoder output logits时，需要将当前时刻的decoder 状态整合encoder的output，一起输入到attention公式中计算，最后得到alignment，这个alignment只与当前时刻t的计算有关，一般是不用保存的。当然后续应用coverage机制的时候，需要将这个alignment的历史信息都先保存下来。 得到alignment后，接下来就是应用 $a^t$ 到encoder的output中，进行加权求和，得到最终的attention context。这个是融合了encoder，decoder输入信息的attention学习成果。 o^*_t = \sum_ia^t_io_i这个学习成果需要和decoder的中间状态$s_t​$整合在一起（一般使用concat，也有其他整合方法），然后输入到全连接层中，并最终输出logits，或者输出一个softmax概率，即词库中每个词在当前位置的概率分布。最后计算当前时刻的 $loss_t ​$。 当然，所有时刻的 $loss_t$ 计算完后，需要将loss加起来，并做sequence_mask，将pad位置的loss过滤掉，然后求平均。 Tensorflow seq2seq上述过程，看着挺简单的，但是实现起来，其实还是挺麻烦的，因此tensorflow提供了一个方便的大礼包，包含了seq2seq的各个阶段的计算接口，只要按照规则正确调用，就能搭建出模型。但是这个大礼包，虽然很方便，但是要进行进一步的自定义修改，还是挺难的，需要对里面的代码进行长时间的阅读和理解，且tensorflow的文档可以说是比较少了，所以这里把我这段时间阅读代码的总结写下来，方便大家参考。 大礼包的主要核心部分在decoder侧，encoder侧，还是使用我们熟悉的RNN的接口，一开始我使用的是CudnnGRU，但是由于其不支持dynamic_rnn，无法对pad位置的信息特殊处理，因此实际使用效果虽然速度快，但是效果却没有原始的gru接口好，因此最后还是用了tf.contrib.rnn.GRUCell（貌似tf.contrib这个包会在后续的版本中逐渐舍弃，所以建议还是用另一个接口）。最后说明一下encoder的输出为两部分： 1、encoder_output,shape=[batch_size,time_steps,rnn_dims]，为rnn的输出 2、encoder_state,shape=[batch_size,dims]，如果为双向rnn，需要将两个方向的state先拼接，然后再对所有时刻的state沿time_steps做average pooling。最后可能还需要对encoder_state做一层全连接层，使得它与decoder的RNN的dimension一致。 decoder的编写流程 下面我先按照正常的流程使用大礼包编写decoder的流程。然后按照代码的执行顺序描述代码的运行机制。inference阶段我使用beamSearch来举例。 123456789101112attention_mechanism = tf.contrib.seq2seq.BahdanauAttention( decoder_dim, encoder_output, memory_sequence_length=sum_len, normalize=True)decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=atten_size)initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size)initial_state = initial_state.clone(cell_state=encoder_state)#trainhelper = tf.contrib.seq2seq.TrainingHelper(decoder_inputs_embedding,title_len, time_major=False)decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state,output_layer=projection_layer)outputs, self.train_dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False)#logitsdecoder_output = outputs.rnn_output 上述是训练过程，下面我们自底向上的顺序来说明这个源码。 源码执行流程 由于下面内容比较繁琐，因此先po出流程图，不喜欢看文字的同志可以看这幅图了解一下大概调用流程。该图只是一个整体的流程，很多细节在后面文字描述。 tensorflow seq2seq大礼包模块简易流程 1、调用栈的最底层就是我们的tf.contrib.seq2seq.BahdanauAttention，它提供了具体的attention计算方法，源码位于tf.contrib.seq2seq.AttentionWrapper中。 可以看到它继承了一个基类，这个基类的主要作用是在_init_方法中对memory做了全连接层的计算（如果定义了dense layer的话），得到attention计算中的key和value。而子类则定义了query_layer和memory_layer,以及最后归一化的方法（一般是softmax） 接下来就是该类的核心方法： 可以看到该方法是一个call()方法，熟悉python的同志肯定知道，一个类实现了这个方法，可以让类变得可以向函数方法一样被直接调用。其中，主要核心就是调用了_bahdanau_score方法来进行attention计算。这个方法具体就不贴出来了，就是按照论文中的计算方法。 备注：这里有一个_probability_fn方法，需要输入历史的alignment，这个就是之前说的，有一些attention方法需要使用历史的alignment数据，因此所有attention方法都默认会调用这个_probability_fn方法，只不过在BahdanauAttention中它是不对历史的alignment做任何操作，直接对score做softmax。 2、往上面一层就是AttentionWrapper，这个类负责调用整个Attention机制的流程。首先看一下它的类定义： 可以看出来，它继承了RNNCell，说明这也是一个具有RNN特性的实现类，且在RNNCell上，封装了Attention的特性。另外，继承了RNNCell的所有子类，需要实现call()方法，使得在构建网络计算图（build())时，能够调用相关逻辑。 分析它的init方法，可以看到它接收decoder中的RNNCell，之前定义的Attention机制，另外关注一下alignment_history，这个属性就是能控制是否保存历史alignment信息的开关。Attention_layer_size属性则定义了Attention操作后是否需要连接一个全连接层到指定的dimension。 该类有两个重要的方法，zero_state和call。前者用于初始状态s的封装和生成，后者主要是用于attention计算的主流程。 下面看一下zero_state方法， 其实该方法最重要的就是返回一个封装过的可用于Attention计算的AttentionState。 下面看一下这个封装类的结构： 可以看到，其实该类就是一个namedtuple的数据结构封装。 cell_state存储的是AttentionWrapper包裹的RnnCell在t-1时刻的状态 attention存储的是t-1时刻输出的context time存储的当前时刻t alignments存储的是t-1时刻输出的alignment alignment_history存储的是所有时刻的历史alignment信息 AttentionWrapperState中还有一个clone方法，在我们的模型图中也有调用的地方： 1initial_state = initial_state.clone(cell_state=encoder_state) 其实就是对我们初始化的AttentionWrapperState对象，将cell_state的属性值对替换为从encoder输出的state（经过average pooling）。 下面是AttentionWrapper类的核心方法：call，该方法定义了attention操作的主流程。 该方法的参数为inputs：即decoder中的当前时刻t的输入，而state则是封装过的AttentionWrapperState。下面对关键代码进行注释： 上述代码的主要操作是将当前时刻input和前一时刻的context拼接后，输入到decoder中的RNN层中做处理。最后输出output，以及下一个时刻的中间状态。 主要核心是方法_compute_attention方法，该方法是attention计算的入口。 得到attention的context和alignment信息后，就是返回需要的信息，其中比较重要的是返回的当前时刻的中间状态为AttentionWrapperState，这个中间状态会被下一时刻t+1的计算使用。 需要注意的是最后返回的时候，有一个flag，_output_attention,这个是控制当前是否返回attention的信息还是rnn的output信息，对于BahdanauAttention style来说，是返回cell_output. 3、再往上一层，找到tf.contrib.seq2seq.BasicDecoder，这个类主要作用是将上述的所有操作流程按照decoder的序列长度依次按顺序执行。 看到它继承了Decoder这个类。它的核心方法为step方法，下面就basicDecoder的step方法具体描述： 它的参数为time：当前时刻，inputs:decoder的输入，state：前一时刻传递而来的状态。下面是具体的代码流程： 首先这里的_cell，是我们之前定义的AttentioWrapper，因为它是继承了RnnCell，因此具有RnnCell的特性。这里相当于是调用做了前两个大模块的操作。返回了当前时刻的output,state. 那么在训练阶段，模型是如何推动上面的计算步骤一步一步到最后的呢？下面要用到另一个有用的大礼包的类：tf.contrib.seq2seq.TrainingHelper 上图中的helper就是要用的帮助训练的类（顾名思义）。这里调用了两个方法，第一个是sample，即根据当前时刻的output，获取当前时刻词分布中概率最大的那个词id。 第二个是next_inputs方法，主要是根据当前处理的时刻t，读取下一个时刻的输入，用于下一时刻的计算，并返回序列处理结束标志。 step的最后一个步骤就是返回处理的结果，这里它又封装了一个特殊的数据类型：BasicDecoderOutput BasicDecoderOutput定义如下： 其实根AttentioWrapperState相似，也是一个封装了的namedtuple，主要存储的是rnn_output,以及最后得到的词的id。这样封装的好处是，rnn_output可以用于计算loss时直接使用，而sample_id则是在inference阶段可以用来输出结果。 4、最高一层是大礼包中最重要的一个部分，上述basicDecoder的step方法，如果没有任何上层接口驱动，也是无法完成。因此tf.contrib.seq2seq.dynamic_decode就是用于完成这项工作的。 与其他接口不同，这个是一个可直接调用的方法，其方法定义如下： 其中decoder就是之前定义的basicDecoder.impute_finished 属性表示模型在梯度传递的时候会忽略最后标记为finished的位置。这个一般设为True，能够保证梯度正确传递。而maximum_iterations为我们自定义的decoding最大长度，可以比设置的title_len大或者小，主要看调参。swap_memory表示在执行while循环是否启用GPU-CPU内存交换。 下面只列出该放里面的核心步骤： 首先这是tensorflow中的循环操作。它的循环条件condition为： 他会接受basicDecoder返回的finished标志，并判断当前是否已经处理结束。 然后是循环的body部分，也只放上核心部分： 即调用basicDecoder的step方法来执行decoding，这样就与之前讲的联系上了。 Inference阶段其实train阶段和inference的不同点很简单，在于inference阶段没有decoder的input，因此每个时刻的state计算都需要输入前一个时刻的计算结果。这里以BeamSearch举例。 12345678910111213141516171819202122tiled_encoder_output = tf.contrib.seq2seq.tile_batch(self.encoder_output,multiplier=self.cfg.beam_width))tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(encoder_state,multiplier=self.cfg.beam_width)tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.sum_len, multiplier=self.beam_width)attention_mechanism = tf.contrib.seq2seq.BahdanauAttention( self.cfg.lstm_units, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=self.cfg.lstm_units * 2)initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.cfg.beam_width)initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)decoder = tf.contrib.seq2seq.BeamSearchDecoder( cell=decoder_cell, embedding=self.embedding_init, start_tokens=tf.fill([self.batch_size], tf.constant(2)), end_token=tf.constant(3), initial_state=initial_state, beam_width=self.beam_width, output_layer=self.projection_layer )outputs,_,_ = outputs, _, _ = tf.contrib.seq2seq.dynamic_decode( decoder, output_time_major=false, maximum_iterations=self.decoder_max_iter, scope=decoder_scope)self.prediction = outputs.predicted_ids 整个inference流程与train类似，唯一不同的地方在于beamSearch算法本身，需要将所有的输入和中间状态复制beam_size份，用于beam的搜索。而主要的区分点在于使用的decoder不同，这里我就着重讲一下tf.contrib.seq2seq.BeamSearchDecoder。 主要还是贴出其step方法中的核心过程： beamSearch的方法中，会存在很多merge_beam,split_beam等改变tensor的shape操作，方便一些操作的计算，这里就不仔细讲了，只要记住merge一般和split应是成对出现。 首先当然是调用AttentionWrapper，来计算输出当前时刻的cell_output,以及下一个时刻的state。 然后就是另一个核心方法调用_beam_search_step： 这个方法主要是执行Beam搜索的流程，核心的模块流程如下： 先计算当前时刻为止的所有候选序列计算概率值之和。 然后计算每个beam的分数： 然后是根据指定的beam_size,使用top_k运算，得到最合适的beam_size候选。 最后就是返回一些封装的结果，就不具体列出了。 总结其实还有很多源码并没放到上面分析，鉴于篇幅问题，写的太多，可能看得也越困难。总体来说，通过这次的比赛实践，还是对seq2seq模型有一定的深入理解，无论是理论上还是工程实现上，tensorflow的大礼包的实现确实挺漂亮，一环扣一环，希望能吃透它的工程思维，融入到自己的实践中。]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[达观文本分类大赛17名思路总结]]></title>
    <url>%2F2019%2F03%2F29%2F%E8%BE%BE%E8%A7%82%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%A4%A7%E8%B5%9B17%E5%90%8D%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[最近solo了一个比赛，达观的文本分类比赛，大概搞了20来天，最后b榜成绩17名，比a榜下降了一名。虽然最后没有进前10，不过这次比赛相比之前参加kaggle的文本分类比赛，可以说是有了长足的进步。这里把这次比赛的思路总结一下，后续有时间的话会把代码开源，不过对于已成家的上班族来说，还是不要抱太大期望。。。 先简述一下赛题吧，任务很简单，是一个文本多分类比赛，这个比赛主要有两个难点： 1、文本长度分布不均，短的很短，长的很长，最长的貌似都上万个词了，最短的才几十个词。虽然训练数据只有十万多，但是对于缺少GPU、内存的同学来说，做这个比赛还是比较痛苦的，而且用深度学习时，对文本的截断和padding也是一个影响性能的因素。 2、文本是脱敏的，也就说不知道提供的数据里面的具体文本内容。这个也是现在国内文本比赛的趋势，脱敏的数据意味着很慢做一些传统NLP的数据预处理，比如停止词过滤、具体词的分析，同时也没办法使用已有的预训练好的词向量。 主办方提供了词分割的文本和字分割的文本，由于时间和硬件成本，我只使用了词分割的文本，其实如果将字文本也用上的话，进行模型融合，应该还可以让最后成绩提升，后面有时间再试了。 预处理首先是先看看所有分类标签的占比，有没有样本分类不均衡的现象。不知道是不是主办方处理过了，这次并没有严重的不均衡，一共19个类别，基本上每个类别样本量差距不大。另外，要想办法过滤一下停止词和标点符号。针对脱敏数据，我统计了每个词的idf值，将那些所有文章都出现的词筛选出来，大概筛出来10个左右的词。这些词即使不是标点符号，肯定也是一些对文章主题无用的词，可以过滤点。另外，经过分析，发现训练集样本中有文本重复的记录，应该是数据的噪声，有些重复样本的标签居然是不同的！这种噪声数据，我就全部过滤掉了，而标签相同的重复样本我就保留一条，其余的也删除。 （PS：删除过滤词之后，发现有的样本词全部删光了，这种文本估计也是噪声，重要的是测试集也有这种样本，总不能删除吧，只能使用-1这个UNK标记来填充，索性这种样本数量也不多，对总体的模型影响不是太大。） 词向量主要是自己从头开始训练了word2vec和glove两种词向量，维数是300维，最后两个词向量都使用了，做了concat拼接，当然这个维度直接使用会过拟合的，所以后面接了dropout层，使用0.5的概率。 模型本次比赛主要还是用的深度模型，最后会和xgboost结合，做stacking。 我用的方法其实也没有很复杂的地方，下面简单列出，同时会列出一些trick： 1、深度模型的话，我使用了双向LSTM+self-attention,这里要提一点，我的LSTM并没有使用传统的tf的rnncell，因为实在是太慢了！由于我使用的硬件平台关系，我的job最长时间不能超过12小时，否则我的当前进程会被强制下线。所以，一开始被这个训练成本搞得差点弃赛。 后来各种搜索高性能的rnn实现，发现tensorflow里面早有了cudnn的高性能实现，于是试了tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnnops的CudnnLSTM，效率瞬间飞起！本来我的一个epoch要10000多秒，最后降到了1500秒。但是要注意的是，这个CudnnLSTM不支持对padding的masking，即没有实现dynamic_rnn,所以对于一些padding的序列，它的梯度是会把padding的位置传递进去的。可能这也是我深度模型没有上太高分的一个原因吧。后面使用attention可以说对模型的提升程度也很高，其他的降维方法如max pooling，average pooling等都试过，但是还是attention效果最好，这个方法能够帮助模型对一些重要词赋予较高的权重，这个权重是通过模型动态学习到的。 到这里，我的分数提升路径大概是：bicudnnLSTM+maxpooling:0.777,bicudnnLSTM+attention:0.779，总算是赶上了baseline的水平了。 备注一下：基于性能和模型训练成本的平衡，最后我文本序列长度定为了2500。分析过文本长度的分布，大部分文本长度都在5000以下，但是1000-5000的文本数量也不少。 2、使用了伪标签。因为本次比赛的训练集为10万+，但是对于长本来说，这个量级还是比较少的，训练集不够，而且训练集和测试集的分布也可能会有一定的差异。因此借鉴了kaggle上的这个技巧，将当前预测最好效果的测试集+标签添加到训练集中，作为训练集进行训练，通过这种方式，我的线上成绩达到了0.78+。具体的使用方法就引用kaggle上的一个图例，将图中的评估标准换成比赛的F1_score就可以了。 3、使用了传统的NLP特征。这一步是我深度模型提升最大的。主要是对tfidf向量化后的term-doc矩阵，训练了lsi和lda两种特征，最后将两个特征的稀疏矩阵拼接起来放入我的深度模型中，大概如图所示： 使用这个模型，使用10cv+伪标签训练，最后模型的线上分数能达到0.79+ 4、我训练前期使用的是adam的优化方法，后期使用的momentum gradientDescent手调，由于时间因素，并没有调到很理想的性能，所以我尝试将10cv中，每个cv中所有训练数据（不包括测试集数据）在softmax层前得到的特征保存下来，并输入到xgboost进行训练： 这样，我一个参数组合的深度模型，最后能得到10个xgboost的训练结果，最后我调了两组参数，也就是得到了20个xgboost的训练结果，用于后面的stacking。 5、stacking是我之前比赛没有尝试过的，这次是第一次尝试，使用的是oof方法。我的基模型用的是步骤四得到的20个xgboost的结果，注意这里结果是19个分类的概率，即stacking后，应该是得到一个390个特征的数据。oof方法这个在网上很多都有，就不详细说明了，简单说就是用基模型验证集的结果拼成训练集，然后进行二次训练，而测试集的特征则是所有基模型的测试结果的平均。通过这种方法，我的线上分数升到了0.794+。（注意，stacking我并没有使用伪标签数据，一旦用了很容易过拟合，这个要注意。） 6、后续就是各种参数的调优和lgb、svm的尝试。基本上没有什么多说的。最后最好的A榜分数大概在0.7943，但是悲催的是我没有选择用于b榜评测的提交结果，幸好最近两次提交中有一次结果还可以，大概在0.7939左右。不过跟我的最好成绩也差不多，跟前面的差距也挺大的。。。 总得来说，这次比赛，将之前我参加比赛的未实现的想法都实现了，但是还是有一些遗憾的，比如没有使用字的文本，另外后来有想到将每个文本拆开来，单独训练出不同模型，最后再融合，这样可以解决文本过长的问题，不过时间不够就没有尝试。还有最后想做error analysis，发现对脱敏的数据基本无从下手，只是用了textrank提了关键词，发现一些互相分错混淆的类别，确实在关键词集上式相似的，估计是主题很接近的类别，这种类别要区分开来估计要更复杂的网络。希望在观看前10答辩时，能有一些启发。 最后，吐槽一下，一个人比赛实在是太累。。。由于工作关系，也不好与他人组队。后面选择比赛的时候，还是只能选一些数据相对较小的比，不然真的吃不消。不过做了那么多判别分类式的比赛，希望后面做一些生成式的比赛，比如机器阅读、自动摘要、QA等。最近有AIC和字节跳动的比赛，可能会选一个参加一下吧。欢迎有兴趣的同学私聊。]]></content>
      <categories>
        <category>比赛总结</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>attention</tag>
        <tag>text classificaton</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（基础算法复习）之最长回文子串的动态规划解法]]></title>
    <url>%2F2019%2F03%2F29%2F%EF%BC%88%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%EF%BC%89%E4%B9%8B%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%A7%A3%E6%B3%95%2F</url>
    <content type="text"><![CDATA[再开一个新坑，以后会不定期总结一下对于一些基本算法题的解法思路和实现。作为一个AI领域的从业者，不仅对于AI本身的技术要有掌握，对于计算机科学中的基本算法思想也要扎实的基础，这样才能做一个合格的算法工程，本人在这方面的能力还不够成熟，因此开这个坑，希望通过这种方式，真正去理解算法的本质。 本文要总结的题目是找最长回文子串，对应leetcode中的题目如下： Longest Palindromic Substring Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: Input: “babad” Output: “bab” Note: “aba” is also a valid answer. Example 2: Input: “cbbd” Output: “bb” 本文主要集中在如何一步一步使用动态规划的方法来解决这个问题并进行优化的过程，有其他更好的解决该问题的方法不在本文中讨论。 最简单思想首先，这个题目最简单的思想就是暴力遍历所有可能结果，假设当前字符串序列为 $s_1…s_n$ 具体做法是遍历所有可能长度的子串，判断其是不是回文子串，然后记录其长度，最后返回那个长度最长的子串即可。 暴力搜索的问题暴力搜索的时间效率肯定是最低的，那么低在哪里呢？我们以$s_1…s_n$为例，列举暴力搜索的前几个步骤，很明显就能看出来问题： 子串长度为1时，遍历所有单个字符 $s_1,s_2,…,s_n $，此时所有单个字符都是回文。 子串长度为2时，遍历所有二元字符子$s_1s_2,s_2s_3,s_3s_4,…,s_{n-1}s_n $，然后判断每个元素是否为回文，判断依据是 $s_i?=s_{i+1}$ 子串长度为3时，遍历所有三元字符串 $s_1s_2s_3,s_2s_3s_4,…,s_{i-1}s_is_{i+1},s_{n-2}s_{n-1}s_n$ ，这里判断每个元素是否为回文。 子串长度为4时，遍历所有四元字符串 $s_1s_2s_3s_4,s_2s_3s_4s_5,…,s_{i-2}s_{i-1}s_is_{i+1},s_{n-3}s_{n-2}s_{n-1}s_n $，然后判断每个元素是否为回文。 重复上述步骤，直到遍历到 $s_1…s_n​$ 这个长度为n的字符串为止。 进一步分析，其实步骤3时，其实会用到1的结果，因为每个元素中间项 $s_i $通过1已经知道是回文了，那么只要判断 $s_{i-1}==s_{i+1}$ 是否成立就行了。然而步骤3是重复处理了。 同样的，步骤4同样用到了步骤2的结果，对于 $s_{i-2}s_{i-1}s_is_{i+1}$ ，如果知道 $s_{i-1}s_i $是回文，那么只要分析 $s_{i-2}?=s_{i+1} $就可以了。因此步骤4也重复处理。 引入动态规划通过上面的分析可以知道，暴力搜索时，并不是每一个元素都需要去专门遍历并判断是否为回文，有些元素可以通过历史的结果来帮助判断一个元素是否为回文。同时我们也可以总结一些规律： 假设 $s_i,…,s_j $是我们要求解的最长的回文子串(其中j&gt;=i)，那么它必然符合两个条件： a. $s_i==s_j $ b. $s_{i+1},…,s_{j-1} （j&gt;=i+2）$必然也是一个回文子串，否则上述假设不成立。 基于上述规律，我们可以将这个问题进行拆分，分成最基本的优化目标问题单元：即只要到一个回文子串，然后以该子串为中心，左右两边的字符若相等，则找到一个更长的新的回文子串。即从一元字符开始，以该字符为中心向两边扩展的方式，逐渐找到更长的回文子串，有点像穿衣服一样，一层一层往上套。这个就是动态规划的思想，当然我只是用口语化的方式描述了一下，理论化的动态规划肯定更加严谨公式化，但是我就不在这里讨论了，有兴趣的可以自行查阅。 那么如何实现上述的思想呢？前面说了，我要知道$s_i,…,s_j$是否为回文，那么主要是需要知道$s_{i+1},…,s_{j-1}$是否为回文。这里可以将其视作一个填表问题。我们设计一个bool型的n*n的二维矩阵A，如果$s_i,…,s_j$是一个回文串，那么 $A_{ij} $位置就填上true，否则为false。填表的顺序则是从长度为1的字符串开始，逐级向长度更长的字符串填，间而言之就是一种周期性的斜向上方向，具体如图： 其实上图也说明了一个事情，就是我们并不需要把所有表格都填上，而是只需要填满上三角的表格就能得到最终结果。因此，在代码实现的时候，我们可以初始化一个上三角形状的二维数组，这样可以节省一半的空间。（代码实现的时候，我还是初始化了整矩阵，为了与后续优化区分）。 我们要做的，就是当遇到回文串时，不仅在上述表格相应位置填上值，而且需要记录当前子串的起始位置和长度，用于后续的返回结果。 代码实现1下面给出我写的初步动态规划解法，只列出关键逻辑，完整代码后续会放到github上，到时会贴出来。代码不是很优美，求大佬轻喷。 12int s_size = s.size();vector&lt;vector&lt;bool&gt; &gt; result(s_size, vector&lt;bool&gt; (s_size,false)); 初始化一个n*n的bool型矩阵，n为字符串长度。 12345678910111213for(int i=0;i&lt;s_size;i++)&#123; result[i][i] = true; if(i &lt; s_size-1) &#123; if (s[i]==s[i+1]) &#123; result[i][i+1] = true; cur_max = 2; start = i; &#125; &#125;&#125; 这里是先填充对角线，以及 $(i,i+1)$ 上斜线的位置，对角线对应一元字符，这个很好理解，都是回文，而上斜线的位置也很好处理，只要判断$s_i?=s_{i+1} $就可以了。同时要记录当前最长回文的起始位置start以及长度cur_max。 1234567891011121314151617for (int distance = 2;distance &lt; s_size;distance++) &#123; // cout&lt;&lt;distance; for (int i = 0;i+distance&lt;s_size;i++) &#123; result[i][i+distance] = (result[i+1][i+distance-1]&amp;&amp;(s[i]==s[i+distance])); if(result[i][i+distance]) &#123; if(distance+1&gt;cur_max) &#123; cur_max = distance+1; start = i; end = i+distance; &#125; &#125; &#125; &#125; 这里最外层的循环表示字符串的长度遍历，里层遍历则是对字符串起始位置的遍历。循环内部的逻辑在上面章节已经介绍过，即根据$s_{i+1},…,s_{j-1}$是否为回文以及$s_i?=s_j$来判断$s_i,…,s_j$是否为回文。同时记录回文子串的起始位置和长度。 进一步优化上述代码在leetcode运行后的时间和空间成本如下： 可以看到无论是时间成本还是空间成本都是很高的，优化的空间很大呀！ 回头看我们的上述解法过程，我们将一个矩阵中一个斜线上的所有位置填表称为一个epoch操作。每轮epoch其实都隐含得使用了前一个epoch的信息，更加具体的说是前一个epoch的某一个位置上的值。我们是否需要一整个二维矩阵表格来填某个epoch的信息呢？答案是不需要！具体原因看下图图示： 具体来说，每次填表格，其实只需要其右下角的表格的值就可以了，而初始的对角线和上斜线位置的判断也是很简单就能实现也不用记录，因此实际上我们不用一个二维矩阵，只需要几个临时变量记录右下角的表格值就可以了。 另外，我们需要变更填表的顺序，之前我们的顺序都是每个epoch斜上方向45度填。这里我们改换成如下的填表顺序： 其中箭头上标的数字代表循环的epoch轮数。即每次我向左斜向上方填表，填到边界时，跳到下一轮的其实位置，接着填。 优化代码实现代码的话其实注意几个点就行，一个是每一轮遍历的顺序是往左斜向上的，另一个就是每一轮要记录当前轮起始的位置，因为下一轮的起始位置是与前一轮的起始位置和当前轮数相关的，如图： 主要代码逻辑如下： 123bool pre_flag = false;int epoch_start_i = 0;int epoch_start_j = 0; 这里直接初始化一个临时变量存放右下角的值，然后初始化两个位置变量来存每一个epoch的起始位置。 对于对角线和上斜线的处理就不贴出来了，这里贴一下其他位置的填法： 12345678910111213141516171819202122232425262728293031//根据右下角的表格填当前表格pre_flag = (pre_flag &amp;&amp; s[i]==s[j]);//如果是回文，且长度最长，则记录起始位置if(pre_flag &amp;&amp; j-i+1&gt;cur_max)&#123; cur_max = j-i+1; start = i;&#125;//到边界位置，需要换到下一轮的起始位置if(i-1&lt;0 || j+1 &gt;=s_size)&#123; //本轮起始位置为对角线上 if (epoch_start_i == epoch_start_j) &#123; i =epoch_start_i; j = epoch_start_j+1; &#125;else &#123; //本轮起始位置在上斜线上 i =epoch_start_i+1; j = epoch_start_j; &#125; continue; &#125;else&#123; //向左斜上方向处理 i--; j++; continue;&#125; 最后，上述代码在leetcode上运行的花费如下： 可以看到，无论是时间复杂度，还是空间复杂度，都有了很大的优化。]]></content>
      <categories>
        <category>基础算法</category>
      </categories>
      <tags>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow模型线上部署]]></title>
    <url>%2F2019%2F03%2F24%2Ftensorflow%E6%A8%A1%E5%9E%8B%E7%BA%BF%E4%B8%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[这次写一个比较短的工程部署总结，说是tensorflow线上部署，并没有使用目前推荐的tensorflow serving。一则是该功能只是整个项目中的一个很小的功能点，如果单独为该功能部署tensorflow serving，成本和时间上会比较超标；二则由于公司内部网络环境限制，无法完整顺利获取tensorflow serving需要的依赖包（在没有网络环境下去装tensorflow serving环境的同学都懂的）；三则，项目并没有对模型在线学习的需求，只需要模型离线训练后，部署到线上即可。基于上述三点条件，选择了使用tensorflow python开发并训练模型，最后将模型进行序列化。然后在java端（项目主开发语言）调用模型进行在线预测。 前置条件tensorflow 1.3 所需的tensorflow java jar包：libtensorflow-1.4.1.jar 所需的tensorflow jni 库文件：libtensorflow_jni.so libtensorflow_framework.so 模型序列化一开始，由于未考虑模型如何移植到java平台的原因，模型训练和保存的时候，还是按照传统的方式，即通过tf.train.Saver()方法得到saver对象，然后调用saver.save方法保存模型，得到模型的checkpoint,meta,data,index四个文件。 上述文件中保存了模型的中间参数，模型当前训练的状态等信息，在某种意义上是动态的，只能通过tensorflow本身的python接口来调用该保存好的模型，并不能跨平台直接使用。其实模型的本质还是一堆权重数据和具体的权重计算流程，因此需要某种机制能固定住模型的权重数据和计算流程，即freeze模型。 1234567891011saver_predict = tf.train.import_meta_graph(model_config.ckpt_path + model_name)with tf.Session() as sess: if os.path.exists(model_config.ckpt_path): print("Restoring Variables from Checkpoint") saver_predict.restore(sess, tf.train.latest_checkpoint(model_config.ckpt_path)) else: print("Can't find the checkpoint.going to stop") exit() output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node] frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names) tf.train.write_graph(frozen_graph_def, model_config.ckpt_path, target_model_path, as_text=False) 上述代码的作用是对使用import_meta_graph读取训练好的模型,然后获取当前计算图中的所有节点，并将这些节点中的所有权重参数转化为常量，最后将这些常量保存到一个pb文件中，pb文件即protobuf，是 Google 推出的一种二进制数据交换格式。能够用于跨平台间的数据交换。上述代码实际使用时，会有问题，在java侧使用java的api调用模型时，会出现如下错误： 1Invalid argument: Input 0 of node XXXXXXXXXXX/BatchNorm/cond/AssignMovingAvg_1/Switch was passed float from XXXXXXXXXXX/BatchNorm/moving_variance:0 incompatible with expected float_ref. 经过一番google，找到了这个github的issue，貌似并没有得到很好的解决，推测问题原因是在freeze模型的权重参数时，对一些tensor的data type处理有问题。问题链接如下，有兴趣的同学可以看看，我最后换了另外一种方式来做。 []: https://github.com/davidsandberg/facenet/issues/161 最终，我使用的是另外一种方法，即在模型训练完时，直接对模型的权重参数序列化，保存为pb文件。代码如下： 1234# 将模型序列化保存`builder = tf.saved_model.builder.SavedModelBuilder(model_config.pb_path.format(epoch))builder.add_meta_graph_and_variables(sess, ["XXX"])builder.save() 调用SavedModelBuilder得到builder对象，然后将session中的所有图结构和权重参数存入到builder中，最后保存为pb文件。 Java调用模型在线预测这里主要使用的是tensorflow的Java版本api，说是Java版本，其实功能很局限，并没有模型训练方面的功能，所幸它有读取模型和数据，然后在线预测的功能，因此可以适用于当前场景。在部署时，有几点需要注意一下： 1.最基础的事情，当然是记得将tensorflow jar包加入到build-path中。 2.在linux上进行部署时，需要将两个so文件部署到项目工程的java build path中，因为我们的工程中的path包含/usr/lib/，因此可以将两个so文件放到这个路径下。两个so文件主要是用于tensorflow 上层的api与底层操作系统native library进行通信的接口。 下面主要介绍一下如何使用java来调用模型预测。 首先列出用到 两个主要的操作对象： 12SavedModelBundle tensorflowModelBundleSession tensorflowSession SavedModelBundle为java侧与pb文件接口的对象，能够读取pb文件。而Session对应的是tensorflow中的会话对象，java中，tensorflow的预测操作也是需要在一个会话中进行的。 12tensorflowModelBundle = SavedModelBundle.load(tensorflowModelPath, "XXXX");tensorflowSession = tensorflowModelBundle.session(); 然后就是构造输入模型的数据了。同python中的情况类似，java侧的模型接收的数据类型必须为tensor类型，因此需要将数据转化为tensor。因此要用到Tensor对象的create方法来生成Tensor，假设当前我们处理好后的数值型输入数据为一个二维数组testvec： 1Tensor input = Tensor.create(testvec); 当然如果有其他输入的话，也要都转化为Tensor，简单说就是原来模型中feed_dict中的所有输入都要转化为Tensor对象。 然后就是调用session，输入需要的数据，然后调用具体的计算节点输出结果： 1Tensor output = tensorflowSession.runner().feed("input",input).feed(XX,XX).fetch("computation node_name").run().get(0); 这行代码有几个注意点： 1、feed方法返回的仍然是Runner对象，这个机制使得我们可以链式调用feed方法，将所有需要喂入模型的数据装载。 2、Runner对象的fetch方法是定位到具体的计算图中的计算节点（tensor），这个与python中调用模型预测的方法类似，需要在构造计算图的时候，对模型输出样本预测概率（或者logits）的tensor指定名称。 3、最后的get()方法则是获取返回的结果，这里我输入了参数0，是因为run()方法默认返回的是一个List，因为有可能有的需求会调用多个计算节点，因此会返回多个tensor，但是此时我只需要得到一个tensor结果，因此获取List中的第一个元素。 上述方法返回的是一个Tensor对象，为了输出结果，需要将它转化为原始的二维数据格式： 1float[][] resultValues = (float[][]) out.copyTo(new float[1][1]); 调用Tensor的copyTo方法，能够将Tensor转化为指定数据格式的数组。之所以是二维数组，是因为我们的输入数据是二维数组，虽然一次一般是预测一个样本，但为了开发的普适性，统一处理为二维数组，数组存储的就是该样本的预测概率。 最后有一点需要注意一下，在使用完模型后，需要将所有创建的Tensor关闭，销毁资源，当然这个是开发的一个好习惯，能够避免资源的泄露和低效利用。 12out.close();input.close();]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记系列（一）SGM for multi-label classification]]></title>
    <url>%2F2019%2F03%2F24%2F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89SGM-for-multi-label-classification%2F</url>
    <content type="text"><![CDATA[阅读论文，不能读完就过去了，要思考和记录论文的创新点和有用的思想。这个系列就是以尽可能的简单，尽可能少的文字去将一些核心的东西提取出来，方便自己以后查阅。 论文题目：SGM:Sequence generation model for Multi-label classification 论文target：文本分类，多标签分类，即一个文本样本分类标签会有多个。 论文intuition：多标签分类问题中，不同标签之间往往存在着一定的相关关系，使得每个标签并不是独立的。而传统的多标签分类中，通常是在最后一层对每个分类标签使用sigmoid_crossentropy_loss来计算损失，然后将损失求和，同时计算每个分类标签的概率，忽略了标签之间的关联性。比如kaggle上的toxic-classification比赛中，不同类别toxic标签之间的关联性是很大的，比如性别歧视和种族歧视等。 论文主要关注点：1、使用seq2seq的方式来解决多标签分类问题。该网络架构本身就很新颖。通过这种方式，能够在一定程度上建模标签之间的关联信息。结构如下： 其实，网络结构主体与经典的带attention的seq2seq是非常相似的。encoder并没有变化，主要改进点在decoder上。由于并没有target文本，我们需要建模的是不同标签之间的关系，因此将所有标签类别作为一个序列，假设有n中标签，那么decoder就有n个时刻。其中，每个时刻t的解码器（假设是一个rnn），一共接受三个不同来源的信息：前一个时刻t-1的rnn hidden state；encoder的attention上下文信息（attention的输入为encoder的隐层信息和decoder当前时刻t的隐层信息）；分类标签的global embedding信息。 备注： 论文在此处有一个不太对应的地方，在上图中，画出的是每个时刻t，decoder的rnn接收的是当前时刻的 $c_t$ ，但是按照attention本身的使用方法，以及论文中的公式： s_t = LSTM(s_{t-1},[g(y_{t-1});c_{t-1}])应当是先将前一时刻的attention信息和前一时刻的标签global embedding先拼接，然后在输入到rnn中。这与图上的画法不符，只能认为是作者的图画错了。另外，在作者给出的源码中，并没有将c信息与global embedding拼接操作，而是直接将global embedding输入到了rnn中。这个在github上有人提出了issue，但至今没有回应，不知道是什么原因。 2、masked softmax以及sorted label。这个是本文另一个重要的点。decoder中，每个时刻计算得到$s_t$后，如何得到标签呢？需要进行两步操作，第一步原图中并没有详细画出，我就补了一条线，即会在后面接一个类似于全连接层的计算，其接收的输入为当前decoder的$s_t$以及当前时刻计算得到的attention context $c_t$ : o_t=W_of(W_ss_t+W_cc_t)其中 $W_o,W_s,W_c$均为需要学习的权重参数矩阵。而f表示一个non-linear函数。 第二步为masked softmax，即上图中的MS。 y_t=softmax(o_t+I_t)使用mask的方式就是对上一步的输出 $o_t$添加一个mask vector $I_t$ ，这个向量的值可是有一定说法的，如果当前时刻输出的标签结果在前面t-1时刻中有出现过，则赋予 $I_t$ 一个极小值，否则则赋予零向量。至于这个极小值，官方源码中是赋予了-9999999999。 为什么要使用mask vector？论文说明是为了防止重复预测相同的标签，有时候预测错了标签会因此导致错误一直传递下去，而模型也不希望预测出重复的标签。 至于这样mask有用吗？虽然论文中做了相关的w/o实验，但是由于实验数据本身标签的量级不大，所以个人感觉并不能看出这个做法的有效性。这个还有待后续的验证。 另外还有一个注意点就是sorted label，即训练时，将每个样本的标签按照频率来排序，即出现较多的标签排在序列的较早时刻。这么做的目的在于，我们训练的loss还是使用交叉熵loss，每个时刻上的标签y都需要给定，因此其顺序也要给定。将频率较高的标签放在开头来训练，能够让模型提早学习对数据整体而言最有用的信息，如果一个标签出现次数很少，模型在一开始花了大力气学习它的性价比就不高，容易拖模型的后腿。 3、global embedding。这个可以说是该论文中第二重要的一个点。它的最大作用就是能让每个时刻都能编码记录前面t-1时刻的标签预测进程。传统的做法是直接使用上一时刻预测出来的结果 $y_{t-1}$但是这个结果是通过取所有标签概率分布中最大的那个结果，不一定就是正确的，如果错误，那么就会使这个错误传递。因此论文借鉴了LSTM中的关于门机制的思想，通过门的机制，将前面t-1时刻的所有预测出来的y信息都编码整合。具体做法： a. 为每个标签初始化一个embedding$e_i$ b. 根据decoder计算得到的标签的概率分布，以概率为权重，计算所有标签的加权和$\bar{e}$ ： \bar{e}=\sum_{i}^{l}{P(y_i)e_i}c.构建门H: H=\sigma(W_1e+W_2\bar{e})可以看到这个门是由embedding和加权和embedding共同控制的。 再次吐槽一下，这里论文并没有sigmoid函数的计算，个人认为应当是漏掉了，因为门机制本身就是控制0-1之间的通路的作用，且源码中，是有这一步的操作的。 d.使用门，计算得到最后的global embedding： g(y_{t-1})=(1-H)\odot e+H\odot\bar{e}注意，这里是向量element-wise乘。 通过上述方式，根据加权和$\bar{e}$ 以及门的控制，能够学习所有标签label的综合信息，防止模型在错误的道路上”一往无前“。 后记虽然， 这篇论文写得有一些问题，但是其中有一些思想是可以借鉴的。 1、使用seq2seq来建模多标签之间的相关关系。 2、使用门机制的思想，来综合考虑所有标签的信息，防止对某个标签预测错误的短视。]]></content>
      <categories>
        <category>论文总结</category>
      </categories>
      <tags>
        <tag>multi-label text classification</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN循环神经网络—梯度爆炸和消失的简单解析]]></title>
    <url>%2F2019%2F03%2F23%2FRNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AE%80%E5%8D%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[通过cs224n关于循环神经网络一章的内容，以及自己对其他相关论文和博客的研读后，发现对循环神经网络中BPTT（即backpropagation through time）中，关于产生梯度爆炸和梯度消失的数学推导不是太理解，因此自己翻阅了线性代数中关于雅克比矩阵的相关内容，经过一番推导，终于搞懂其中的原委。故总结一下，若有错误，请各位大牛指正。 雅克比矩阵先百科一下什么是雅克比矩阵。引用维基百科上的定义，the matrix) of all first-order partial derivatives of a vector-valued function。首先，他是一个矩阵，其次矩阵的元素是一个一阶函数的偏导数，最后这个一阶函数的偏导数的对象是一个向量函数。 举个例子，大家应该就能明白这个定义意思： 假设有如下函数排列： 可以将$y_1…y_n$组成一个向量Y，其shape为$n_1$,同理$x_1…x_n$也组成一个向量X，其shape也为,可以$n_1$看到该平方函数就是一个对向量X的一个向量函数，而其雅克比矩阵可以写成如下形式：（公式比较难看，望包涵） 可以看到除了对角线上的所有矩阵元素值都是0，而对角线上的矩阵元素值就是对应y与x的一阶偏导数值。 RNN中的梯度消失和梯度爆炸问题RNN的BPTT，很多博客包括中都详细推过了，这里我不做重复说明，我只聚焦到其中的一点，即梯度消失和爆炸的问题探讨。 首先，我使用一下cs224n中，对该问题的所有notation。 该公式表示了RNN的隐藏层的计算过程，其中，h表示隐藏层的输出，W（hh）表示隐藏层到隐藏层的权重矩阵。（是个方阵）在梯度反向传播过程中，需要计算损失函数对W权重矩阵的梯度，可以得到如下公式： t表示时刻，E表示损失函数，该公式表示将所有时刻t的权重偏导数求和，得到这个序列的最终权重偏导，继续使用链式推导，可以得到： 其中，损失函数E对y的偏导数、y对ht的偏导数，以及h对W的偏导数都能很容易的求得，除了中间的ht对hk的偏导数。我们下面就重点关注这一项的求解。该式子最终可写成： 可得知，我们最终需要求的是某时刻的隐藏层值h对上一时刻隐藏层值h的偏导数在所有时刻上的和。 将这个式子再用链式法则拆开一下，令 得到： 其实，从上面的式子可以得知，等式右边的两个乘子项分别是两个不同的雅克比矩阵，因为$h_{t-1}$本身是一个向量函数，而$z_t$是对$h_{t-1}$的向量函数，因此分别将两个乘子式化为雅克比矩阵： 这个矩阵怎么求？回想一下我在上一章介绍雅克比矩阵时举的例子，其实可以类比到这个矩阵，其中$h_t$是一个$n_1$维的向量，而$z_t$也是一个$n_1$维的向量，写成函数排列可得： 简单的说，就是该雅克比矩阵中，某一行或某一列上，只有一个偏导数是非0的元素，即该矩阵除了对角线元素，其他元素都为0，因此可写成： 该矩阵是一个对角矩阵，用diag来表示，其中对角相当于一个向量，每个元素是h对z的偏导数，h与z的关系映射函数是我们网络中的激活函数，这里用 $f’(z_t)$表示h对z的偏导，则上述可写成：$diag(f’(z_t))$ 现在关注一下另一个雅克比矩阵： 同样写成函数排列形式： 看到这个函数排列式，是不是对上面的雅克比矩阵求解比较清晰了呢？是的，该雅克比矩阵的每个元素都是对应W权重矩阵的对应元素，即 最终，可得到： 可以得知，当序列长度越长，对一个序列进行BPTT，每次时刻多有对 的连乘操作，即相当于W权重矩阵的连乘，刚开始的几层可能影响不大，因为连乘较少，但是当越往前传播，W连乘的级数是急剧上升的，若W矩阵初始化不当，小于1或大于1，都将会使得梯度计算朝着接近于0或者无限大的趋势发展。而我们的激活函数sigmoid或tanh，在梯度很大或很小时的曲线都是很平滑的，很容易导致越往后训练，梯度几乎不变。因此产生了梯度消失和梯度爆炸问题。 reference： 1、cs224n公开课 2、Jacobian matrix and determinant]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
</search>
