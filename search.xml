<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PCA算法之特征值分解]]></title>
    <url>%2F2019%2F04%2F08%2FPCA%E7%AE%97%E6%B3%95%E4%B9%8B%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前面一些文章主要对一些算法模型进行了一些总结，牵扯到概率统计的知识比较多，涉及线性代数的知识相对来说较少，且在以往参加的一些数据竞赛中，经常使用PCA方法来对数据进行一个二维或者三维的可视化，使得我能够对高维数据的分布有一个大致了解。然而，对PCA的具体实现原理也止步于了解到其协方差矩阵进行特征值分解或者奇异值分解，具体的特征值分解和奇异值分解到底是什么？为什么能够满足PCA的要求？这些问题还需要我深入探究。因此本文将着重对线性代数中的特征值分解进行一个深入的探究，以完善自己对PCA的理解。下一篇文章会对奇异值分解做一个简单的总结描述。 奇异值分解和特征值分解都是对矩阵进行分解的不同方法，所谓分解，即使将原矩阵拆分成另一种表达方式，且新的表达方式比原矩阵更简单，更具有实际意义。奇异值分解对象可以使任意nm维的矩阵，而特征值分解只能针对nn的方阵，特征值分解从某种意义上来说可以算是奇异值分解的一种特殊情况，所以下面先对特征值分解进行一个描述。 特征值分解首先，我回顾一下以前大学学过的关于特征值和特征向量的内容。 什么是特征值和特征向量？ 一个n*n的方阵A的特征向量指的是一种向量v，该向量与A相乘后相当于是对该向量进行数值上的缩放操作，且该向量是非零向量，用公式表示就是： $Av=\lambda v$ 。其中，标量 $\lambda$ 称为这个特征向量对应的特征值。 之前大学学线性代数的时候，学完了上面的定义描述后，就学习了怎么计算特征值和特征向量，然后就直接给出了特征分解的定义和矩阵的相似对角化内容。当时我还没办法将这些知识串联起来。直到最近看了MIT的线性代数公开课以及3Blue1Brown的关于线性代数的科普视频，才发现原来向量和矩阵是具有几何上的意义的，对应的特征向量和特征值也是有对应的几何意义的。通过几何角度去学习特征向量和特征值的内容，可以帮助我们对矩阵和向量有更直观的理解。 矩阵的几何意义 在几何意义上，每个矩阵都代表了某种变换。为了描述方便，下面我都会以二维举例，三维甚至更高维的情况与二维的原理相同，只是维数更高，不好可视化。假设现在有个2*2的矩阵A： $\left[ \begin{matrix} 3 &amp; 1 \ 0 &amp; 2 \ \end{matrix} \right] $,这个矩阵可以有多种解释，我在大学学习线性代数时，通常老师只会以行向量的角度去看，比如解该矩阵对应的线性方程组，会写成： $\begin{equation} \left\{ \begin{aligned} 3x_1+x_2=a\ 2x_2=b \end{aligned} \right. \end{equation}$ ,其实要从几何意义上了解一个矩阵，从列向量的角度上去看会比较好，此时上述方程组可以写作： $\left[ \begin{matrix} 3 \ 0 \ \end{matrix} \right]x_1+\left[ \begin{matrix} 1 \ 2 \ \end{matrix} \right]x_2=\left[ \begin{matrix} a \ b \ \end{matrix} \right] $，其中 $x_1$ 和 $x_2$ 分别表示二维向量 $\left[ \begin{matrix} x_1 \ x_2 \ \end{matrix} \right] $的两个维度。而在二维平面系中，任意一个向量可以由两个基向量线性表示，我们日常使用的基向量为 $\left[ \begin{matrix} 1 \ 0 \ \end{matrix} \right]$ 和 $\left[\begin{matrix} 0 \ 1 \ \end{matrix} \right]$ ,分别对应坐标系的x轴上的单位向量，和y轴上的单位向量。如图： (备注：基向量可以理解为能够线性表示该维度平面上的任意一个向量，因此基向量不唯一，这里举的例子纯属习惯。) 因此平面上的任意向量都可以用 $\left[ \begin{matrix} 1 \ 0 \ \end{matrix} \right]x_1+\left[ \begin{matrix} 0 \ 1 \ \end{matrix} \right]x_2$ 得到。而上面提到的方程组，现在可以很清晰得看到它相当于是将x轴上的基向量增长到3倍，而y轴上的基向量变换略微复杂，是一种shear操作，形象得说相当于将一条垂直的线倾斜一定的角度，变换后的基向量变为了 $\left[ \begin{matrix} 3 \ 0 \ \end{matrix} \right] $和 $\left[ \begin{matrix} 1 \ 2 \ \end{matrix} \right]$ ,如图所示： 当基向量改变时，原来的由基向量线性表示的向量也会发生改变，因此整个平面的网格都会发生变化，上面的例子中就发生了这种变化，但是这个变化有两个不变，一个是本来是原点的位置，变换后还是原点；另一个是网格之间的相对距离仍然没有变化。这种变换通常我们叫做线性变换。其实矩阵的线性变换有专门的定理和证明，要满足可加性和乘子性，具体的就不展开了，有兴趣的同学可以深入研究。（简单说就是线性变化f,要满足f(A+B)=f(A)+f(B),以及f(kA)=kf(A)，其中k为实数。就像我之前说的，作为一名非科班的人员，需要对数学机理有一定了解，但是切忌太过深入其中，出不来就不好了。） 说了那么多，还没扯到特征向量是怎么回事？别急，马上就讲到了。一般一个向量在经过一个线性变换后，不大会维持在原向量所张成的空间上的，这里又出现个新名字张成（span）,这个概念说实话我在看《线性代数应该这样学》时，很头疼，这边大家可以理解为一个向量张成的空间为这个向量的延长线，如图： 它经过上述线性变换后，与原向量不共线。 图中，绿色的为原向量，而蓝色的为线性变换后的向量。两个向量并不共线。但是有时候会存在这样的一个向量，它在线性变换后，仍然处在原向量张成的空间上，上述例子中有一个很明显的例子，就是位于x轴上的向量，在线性变换后，仍然在x轴上。这样的向量我们就称之为特征向量，而该向量相对于原来向量的缩放比例则称为其对应的特征值，值得注意的是特征值可以为负数，表示的是特征向量与原来的向量的方向相反。这个几何意义可以与我们一开始介绍的公式相对应，等式左边表示 $A\vec v $表示对向量 $\vec v$ 进行一个矩阵A对应的线性变换，等式右边表示该向量做 $\lambda$ 的缩放。 关于特征向量的解法，相信大学都教过，就是得到 $(A-\lambda I) \vec v=\vec 0$ ，下面的过程可能大家就不太明白里面的意思了，即令 $|A-\lambda I|=0$ ，然后根据行列式求 $\lambda $的值。为什么令行列式为0就可以满足上个等式呢？这里牵扯到行列式的几何意义，这里就提一下，一个矩阵的行列式相当于是将一个坐标系变换后得到的新的基向量所围成的四边形的面积（二维下）。上图中，相当于是一个单位菱形格的面积。那么要使得向量 $\vec v$ 经过变换后，为零向量，就要使得我的变换后的单位菱形格面积为0，即对应的我的变换矩阵的行列式为0。 特征向量的用处在哪里呢？首先根据上面的描述，一个矩阵的特征向量通常是那些经过矩阵变换后，在方向和位置上仍然没有发生变化的向量，而其他向量或多或少都发生了一些偏移，因此特征向量能够帮助我们很好得去描述一个矩阵。 其次，了解下这样一个矩阵： $\left[ \begin{matrix} -1&amp;0 \ 0&amp;2 \ \end{matrix} \right] $，该矩阵的特征向量可以很轻易就能看出来，分别为原来的基向量 $\left[ \begin{matrix} 1 \ 0 \ \end{matrix} \right] $和 $\left[ \begin{matrix} 0 \ 1 \ \end{matrix} \right] $,其对应的特征值分别为矩阵对其各自维度的缩放程度。此时我们的基向量就是该矩阵对应的特征向量。 如图红色实线为y轴方向原基向量，绿色实线为x轴原基向量，对应颜色虚线为线性变换后的基向量。其实这种矩阵有特殊性，比如除了其主对角线元素外的所有元素都是0，这种矩阵我们称之为对角阵。为什么要讲到这个矩阵？它可是有大用处。根据上面的描述，可以知道对角阵的特征向量是该维度空间上的基向量。对角阵有以下便利的性质： 如果对一个向量做n次相同的对角阵的变换，即多个相同的对角阵的连乘，按上面的例子,可得：$\{\prod_{}^{n}\left[ \begin{matrix} -1&amp;0 \ 0&amp;2 \ \end{matrix} \right] \}\left[ \begin{matrix} a \ b \ \end{matrix} \right]=\left[ \begin{matrix} (-1)^na \ 2^nb \ \end{matrix} \right] $，最后的结果就是向量对应的维度元素与变换矩阵的特征值的n次方的乘积。 如果是普通的非对角矩阵，那么做n次连续的矩阵变换，要获取最后变换得到的向量是需要庞大的计算量的，且非常复杂。 因此基于上述性质，可以利用对角阵的这种优点，来将原始矩阵进行转换化简，将其与对角阵建立某种变换关系，使得我对原始矩阵的计算可以转换到对角阵上，最后化简我们的计算。这就是相似对角化的内容。 备注：一个矩阵是否能相似对角化是有条件的，即能够找到这样一组基，使得该矩阵对应的线性变换对于该组基上的每个向量都是一个缩放动作。 特征值分解 为什么还要讲相似对角化？首先截图一下百度百科中关于特征值分解的定义： 其实这个定义可以与上面的描述对应，首先能够特征值分解的矩阵必须是方阵，其次具有N个线性无关的特征向量，由于n维向量空间中，n个线性无关的不同向量可以作为该空间中的一组基，使得该空间中任一向量都可以用这一组基线性表示，这个也是A需要满足能够相似对角化的条件。此时，A可以相似对角化为一个对角矩阵。下面探究特征分解等式的几何意义。 首先明确一个概念，基变换。这里我简单说明：以二维空间为例，存在着多个不同的基向量组，因此我们二维空间中的任意向量都可以用不同的基向量来表示，虽然是同一个向量，但是用不同基表示时，得到的表示值会不一样。举例：以 $\left[ \begin{matrix} 1 \ 0 \ \end{matrix} \right] $和 $\left[ \begin{matrix} 0 \ 1 \ \end{matrix} \right]$ 为基，向量 $\left[ \begin{matrix} 3 \ 2 \ \end{matrix} \right] $可以表示为 $3\left[ \begin{matrix} 1 \ 0 \ \end{matrix} \right]+2\left[ \begin{matrix} 0 \ 1 \ \end{matrix} \right] $。但是若用另外一组基： $\left[ \begin{matrix} 2\ 1 \ \end{matrix} \right] $和 $\left[ \begin{matrix} -1 \ 1 \ \end{matrix} \right] $为基，上述向量应该表示为 $\frac{5}{3}\left[ \begin{matrix} 2\ 1 \ \end{matrix} \right]+\frac{1}{3}\left[ \begin{matrix} -1\ 1 \ \end{matrix} \right] $，此时新的基 $\left[\begin{matrix} 2\ 1 \ \end{matrix} \right]$ 和 $\left[ \begin{matrix} -1 \ 1 \ \end{matrix} \right] $其实扮演的是原来 $\left[ \begin{matrix} 1 \ 0 \ \end{matrix} \right]$ 和 $\left[ \begin{matrix} 0 \ 1 \ \end{matrix} \right] $的作用，但是新的向量表示确变成了 $\left[ \begin{matrix} \frac{5}{3}\ \frac{1}{3} \ \end{matrix} \right]$ ，虽然向量还是那个向量，但是确实不同坐标系下的表示。这时矩阵 $\left[ \begin{matrix} 2&amp;-1\ 1&amp;1 \ \end{matrix} \right] $其实代表了一个基变换。此时如果我们已知某坐标系有一个线性变换矩阵，我们想知道该线性变换在另一个坐标系上的样式，就可以使用这个基变换，假设线性变换的矩阵为$ \left[ \begin{matrix} 0&amp;-1\ 1&amp;0 \ \end{matrix} \right] $,是对A坐标系的线性变换，假设有向量 $\vec v$ 是在B坐标系下的表示，$ \left[ \begin{matrix} 2&amp;-1\ 1&amp;1 \ \end{matrix} \right]$ 可以将B坐标系转换到A坐标系，具体转换步骤如下: 首先利用基变换矩阵，得到向量在A坐标系的向量表示： $\left[ \begin{matrix} 2&amp;-1\ 1&amp;1 \ \end{matrix} \right]\vec v $ 然后在新的A坐标系上，对向量进行矩阵的线性变换： $\left[ \begin{matrix} 0&amp;-1\ 1&amp;0 \ \end{matrix} \right]\left[ \begin{matrix} 2&amp;-1\ 1&amp;1 \ \end{matrix} \right]\vec v $，得到了A坐标系中转换后的向量。 最后需要将A坐标系的新向量转换回到B坐标系，因此对步骤2的得到的向量进行逆基变换： $\left[ \begin{matrix} 2&amp;-1\ 1&amp;1 \ \end{matrix} \right]^{-1}\left[ \begin{matrix} 0&amp;-1\ 1&amp;0 \ \end{matrix} \right]\left[ \begin{matrix} 2&amp;-1\ 1&amp;1 \ \end{matrix} \right]\vec v$ ,最后得到我想知道的变换后的向量在B坐标系上的向量表示。 利用上述基变换的概念，我们可以将原始坐标系转换为用线性变换矩阵的特征向量组成基的坐标系。 还是以矩阵 $\left[ \begin{matrix} 3 &amp; 1 \ 0 &amp; 2 \ \end{matrix} \right] $为例子，它的特征向量分别为： $\left[ \begin{matrix} 1\ 0 \ \end{matrix} \right]$ 和 $\left[ \begin{matrix} -1\ 1\ \end{matrix} \right] $，此时将两个向量组合成的矩阵 $\left[ \begin{matrix} 1&amp;-1\ 0&amp;1 \ \end{matrix} \right]$ 作为我们的基变换矩阵，（我们的目的就是要将原始坐标系转换为我们用特征向量作为基的坐标系），然后运用上述基变换过程，得到一个综合的转换矩阵： $\left[ \begin{matrix} 1&amp;-1\ 0&amp;1 \ \end{matrix} \right]^{-1}\left[ \begin{matrix} 3 &amp; 1 \ 0 &amp; 2 \ \end{matrix} \right]\left[ \begin{matrix} 1&amp;-1\ 0&amp;1 \ \end{matrix} \right] $. 这样一个转换矩阵应该得到一个对角阵，原因在于在这个新的坐标系中，基就是该矩阵对应的特征向量，因此它对应的线性变换只是对两个基向量做了缩放，并没有改变其原本的轨迹。之前已经说到了对角阵的好处，是不是与这个有惊人的吻合！对角阵的主对角元素为对应的特征向量的特征值，即线性变换矩阵 $\left[ \begin{matrix} 3 &amp; 1 \ 0 &amp; 2 \ \end{matrix} \right]$ 对其特征向量的缩放程度，在这里，对角阵为 $\left[ \begin{matrix} 3 &amp; 0 \ 0 &amp; 2 \ \end{matrix} \right]$ 。这样通过矩阵的运算，我们可以将一个矩阵转换为对角阵的表现形式： $\left[ \begin{matrix} 3 &amp; 1 \ 0 &amp; 2 \ \end{matrix} \right]=\left[ \begin{matrix} 1&amp;-1\ 0&amp;1 \ \end{matrix} \right]\left[ \begin{matrix} 3 &amp; 0 \ 0 &amp; 2 \ \end{matrix} \right]\left[ \begin{matrix} 1&amp;-1\ 0&amp;1 \ \end{matrix} \right]^{-1} $ 最后的计算过程是不是与我一开始给出的特征值分解公式吻合！此时一个矩阵的任意次连乘得到极大的简化。（具体计算不讲了，总之在连乘过程中，右边式子可以得到无数个单位矩阵连乘，而对角阵本身的连乘是很简单的。） 最后看一下这个特征分解的形式，中间的对角阵其实告诉了我们这个线性变换的变换程度，而两边对应的基变换矩阵中的特征向量告诉了我们线性变换的变换角度等性质。这样一个比较复杂的矩阵变换通过特征分解，就能得到很好的解释。这也是PCA中能够使用该方法的原因之一。 总结虽然特征分解很好用，但是也有其缺点，最重要的就是它不是对所有矩阵通用，需要满足一定的条件，否则就无法进行分解。因此有另外一种分解方法叫做奇异值分解，它能够对任意维度的矩阵进行分解，并且能够帮助我们删选不需要的特征维度。因此PCA主要使用的还是奇异值分解。之所以讲了这么多特征值分解，其实为了奇异值分解做知识的铺垫。下一篇将继续讲解奇异值分解。 后记： 我说这篇文章我断断续续写了两个星期你敢信？？！！线性代数不愧是我的苦主，在大学期间是我比较头疼的一门课，当时学的也是不扎实。但是在钻研机器学习和深度学习时，发现线性代数的重要性，同时看了著名博主3Blue1Brown的线性代数小视频，顿时感觉打开了新世界的大门，因此狠下心来对线性代数做一个新的角度的学习。当然学习深度肯定不能和科班同学相比，只是达到能在研究模型算法时，遇到相关知识点能够知其所以然足矣。 reference: 百度百科 3Blue1Brown的视频]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>linear algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法总结]]></title>
    <url>%2F2019%2F04%2F08%2FEM%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[最近因工作需要，要学习概率图模型，来对一些序列对象进行一些建模。概率图模型根据不同情况的不同概率假设，能够有效解释一些现实问题，使得我们的模型能够在一定程度上拟合现实的问题，但是存在模型不确定、参数不确定，建模比较难的特点。因此有很多思想方法帮助我们去进行参数估计和建模，本文准备将EM算法总结一下。 什么是EM算法EM算法全称Expectation-Maximization算法，即最大化期望算法。根据这个名字就能很清楚知道这个算法的核心元素，一个是期望，另一个是期望的最大化。而这两个元素代表了EM算法的两个流程。首先EM算法是一个迭代性质的算法，即每个迭代都能得到新的经过学习后的参数，迭代的结束标志一般是人为定义阈值或者参数收敛。其次EM算法每个迭代主要执行两个步骤： 求期望，得到一个关于参数的期望函数式。类似于我将样本数据输入到 对期望函数式求使其最大化的参数。 上述描述比较简略，下面会详细将这两个步骤说明。 为什么用EM算法EM算法的好处，或者说它被需要的原因是什么？其实之前我已经在HMM模型的参数估计章节中，对EM算法的具体应用已经有了一个专门的描述，感兴趣的同学可以去看我之前的文章：《隐马尔科夫模型学习总结之三》。从HMM的例子中，我们知道当一个实际问题中，除了可观测的变量外，还需要一些隐变量帮助我们去建模，此时EM算法可以帮助对这种包含隐变量的问题进行参数估计，除了HMM外，还有k-means聚类算法也运用了EM算法，它的隐变量可以说是具体的那些聚类本身。 除此以外，我们会存在无法确定问题对应的具体的模型的样式的时候。说的有点拗口，我举个例子，大家感受一下。 logistics regression的二分类例子 首先，我举个能确定具体模型的例子，即使用logitstic regression对二分类数据进行建模，其中样本数据之间是相互独立的，没有关联，且当前数据标签只有两种0和1。其实这种数据分布可以看成是一个二项分布，即重复n次独立的伯努利试验。在每次试验中只有两种可能的结果，而且两种结果发生与否互相对立，并且相互独立，与其它各次试验结果无关。对应于我们的数据就是每个样本相当于一次抛硬币，要么正面要么反面，且两次抛硬币（两个数据样本）之间没有任何关联，互不影响。那么我们假设模型参数为 $\theta $,模型为 $h(x,\theta)$ 样本数据集为 $\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\} $，m表示m个样本，那么我们给定参数的情况下，每个样本数据标签的条件概率为： $P(y_i|\theta)=h(x_i,\theta)^{y_i}(1-h(x_i,\theta))^{(1-y_i)}$ ,这个式子其实是两个式子的合并，我们知道logistics regression是生成样本属于标签1的概率，因此有如下两个式子： $P(y_i=1|\theta)=h(x_i,\theta) $， $P(y_i=0|\theta)=1-h(x_i,\theta) $，两个式子合并，就成上面的式子。 由于样本数据之间相互独立，因此整个样本集的预测目标概率为： P(y|\theta)=P(y_1|\theta)P(y_2|\theta)...P(y_m|\theta)我们训练模型的利用已知的样本数据，反推最大概率能够得到样本数据的模型参数。因此对于这种已经确定模型形式，需要进行参数估计的问题，我们很自然想到使用极大似然估计方法MLE，即 $argmax_{\theta}P(y|x,\theta) $。由于连乘式的求极值很好不做，且概率的多次连乘，尤其是在数据样本很大的情况下，很容易造成最后结果几乎为0，因此考虑在概率式子之前增加log函数，由于log函数的单调性，保证了原来概率式的性质，且可以将连乘式化简为求和式。 然后就是以 $\theta$ 为参数，对 $L(\theta)$ 求偏导数，并令偏导为0，求出 $\theta$ 的表达式。 上述的例子说明只要是数据分布确定后，求具体数据分布的参数是比较容易的，一般是使用MLE方法。然而现实生活中存在很多数据分布无法确定的情形，如之前讲的HMM中的问题，因为它引入了一个隐变量，带来了不确定性，因此不能简单地使用MLE方法，一步到位得将我们的参数估计出来，因此需要EM算法，迭代得去估计我们的参数。 高斯混合模型应用EM算法举例EM算法问题描述 下面准备拿很多人都举过的高斯混合模型的例子来讲解EM算法，参考了cs229以及徐亦达老师的讲义。 假设当前数据符合上述分布，很明显，该数据不是一种概率分布就能简单得描述的，大致可以看出该数据有三个不同的概率分布，每个概率分布可先用高斯分布来解释（分布中心数据密度大，周围数据密度小，与高斯分布特征相符）。高斯分布的概率密度公式如下： 其中， $\mu$ 表示期望， $\Sigma $表示方差。在混合高斯模型中，假设有k个高斯分布混合，概率密度公式为： 其中，有约束条件： 因此参数集合为： 若使用MLE方法来进行参数估计，则极大似然估计函数为： 如果单纯使用求偏导的方式来求极值的参数是很难的，无法简单做到。 此时，由于我们不知道这些数据具体从属于哪个概率分布，因此引入一个隐变量Z，表示数据属于哪个高斯分布。原始的MLE函数式为： 这个函数式无法一次使用求偏导的方法求出极值，因此需要迭代的方法，每个迭代产生参数估计。然后我们引入隐变量Z，在不改变原式子的情况下，我们可以采用贝叶斯公式以及期望积分的方式，将隐变量Z引入上式（之前讲HMM的时候已经讲过这种方法，感兴趣的同学可以回头去看一下），因此得到EM算法每步迭代的问题描述式： 首先这是某步迭代的函数式，假设之前的迭代已经估计出了参数 $\theta^{(g)}$ ,因此在此式中 $\theta^{(g)}$ 是一个已知的常数值，这个在后面化简推导的时候很有用。其次在外层增加了对隐变量z的积分，结合内部的贝叶斯条件概率公式，可以将z的影响消除。 备注：隐变量引入不是随便的，有一些要求，比如需要引入后简化模型的求解，另外不能改变原边缘分布，即 $P(x)=\int_{z}^{}P(x|z)P(z)dz$ ,在高斯混合模型中，引入的隐变量z表示对应数据属于哪个高斯，因此P(z)属于先验知识，来自于上面提到的 $\alpha_l$,表示高斯分布的权重。因此可以很容易证明 $P(x)=\int_{z}^{}P(x|z)P(z)dz$ ，这边就不写了，有兴趣的同学自己推导，很简单. EM算法迭代收敛性证明 既然是迭代的方法，那么势必要保证这种方法最后能够收敛，事实上EM算法的收敛性是可以证明的，下面简单对这个收敛性做一个证明。即证明： 首先根据条件概率公式，有： 等式两边同时加log，得到： 等式左边引入隐变量z，并对z做积分的期望，右边同理，可得： （上述使用了这个等式： $\int_{z}^{}P(z|x , \theta^{(g)})=1 $） 我们将上面等号右边的被减子式定义为 $Q(\theta,\theta^{(g)})$ ,减子式定义为 $H(\theta,\theta^{(g)})$ ，因此$log(P(X|\theta))=Q(\theta,\theta^{(g)})-H(\theta,\theta^{(g)})$ ,我们要证明 $log(P(X|\theta^{(g+1)}))\geq log(P(X|\theta^{(g)})) $，可以看到我们的Q项其实与我们一开始提出的EM算法问题描述式是一样的，即每个迭代， $\theta^{(g+1)}$ 都是在Q项上求极大值后的参数，因此$Q(\theta^{(g+1)},\theta^{(g)})\geq Q(\theta^{(g)},\theta^{(g)})$ 是必然成立的（因为本来就是求极大值，因此极大值只可能比上个迭代大或者一样）。那么我们接下来只要证明H项在每个迭代都在逐渐减小或者不变，即： $H(\theta^{(g+1)},\theta^{(g)})\leq H(\theta^{(g)},\theta^{(g)}) $ 证明：我们将要证明的不等式做一个转换，问题可以转换为： 对任意的 $\theta$ ，我们要证明$H(\theta,\theta^{(g)})\leq H(\theta^{(g)},\theta^{(g)}) $，有如下化简： 其中-log()是一个凸函数，函数图如下： 对于凸函数我先介绍一个性质：Jensen不等式： 我们引入一个变量 $t\in(0,1)$ ，假设当前数据区间为 $[x_1,x_2] $，那么该区间内任意一个数都可以用如下式子表示： $(1-t)x_1+tx_2$ 。而将函数曲线的两端的点连接构成一条直线，该直线上的点可以表示为：$(1-t)f(x_1)+tf(x_2)$。那么在函数为凸的情况下，根据图上的示例（虽然比较难看），可以看出 $(1-t)f(x_1)+tf(x_2)\geq f((1-t)x_1+tx_2) $，这个就是Jensen不等式，应用到我们这边证明，其实可以泛化为不等式左边的式子可以看成是函数的期望，不等式右边看成是期望的函数，其中t，1-t都是某个函数值的概率，因此可以写成 期望的函数小于等于函数的期望 ，即 而 是函数的期望，因此它比大于期望的函数： 因此，证毕。 使用EM算法解决高斯混合模型问题 下面定义问题描述式中的具体项： 其中 $N(X|\mu_l,\Sigma_l)$ 表示一个高斯分布， $\alpha_l$ 表示这个高斯分布的权重。k表示有k个不同的高斯分布，n表示数据样本数。 定义 $P(X,z|\theta) $，因为数据间独立，且有贝叶斯公式： 因此有： 定义 $P(z|X,\theta) ​$，根据数据独立性，贝叶斯公式和积分期望，有如下式子： 因此可定义 $P(z|X,\theta)$ 为： 将上述两个定义式代入之前的Q项中（H项已不需要），开始EM算法中的E-step： E-step： 将式子 定义为 $f_i(z_i) $,将式子 定义为 $P(z_1,….z_n) $。 对上述式子肯定是要进行化简的： 把上述式子展开，其实是一个包含N个sum项的式子，我们把第一个sum式拿出来单独分析： 上式中，后面对z2…zn的积分式，其实可以看做是z1变量的边缘概率分布，因为其他z都被积分积掉了（原谅我用这么口语的方式表达），因此上式可化简为： 很明显，其他sum项也可以做这样的化简，因此对所有N个sum项都做如此化简后，可得如下式子： 最后得到的式子中，加号前面的只包含参数 $\alpha$ ，加号后面的只包含参数 $\mu,\Sigma$ ，因此可以单独对两个式子分别求极大值，然后求得本次迭代的参数。下面就是我们的M-step。 M-Step： 主要是开始对E-Step构造的式子进行参数最大化，按照上面说的，将式子根据加号一拆为二，首先估计参数 $\alpha $。对式子求偏导，并令偏导为0，得到： 说明一下，为了描述方便，这里定义 $z_i=l ​$。 这个式子求解方法已经很熟悉了，之前SVM以及HMM中都讲过这个，就不多说了，直接使用拉格朗日乘子法，得到： 接下来就是求参数 $\mu,\Sigma$ 。这个推导比较复杂，因为牵扯到一些矩阵向量的求导的trick和小公式，先把公式列出来，有同学感兴趣，我再把这些公式的推导过程列出来： 我们对Q项加号的第二项进行分析，即： 首先，我们对参数 $\mu$ 进行估计，我们将下面的高斯分布的概率密度公式代入。 其中， $\Sigma$ 的多项式在此式可以视作一个const最后，对式子求偏导，令偏导为0，得到： 接下来是对 $\Sigma $的推导，为了推导简便，我们改变求偏导的变量对象，转而对 $\Sigma^{-1}$ 求偏导，这样利用上面的一些facts（fact2），然后得到： 这样我们就将所有本迭代的参数求出来了，细心的同学可以发现所有参数里面都包含一个式子： $P(l|X_i,\theta)$ ，这个式子我们在之前的推导中已经得到，为： 因此，使用这个式子，加上迭代的方式，就可以一步一步逼近最优解。 备注：对于EM算法来说，有一个非常重要的一个地方就是它不能保证对每个应用场景的问题都能都到全局最优解，如果我们的目标函数是凸函数（本文我们讨论到的-log()），那么可以保证得到全局最优解。但是，若我们的目标函数不是凸的，比如在聚类问题中，若对文本分类中的余弦相似度计算函数就不能保证是凸的，此时EM算法就有可能给出局部最优。 reference: cs229 notes 徐亦达 notes 《数学之美》 吴军]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机SVM总结之soft-Margin与SMO]]></title>
    <url>%2F2019%2F04%2F05%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E6%80%BB%E7%BB%93%E4%B9%8Bsoft-Margin%E4%B8%8ESMO%2F</url>
    <content type="text"><![CDATA[上一章节总结了核方法在SVM中的应用，能够在一定程度上解决原数据线性不可分的问题。另外，还有一种方法，相对于核方法来说，也能解决数据的线性不可分问题，它能够让SVM，模型在一定程度上包容不能被正确分类的点，且理解起来比核方法较为简单，这种方法叫做soft-Margin。本章节就对这个soft-margin方法进行探索，同时研究一个经典的SVM的实现算法SMO模型。 Soft-Margin SVMSVM模型对数据是否线性可分是有要求的。所以有了用核方法将数据映射到高维特征空间上，来提升数据线性可分的可能性。但是总有一种可能是无论怎么映射，终将无法将得到线性可分的数据；另外，有时候样本数据中是存在异常数据点的，如果此时硬是要将所有点分类正确的话，将会导致模型过拟合，对异常数据的鲁棒性就较差。如下图： 因此，基于上述问题，可以对我们原来的SVM最优化目标函数使用regularization的机制，增加一个l1的正则化，如下： 与之前的hard-SVM的优化目标函数对比： 原始情况是当 $y(w^Tx+b)&gt;0$ 时，表示该数据样本分类正确，当 $y(w^Tx+b)1$ ，可以允许模型分错一些数据，不像之前的模型那么“固执”。再看约束条件，现在约束条件要求数据样本的函数间隔小于1就可以了，放宽了要求，但同时对这些样本数据是有一个惩罚的，惩罚的结果就是在目标函数的cost上，再增加$C\xi_{i}$ 的惩罚。参数C是一个权重控制参数，它在使得 $||w||^2$ 尽量大（即间隔尽量小，第一章讲解过原理）与保证绝大多数数据样本点的函数间隔至少为1两个要求之间维持一个平衡。 接下来就是按照之前解决hard-SVM的套路，推导得到其对偶问题形式。照例，我们写出原式的拉格朗日乘子式： 对w求偏导，并使偏导为0得到： 对b求偏导，并使偏导为0得到： 对 $\xi_{i}$ 求偏导，并使偏导为0得到： 将上面所有得到的表达式反代入L中，得到： 因为 $\gamma_{i}=C-\alpha_{i}\geq0 $,所以 $0\leq\alpha_{i}{}\leq C$ 。 因此可以得到带约束条件的对偶问题形式为max(min(L))，即： 可以看出，这个对偶问题与之前的hard-SVM的对偶问题形式很相似，唯一区别的地方在于 $\alpha_{i}$ 的约束条件不一样了。因此对应的KKT条件也不一样，下面来推导一下这个KKT条件（主要是推导KKT中的补充条款，其他条件很容易就能看出来）。 首先构造 $g_{i}(w)=1-\xi_{i}-y^{(i)}(w^Tx+b)\leq0$ ,根据KKT补充条款中：不等式约束函数与对应的拉格朗日乘子的乘积应当为0，以及原始问题中，同时存在两个不等式约束，因此应有两个KKT补充条件如下： 同时我们备注一个概念，即分隔超平面上下两边margin距离为1的区域称为margin内部，而距离大于1的区域称为margin外部。 当 $\alpha_{i}=0$ ，此时 $\gamma_{i}=C $ ， $\xi_{i}=0$ ，则没有任何惩罚项，所以 $y^{(i)}(w^Tx+b)\geq1$，满足这个条件的样本点对应的是margin外部两边分类正确的点，不需要惩罚，其中 $y^{(i)}(w^Tx+b)=1$ 的点对应在margin线上的点。 当 $\alpha_{i}=C $， $g_{i}(w)=0$ ，此时 $y^{(i)}(w^Tx+b)=1-\xi_{i}\leq1$ ，满足这个条件的样本点对应的是margin内部分类错误的点，此时是需要对这些点进行惩罚的。 当 $0&lt;\alpha_{i}&lt;C ​$,此时 $g_{i}(w)=0 ​$，又因为 $\gamma_{i}=C-\alpha_{i}\ne0​$ ,因此 $\xi_{i}=0​$ ，所以 $y^{(i)}(w^Tx+b)=1-\xi_{i}=1​$ ，满足这个条件的样本点对应的是在margin先上的点。 SMOSMO，即sequential minimal optimization算法，是由John Platt给出的一种高效率的解决SVM的对偶问题最优化的算法。下面就这个算法总结一下其原理。 Coordinate ascent algorithm 以下简称caa算法。之前了解机器学习的同学肯定知道常用的优化方法有梯度下降法、牛顿法等等。这些方法都是求函数极值解的方法。实际上，目前SVM中需要求解对偶问题的目标函数为一个以 $\alpha_{i} ​$为自变量的函数 $W(\alpha_{1},\alpha_{2},\alpha_{i},…\alpha_{m})​$ ,其中m为样本个数，最终是要求这个函数的极大值即 $max_{\alpha}W​$ ，同时还带有约束条件。在这个问题中，如果我们只对一个参数 $\alpha_{i}​$ 进行优化，其他参数固定为常数，这时对一个参数求极值是比较容易的。这就是caa算法的核心。具体算法如下： 外层循环表示每次迭代训练直到收敛。而内层循环为对所有样本数据进行循环，每个样本只对其对应的 $\alpha_{i} $进行优化，其他参数固定为常数。 其实这里选择哪个 $\alpha_{i}$ 作为优化目标是有讲究的，在John Platt的论文中给出了一种启发式的选择 $\alpha_{i} $的算法，这个在后面会讲到。 下图描绘了caa算法的一个过程，很容易看到每次迭代优化的方向都是与坐标轴的某个轴平行，这是因为每次我们只优化一个参数。 Platt的SMO SMO就是使用了caa方法，但是与上述的不同。首先，我们使用上述方法，每次只优化一个参数，其他参数固定。此时假定我们选择 $\alpha_{1}​$为优化目标参数，那么因为有约束条件 $\sum_{i=1}^{m}{\alpha_{i}y^{(i)}}=0​$，所以有 $\alpha_{1}y^{(1)}=-\sum_{i=2}^{m}{\alpha_{i}y^{(i)}} ​$，等式两边同时乘以 $y^{(1)}​$ ，因为 $y^{(1)}\in \{-1,1\}​$ ，所以上述等式可化为： $\alpha_{1}=-y^{(1)}\sum_{i=2}^{m}{\alpha_{i}y^{(i)}}​$ ，由此式可以得到 $\alpha_{1}​$ 其实就是其余参数的线性组合，一旦固定了其他参数，那么 $\alpha_{1}​$ 也就固定了，无法优化。所以只对一个参数优化，固定其他参数是无法进行下去的。因此至少需要一次对两个参数进行优化。因此算法如下： 迭代直到收敛{ 通过某种算法选择两个参数 $\alpha_{i} $和 $\alpha_{j}$ . 使用 $\alpha_{i}$ 和 $\alpha_{j}$ 为优化目标参数，去优化函数$W(\alpha)$ ,同时固定其他参数为常数。 } 由等式约束得到， $\alpha_{1}y^{(1)}+\alpha_{2}y^{(2)}=-\sum_{i=3}^{m}{\alpha_{i}y^{(i)}} $，便于描述，等式右边我们定义一个符号 $\zeta$ 表示一个常数，因此有 $\alpha_{1}y^{(1)}+\alpha_{2}y^{(2)}=\zeta$ ，可以将该函数曲线看做是一条斜率是|1|的一个直线，当斜率为1时，表示y1和y2是异号的。结合 $0\leq\alpha\leq C$ 的约束，可以画出如下函数曲线： 可知函数直线与边界相交点，可以得到 $\alpha$ 的上下界分别为： $H=min(C,C-\alpha_1+\alpha_2)$ ,$L=max(0,\alpha_2-\alpha_1)$ 若斜率为-1，即y1和y2是同号，可画出如下图： 可以得到 $\alpha $的上下界分别为： $H=min(C,\alpha_1+\alpha_2) ​$， $L=max(0,\alpha_2+\alpha_1-C) $ 用 $\alpha_{2}$ 表示 $\alpha_1$ ,得到： $\alpha_1=(\zeta-\alpha_2y^{(2)})y^{(1)}$ ，反代入W中，可以得到一个以 $\alpha_2$ 为自变量的一个一元二次方程形式的表达式： $a\alpha_2^2+b\alpha_2+c$ ,对其求偏导，可以求出 $\alpha_2$ ,要保证满足 $L\leq\alpha_2\leq H$ ，先使用 $\alpha_2^{new,unclipped}$ 表示求导求出来的。然后最终与上下界结合得到的最终参数值应为： 同时也把 $\alpha_1^{new} $也可以求出来，由于其余参数是固定不变的，因此old和new的其他参数和应当相同，所以有： $\alpha_1^{new}y^{(1)}+\alpha_2^{new}y^{(2)}=\alpha_1^{old}y^{(1)}+\alpha_2^{old}y^{(2)} ​$,因此 $\alpha_1^{new}=\alpha_1^{old}+(\alpha_2^{old}-\alpha_2^{new})y^{(2)}y^{(1)} $ 关键在于求 $\alpha_2^{new,unclipped} $。我们对 $W(\alpha_2)$ 求导，并令导数为0，最终得到表达式： 其中， $E_i=f(x_i)-y^{(i)}​$ ，表示模型预测值和真实值的误差，而K表示对应的数据样本xi与xj的内积使用核方法替换后的值。（具体推导比较繁琐，具体见【机器学习详解】SMO算法剖析 - CSDN博客） 若K11+K22-2K12=0,则表示样本x1和x2的输入特征相同，其实K11+K22-2K12是 $W(\alpha_2) ​$对 $\alpha_2 ​$的二阶导，因此若K11+K22-2K12=0，表示原函数是一个单调函数，因此是在边界处取极值的。若K11+K22-2K12&lt;0,此时原函数是凸函数，没有极小值，因此也在边界处获得极值。 获取 $\alpha $的值后，相当于获得了w的值，但是b的值仍然需要单独处理，每轮迭代后都要对b值进行更新，因为其影响着 $E_i=f(x_i)-y^{(i)}$ 的计算。 若 $0&lt;\alpha_1^{new}&lt;C $,由KKT补充条款得知， $y^{(1)}(w^Tx+b)=1$ ，因此 $(w^Tx+b)=y^{(1)}$ ，因此 $\sum_{i=1}^{m}{\alpha_iy^{(i)}K_{i1}}+b=y^{(1)}$ ,由此得到: 可以将上式再化简，其中： 因此可得： 如果 $0&lt;\alpha_2^{new}&lt;C $，则与上述推导同理，可得： 若两个同时满足 $0&lt;\alpha_i^{new}&lt;C​$ ，则 $b_1^{new}=b_2^{new}​$ 。 若都不满足 $0&lt;\alpha_i^{new}&lt;C$ ，表示参数的值在边界上，此时，b1与b2之间的任何数都满足KKT条件，都可以作为b的更新值，一般取平均。 拉格朗日乘子的启发式选择方法 最后简单讲一下，Platt是如何使用启发式算法来有效率地选择乘子的顺序，使得我们的算法效率最高。 因为有两个参数选择，因此算法的选择也是分为外层循环选第一个参数，然后内层循环选第二个变量。算法的核心思想在于关注那些会违反KKT条件的样本对应的乘子，这些乘子才最需要去优化，而那些本身就符合KKT条件的样本，则不需要优先优化，可以将优先级放低。 选择 $\alpha_1$ : 先遍历一次样本数据集，找出所有违反KKT条件的样本对应的乘子，适合优化然后选择第二个参数，见后分析，对这两个变量进行优化。 遍历所有非边界的样本集（ $0&lt;\alpha_i&lt;C​$ ），找出其中违反KKT的参数作为第一个变量，再选择第二个变量，进行优化。 重复步骤1,2，即在整个样本集和非边界样本集上来回切换遍历，寻找违反KKT条件的样本对应的参数。直到遍历整个样本数据集后，找不到违反KKT条件的样本点了，此时推出。 选择 $\alpha_2$ : 假设在上述过程中已经挑选了一个参数 $\alpha_1 $，那么我们要找的 $\alpha_2$ 应当是是优化最有效率，即优化后得到的新参数变化越大越好，因为 $\alpha_2$ 依赖于 $|E_1-E_2|$ ，因此 $|E_1-E_2| $越大越好。当$ E_10$ ，那么选择最小的 $E_i$ 作为 $E_2 $是最合适的。 因为边界上的样本对应的 $\alpha_i=0,\alpha_i=C $，在优化过程中难以优化，所以建议先在非边界数据集上进行上述步骤。 若非边界集上没有，则在整个样本集上选择第二个变量。 若整个样本集没有，则跳入到选择第一个变量的过程。 reference: cs229 notes 林轩田的机器学习公开课讲义 【机器学习详解】SMO算法剖析 - CSDN博客） 支持向量机（五）SMO算法 - JerryLead - 博客园 后续对SVM的总结会结合sklearn和hinge loss与logistic regression的关系来进行。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SMO</tag>
        <tag>soft margin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机SVM总结之核方法]]></title>
    <url>%2F2019%2F04%2F05%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E6%80%BB%E7%BB%93%E4%B9%8B%E6%A0%B8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[上一章节最后提到了三个问题： 1、计算样本x之间的内积，计算量还是很大的，有没有什么方法减小计算量呢？ 2、若大部分样本数据在当前维度上不是线性可分的怎么办？ 3、如果出现少量异常点怎么办？ 本章着重总结支持向量机中如何使用核方法来解决前面两个问题。注意，核方法本身与支持向量机没有强相关的关系，是两个独立的知识。核方法本身是一种解决非线性模式分析问题的一种方法，而SVM通过使用核方法，能够对一些在某维空间上不是线性可分的数据点进行分类。由于核方法的知识点很庞大，因此本文只是结合SVM，总结SVM中使用到的核方法的知识，有关于核方法的本质以及其相关知识体系待以后有余力再总结。 前一章节中，我们推导得到SVM的对偶问题形式，如下： 上式中，只用到了原始的特征集X，但是现实情况下，为了表示数据的一些复杂的属性和特征，可能要对一些特征做线性的组合，比如使用诸如 $x^2,x^3 $这样的高次多项式作为高级特征，与原来的原始特征x组合在一起，组成新的特征集，然后代替原来的原始特征到计算式中，即我们定义 $\phi$ 为一个特征映射，表示原始特征到新特征的具体映射关系，如： 将 $\phi $代替x，代入到SVM中的对偶问题形式，得到： 我们定义内积 $&lt;\phi(x),\phi(z)&gt; $为kernel（核）K： 继而可以使用核K(x,z)来代替对偶问题中的所有特征数据的内积。现在，只要给定映射 $\phi $，我们可以很容易得计算出K，而且计算K的成本通常都比较小，即使在 $\phi(x)$ 本身由于维数很高的情况下计算量庞大时，K的计算成本也小于单独计算 $\phi(x)$ 。因此，只要在给定映射 $\phi(x)$ ，只要算法能够有效率得去计算K，那么就可以让我们的SVM算法即使在高维特征数据下也能有很好的性能，同时模型学习能力也能得到保证。 举例说明：给定 $x,z \in\Re^n $,有二阶多项式核函数： 为什么称为多项式核方法？我们可以将该核函数拆开，重新组合，得到： 因为 因此映射 $\phi$ 显而易见是原始特征的两两线性组合的形式，如下： 可以知道直接计算 $\phi(x)$ 需要的计算量为 $O(n^2)​$ ,而计算 只需要 $O(n) ​$的计算量。 上述例子只是多项式核函数的一个特例，若核函数再带一个常数项c，则得到如下核函数： 它对应的映射 $\phi $应当是除了特征之间的两两组合外，还有单独的特征的常数倍数，以及常数c，具体如下： 将上述核函数推广到一般情况，即 $K(x,z)=(x^Tz+c)^d ​$，即d阶多项式核函数，可以将原始特征映射到更高维的特征空间中，同时不需要去计算 $\phi(x)​$ ，而是直接计算K的值（计算成本与特征维数n成线性关系），就可以让模型得到充分学习。 有一种对核方法的在SVM中的应用的理解，即核方法能够间接表达出两个特征之间的相似度，即两个特征如何非常相似，即对应的特征映射 $\phi(x) $也非常接近，在向量空间中表示为两个向量非常接近，因此其内积应当比较大，从而得到K也应当比较大，反之亦然。因此可以说核函数再一定程度上反映了特征之间的相似度。下面介绍一种非线性的核方法，能够在一定程度上衡量出特征之间的相似度。 有核方法如下： 可以看到这个核方法（其实就是sklearn中SVM核方法里面的RBF，Radial Basis Function）有以下特点： 与之前的线性核方法不同，这个是非线性的核方法，加入了自然底数e的转换 通过一个距离的度量 $||x-z||^2$ ,表示两个特征之间的相似度。当两个特征不相似，即离得很远，则exp()内部应当趋向于负无穷，因此K趋向于0；当两个特征相似时，即离得近，exp()内部应当趋向于0，因此K趋向于1。 很明显，这个核方法的表示方法与概率统计中的高斯分布很相似，因此该核方法又称为高斯核。 这个核函数能够将原始特征映射到无限维的特征空间。 上面讲了几个核函数，到底什么样的函数才能作为核函数呢？换个说法，即怎么判断一个kernel是有效的？ 首先，定义一个kernel matrix，其中，矩阵元素 $K_{ij}=K(x^{(i)},x^{(j)})$ 。核矩阵表示了每个样本对应的核。假设当前的核函数是有效的，那么有如下等式： 可以得到，核矩阵首先是一个对称矩阵。其次定义 $\phi_{k}(x) $为向量 $\phi(x)$ 的第k维元素。对于任意向量z，有如下式子： 由半正定矩阵定义可知，核矩阵同样是一个半正定的矩阵（若没有等号的话，则是严格正定）。因此一个核函数如果是有效的，那么他对应样本的核矩阵应当是对称的且是半正定的。这是一个必要条件。其实他也是一个充分条件，这个充分必要条件由一个定理给出，即mercer定理。关于mercer定理的完全证明，其实还要用到L2范数，以及再生希尔伯特空间等相关的凸优化理论中的知识，具体可看维基百科中的内容：Mercer’s theorem（其实我也不太会证明，如果有大牛可以清晰的推导的话，请告知我，无比感谢！） 其实在实际运用中，我们通常先使用线性的核函数对数据进行处理，线性核通常计算速度会快一点，能够得到一个baseline，然后再可以使用一些非线性的核比如RBF，它能够应对数据边界不是规则的数据。有关核函数解决数据线性不可分的效果，可以看如下截图： 可以看到本身二维中的两种不同数据，无法用直接进行分隔，而通过映射到三维中，可以找到一个平面将两种数据分隔。 reference: cs229 notes 机器学习有很多关于核函数的说法，核函数的定义和作用是什么？ 核函数的有效性判定 - CSDN博客 其实遇到数据线性不可分的问题，或者说存在一些异常点和噪点不能完美分隔时，还有一种方法，即引入惩罚项（相当于regularization），使得我们的模型不是以完美分隔所有数据为目标，而是允许有一些点可以分错。此时SVM有hard模式转变为soft模式，相当于更懂得变通。下一章将重点总结这个模型。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>kernel method</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机SVM总结之拉格朗日对偶问题]]></title>
    <url>%2F2019%2F04%2F04%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E6%80%BB%E7%BB%93%E4%B9%8B%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前一章节最后得到了SVM的优化目标函数： 现在我们就需要想方设法去解这个函数优化问题。 凸优化我们的函数优化问题是一个凸优化问题，什么叫凸优化，我下面只是简单得讲一下，不过多深入。 解释一下“凸”。在一段数据区间内[x1,x2]，任意的数据点都可以用 $\theta X_{1}+(1-\theta)X_{2} $表示，其中 $0\leq\theta\leq1$ ，这段区间称为convex set。一个函数如果其变量域是convex set，且任意其变量域中的 $x\neq y$ 都满足： $f(\theta x + (1-\theta)y)\leq\theta f(x)+(1-\theta)f(y)$ ,其中 $0\leq\theta\leq1$ ，则说明这个函数式严格凸的。二次函数曲线在实数域上是典型的一个凸的函数。 凸优化问题就是去寻找凸函数的极值。一般在没有任何约束条件下，求函数极值一般是求偏导，然后令偏导为0，求得参数的值。这是最简单的情况。但是我们的SVM优化目标函数是有不等式约束条件的。之前在隐马尔可夫模型总结时，遇到过有等式约束的条件下，求函数的极值问题，使用的是拉格朗日乘子法。这个方法的具体内涵可以见quora上的经典回答。 https://www.quora.com/What-is-the-meaning-of-the-value-of-a-Lagrange-multiplier-when-doing-optimization/answer/Balaji-Pitchai-Kannuwww.quora.com 等式约束条件下的优化问题已经知道方法了，但是我们的SVM中的优化函数是带不等式约束条件的，这个怎么求？这里我读了几篇博客包括浅谈最优化问题的KKT条件，关于SVM数学细节逻辑的个人理解（二）：从基本形式转化为对偶问题 - xxrxxr - 博客园，给出了详细的介绍，下面我先介绍一下对偶问题，然后结合上述博客，详细说明对偶问题的KKT条件是如何推导出来的。 对偶问题考虑同时带等式约束和不等式约束的优化问题： 我们使用通用的拉格朗日乘子法，得到拉格朗日多项式： 其中， $\alpha_{i} ​$是对应于 $g_{i}(w)\leq0 ​$的拉格朗日乘子， $\beta_{i}​$ 是对应于 $h_{i}(w)=0​$ 的拉格朗日乘子。我们定义原始的优化目标： 这里， $\alpha_{i} $是非负的，这个目标就是最大化我们的拉格朗日多项式，将上述式子进行展开得到： 从上式可以得到，若我们的参数w不满足 $g_{i}(w)\leq0$ ，此时因为 $\alpha_{i}$ 是非负的，那么取 $\alpha_{i}$ 为正数可以让中间的项永远大于0，因此 $\theta_{p}(w)\rightarrow\infty$ ;若参数w不满足 $h_{i}(w)=0$ ，那么让 $\beta_{i}\rightarrow\infty $,使得 $\theta_{p}(w)\rightarrow\infty$ 。相反，若参数w满足所有的原始约束条件，那么此时可以得到， $\sum_{i=1}^{l}{\beta_{i}h_{i}(w)}=0 ， \sum_{i=1}^{k}{\alpha_{i}g_{i}(w)}\leq0$，要使 $\theta_{p}(w)$ 最大，那么就有： $\theta_{p}(w)=f(w)$ 。因此原始优化目标可以写成： 因此我们S原来的找极小值的优化问题也可以写成： 此时为了叙述方便，定义 $p^{\star}=min_{w}\theta_{p}(w)$ 表示我们需要求的最优目标值，这个通常叫做原始问题，表示原始的优化目标。 而往往对应于原始问题，我们能够转化为相应的对偶问题。对偶问题可以理解为在研究目标函数在约束条件下的极大值问题时，通常都存在与之匹配的求极小值的问题。因此上面的例子中，对应的对偶问题形式如下： 相当于是把原始问题中的max和min的位置进行了对调。有一个显而易见的结论，即对一个序列先找极小值，然后在所有极小值中找到一个最大值a；对一个序列先找极大值，然后在所有极大值中找到一个最小值b。 $a\leq b $是明显成立的。因此有： 为何要研究对偶问题？我自己的理解是对偶问题比原始问题更容易解，容易在哪里？可以看到对偶问题外层的优化目标参数是拉格朗日参数，然后通过求得的拉格朗日参数，间接得到我们最终要求的超平面的参数w。而原始问题外层的优化目标参数直接是w，无法直接去优化得到这个参数（因为还存在着其他未知的拉格朗日参数） 那么什么时候对偶问题能够等价于原始问题呢？由于上式为 $d^\star\leq p^\star $，两个问题要等价，相当于要使 $d^\star=p^\star$ 。因此有数学家提出了一些理论来让两种问题等价。假设当前函数f以及 $g_{i}$ 都是凸的，且 $h_{i}$ 是仿射的（理解为带截距的线性函数），同时存在w，使得 $g_{i}(w)&lt;0$ 对所有i都成立。此时必然存在参数解 $w^\star,\alpha^\star,\beta^\star$ （第一个是原始问题的解，后两个是对偶问题的解）,使得 $d^\star=p^\star=L(w^\star,\alpha^\star,\beta^\star)$ 。 $w^\star,\alpha^\star,\beta^\star$ 必须要满足这样一个条件，叫Karush-Kuhn-Tucker（KKT）条件： 其中第三个条件称为KKT对偶补充条件。当 $\alpha_{i}^\star&gt;0$ 时，说明 $g_{i}(w^\star)=0$ 。这个结论对后面提升SVM计算效率很有用，它能够表明只有很少的一部分数据点需要考虑 $g_{i}(w)$ 。 简单推导KKT力学渣：浅谈最优化问题的KKT条件 写在SVM之前—凸优化与对偶问题 - vibe - 博客园 根据上面两个博客中对KKT的推导，我简单得进行总结，尽量用我自己的语言来描述。 首先，明确我们的目标：即参数解集 $w^\star,\alpha^\star,\beta^\star$ 要使得 $d^\star=p^\star$ 。 1、$w^\star,\alpha^\star,\beta^\star$ 必然是 $min_{w}L(w,\alpha,\beta)$ 的极值解。因此其对应偏导应当为0. 2、第二个KKT条件与第一个条件类似 3、由于 $d^\star ​$属于 $L(w,\alpha,\beta)​$ 最小值集中的一个，因此 而 $p^\star$ 在上一节中推导对偶问题时，已知 $p^\star=f(w^\star)$ ,因此有如下等式成立： 因为 $h_{i} =0$本来就是我们的等式约束，因此可以得到： 而由于 $\alpha_{i}\geq0$ 和 $g_{i}(w)\leq0$ 本来就是约束条件，因此 $\alpha_{i}g_{i}(w)\leq0$ ,要使上述等式成立，求和的每项只能等于0，因此得到： SVM中的对偶问题应用上面就是一些对偶问题的数学原理描述，不是很深入，但是对于了解SVM感觉应该够用了。下面就将这个思想应用到我们的SVM模型问题中。重新看一下我们的SVM中的原始问题： 可以看到我们只有不等式约束，因此将不等式约束统一改写为标准形式： 有KKT互补条件得知， $\alpha_{i}&gt;0$ 也即$g_{i}(w)=0$ 的数据点表示的是那些间距为1的数据样本点，这种样本数据上一章我们提到过，叫支持向量。这里就不细说了。等后面讲到具体的SVM实现时，会提一下。下面我们构造我们的 $L(w,b,\alpha)$ : 要得到其对偶形式，首先固定参数 $\alpha ​$,以w和b为参数，求 $L(w,b,\alpha) 的极小值​$。求法很简单，还是用求偏导数，令偏导数为0： 得到w的表达式： 同理对b求偏导，得到： 将w的表达式反代入L中，得到： 最后一项可以得到为0，因此有: 再结合我们的约束条件 $\alpha_{i}\geq0$ 以及 可以得到我们的对偶问题形式： 其实根据上面我们的推导过程，以及约束条件，可以看出我们的对偶问题满足KKT条件。 至此，就可以用这个去解决问题了，相当于先求出最优的 $\alpha$ ,然后推得对应的w和b，具体的在后面的SMO中会有讲解。 这里提一点，我们在训练完模型后，假设得到了最优 $\alpha$ 、w和b,那么预测过程为计算 $w^Tx+b $，若结果大于0，则预测y=1。其实 $w^Tx+b $可以化为： 实际上， $x^{(i)}$ 为训练样本中的数据点。根据支持向量的性质，支持向量对应的 $\alpha$ 不为0，其他数据的 $\alpha$ 均为0，且支持向量的个数通常都比较少，因此可以极大降低计算的成本。 接下来有两个问题： 1、计算样本x之间的内积，计算量还是很大的，有没有什么方法减小计算量呢？ 2、若大部分样本数据在当前维度上不是线性可分的怎么办？ 3、如果出现少量异常点怎么办？ 对于1和2两个问题，下面我们将引入kernel方法，将维度进行转换。对于3这个问题我们将引入惩罚项，类似于regularization，将原来我们的hard-SVM转换为soft-SVM来解决。下两章将着重介绍这两个方面。 reference： cs229 notes 力学渣：浅谈最优化问题的KKT条件 写在SVM之前—凸优化与对偶问题 - vibe - 博客园]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>convex optimization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机SVM总结之问题描述]]></title>
    <url>%2F2019%2F04%2F04%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E6%80%BB%E7%BB%93%E4%B9%8B%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[本系列开始对支持向量机SVM算法的学习总结，由于SVM涉及到的数学知识很多，包括了向量、几何间距、凸优化、核方法等。因此，会分几个章节进行总结。尽量在一定的数学理论深度上将SVM的思想描述清楚，不过多深入其中的数学机制。总结主要是借鉴了cs229中的相关章节以及notes、林轩田的机器学习技法课程、还有其他博客的相关总结。 SVM可以说是传统机器学习中，除去集成模型外，称为off-the-shelf（拿来即用）且效果通常来说都比较好的有监督算法，即可以用来解决分类问题，也可以用来解决回归问题（支持向量回归），为了问题描述更简洁，只讨论二分类问题。（后续有时间会讨论支持向量回归） SVM核心思想其实SVM的核心模型很简单且很好理解，难的是他如何去最优化这个模型。首先，我先总结一下它的核心思想——margin。我们在解决分类问题时，预测样本数据的标签是1还是-1，本质上预测的是一个样本属于标签1的概率是多少，若概率大于我们预设的阈值，则属于标签1，反之属于标签-1。 如上图，是典型的分类问题的几何展示，假设当前的样本是二维的，叉数据表示标签为1，圆圈数据表示标签为-1，可以看出当前数据在二维平面上是线性可分的，我们的分类模型能够在两种不同标签的数据之间划分一个界限，类似于象棋中的“楚河汉界“，不同标签的数据各占一面。这里有两点备注说明一下： 线性可分的概念，这里简单得理解就是在某个n维空间中，能够在n-1维空间上找到一个线性函数将n维空间上的数据分隔，举例子：二维空间中找到一个直线分隔、三维空间中找到一个平面分隔。SVM算法要求数据是线性可分的，如果不是线性可分，需要使用核函数将数据转换到可以线性可分的维度。（后面详细说明。） 线性可分中，能够将数据分隔的线性函数我们称为hyperplane，可以称为超平面。我们的线性分类模型最终就是要得到这样一个超平面能够将数据很好的分隔。 下面为了描述方便，假设当前数据是二维的，那么我们最终要求的就是一条直线，即wx+b这个函数中的w和b的参数值（高维的思想类似，只是w和b的维数要做相应的变更），能够将数据按照“楚河汉界”一样分隔，但是这样的直线其实是有很多的，如下图： 上图中三条直线其实都满足上面的要求，但是我们肯定是要得到一个最优的模型的，那么怎么算是最好的模型呢？即上面三条线哪条比较好呢？有两个思路： 所有的数据点肯定要离这条直线越远越好，这样说明我对某个数据的分类预测正确的信心就越大。 离这条直线很近的点分错的可能性很大，将直线稍微旋转一下，可能就会改变一个数据的分类，此时模型的健壮性很差，容易受异常数据的影响。 因此我们最终要找的这条直线应当朝着最大化所有数据点到该直线间距的目标而去的，称为maximize margin。那么这个margin怎么计算呢？ 首先这个margin就是计算数据点到超平面（即直线）a的距离。这个点到直线的距离在高中已经学过了，很简单，直接通过该数据点，画一条与a平行的直线b，计算直线a按照与w垂直的方向平移到直线b的距离，可得到： $\gamma^{i} $表示第i个数据点到超平面的间距，||w||表示 $\sqrt{w^{T}w} $。由于我们的分类标签为1和-1，因此我们同时考虑两种不同分类标签数据的间距margin，如下：相当于上面的间距公式乘以标签 其次，由于对所有点求最大间隔这个优化问题很难，可以说是一个NP-hard难度问题。因此我们转换一下思路，我们只要保证那些距离超平面（后面都以直线wx+b=0相称）最近的那些数据点离直线wx+b尽量就可以了，形式化说法就是，我们定义 $\gamma=min_{i=1,…,m}\gamma^{i} $为所有数据点到直线wx+b最小的间距，我们的目标就是要找到一条直线，去最大化这个间距，即： 这里有几点说明： 该优化的目标函数是 $\gamma$ ,优化的参数是 $\gamma,w,b$ 约束条件有两个，第一个就是要求所有数据点的间距要大于 $\gamma$ ，其实就是我上面说的距离直线最近的点，因此其他所有点肯定比这些点要远。 第二个约束条件的意思是上述间距公式表示的是几何间距，而优化函数我们习惯于对函数进行操作，因此还存在一个函数间距， $y^{(i)}(w^{T}x^{(i)}+b) $，为了将几何间隔与函数间隔对应起来，对我们的 $\gamma$ 公式进行scaling，不会对我们的优化问题有任何影响，因此可以将 $||w||=1$ ，此时也与第一个约束不等式对应起来（若不带这个等式约束，则第一个约束条件要除以||w||）。 但是上述优化问题显然很难解决，因为 $||w||=1$ 这个约束条件是非凸的（绝对值函数，并没有一个明确的最优），并不好优化。因此要将上述问题进行转化，将这个等式约束条件去掉： 上式将等式约束去掉了，优化目标函数也变化了，但是这个优化函数也是个非凸的函数，也不好优化。那么我们可以对间距函数继续scaling，即同时调整w和b的倍数，使得 ，即相当于距离wx+b=0最近的点到该直线的距离设为1，得到优化问题： 有的notes中的优化目标函数为2/||w||，原因是将正反两种标签的数据点到超平面直线的距离都加上了，道理是一样的。如下图所示。 图上这些margin间距最小的点有个名字，叫做support vector，即支持向量，通常一个数据集中的支持向量的个数不会很多，因此SVM利用这种特性能够提高算法模型的效率，这个在后面讲解具体算法时会说到。 其实上述目标函数还是不好优化，因此还可以继续转化： 其实做了个最大最小化的转化，此时得到一个二次函数，且是求在约束条件下，它的最小值，这是个凸函数，可以使用凸优化相关方法来解决该问题了。至于为什么要加1/2,我理解是后面求偏导时，可以将w前面的系数约去，方便计算。 下一章继续讲解如何使用拉格朗日对偶方法来解决这个最优化问题。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔科夫模型学习总结之Viterbi算法应用]]></title>
    <url>%2F2019%2F04%2F03%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8BViterbi%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本章将描述总结隐马尔科夫模型的最后一个问题，即已知观测序列和模型参数集，求该观测序列对应的最有可能出现的隐状态序列是什么样的？即找到出现概率最大的隐状态序列。这个问题常见于NLP中的分词、词性标注、语音识别等任务中，如中文分词任务中，每个中文字符作为观测变量，而该字符在词中的角色（如词首，词中，词尾，单独成词）作为隐变量状态，假设当前已经通过大量文本训练得到了HMM模型的参数集，我们的任务就是给定一段中文的文本，给这段文本的每个中文字符打上隐状态标记，（识别词首到词尾，以及单独成词的字串）从而完成分词任务。通常这个也叫做标注任务。 那么如何解决这个问题呢？一种方法是在给定观测序列的条件下，穷举所有可能的隐状态序列，根据参数集，计算该隐状态序列的联合概率（使用贝叶斯公式）。最后找出联合概率最大的那个序列即可。这种解法很简单，但是需要 $O(k^T) $计算复杂度，其中k表示隐状态个数，T表示序列长度（或者说是时刻长度）。 因此基于上述问题，需要使用一种动态规划算法来解决该问题，这个问题相当于可以转化为对一个有向无环带权重图，找到起点至终点路径最短的（即概率最大）的结点集合。这里使用了一个非常著名的动态规划算法，Viterbi算法，该算法在通信行业领域使用比较广泛，它特别适合上述这种寻找最短路径的问题。 我是在看了吴军的数学之美以及一篇博客后，对该算法有了一定的了解。博客链接如下： （该博客貌似有点错误，具体先下面详细说明） Viterbi算法的基础概括成下面三点： 1、如果概率最大的路径P（或者说是最短路径）经过某点a，那么这条路径上从起始点s到a的这一段子路径一定是s到a之间的最短路径。否则用s到a的最短路径来替换上述路径，便构成了一条比P更短的路径，矛盾。 2、从S到E的路径必定经过第i时刻的某个状态，假定第i时刻有k个状态，那么如果记录了从S到第i个状态的所有k个节点的最短路径，最终的最短路径必经过其中的一条。这样，在任何时刻，只需要考虑非常有限条最短路径即可。 3、结合上述两点，假定当我们从状态i进入状态i+1时，从S到状态i上各个节点的最短路径已经找到，并且记录在这些节点上，那么在计算从起点S到前一个状态i所有的k个结点的最短路径，以及从这k个节点到Xi+1，j的距离即可。 基于上述基础，有如下算法： step1：从点S出发，对于第一个状态 $X_{1} $的各个节点，不妨假定有 $n_{1}$ 个，计算出S到它们的距离 $d(S,X_{1i})$ ，其中 $X_{1i}$ 代表任意状态1的节点。因为只有一步，所以这些距离都是S到它们各自的最短距离。 step2：对于第二个状态 $X_{2} $的所有节点，要计算出从S到它们的最短距离。对于特点的节点 $X_{2i}$ ，从S到它的路径可以经过状态1的 $n_{1}$ 中任何一个节点 $X_{1i}$ ，对应的路径长度就是 $d(S,X_{2i})=d(S,X_{1j})+d(X_{1j},X_{2i})$ 。由于j有 $n_{1}$ 种可能性，我们要一一计算，找出最小值。即： d(S,X_{2i})=min_{j=1,n_{1}}d(S,X_{1j})+d(X_{1j},X_{2i})这样对于第二个状态的每个节点，需要 $n_{1} $次乘法计算。假定这个状态有 $n_{2} $个节点，把S这些节点的距离都算一遍，就有 $O(n_{1}*n_{2})$ 次计算。 step3：接下来，类似地按照上述方法从第二个状态走到第三个状态，一直走到最后一个状态，就得到了整个网格从头到尾的最短路径。每一步计算的复杂度都和相邻两个状态Si和Si+1各自的节点数目 $n_{i}$ ， $n_{i+1}$ 的乘积成正比，即 $O(n_{i}*n_{i+1})$ step4：要获取最终路径节点，只要在回溯上述计算过程，就能得到对应的路径节点。 补充：原博客中并未提及回溯的过程，且他确定每个时刻的状态时，貌似采用的是每个时刻取最大的贪心算法，而看了知乎上一个回答，发现确定隐状态结点序列，是需要根据当前已确定的最新结点往前回溯，即假设t=3的天气是Rainy，则需要在 $P_{2}(Sunny)A_{Sunny,Rainy},P_{2}(Cloudy)A_{Cloudy,Rainy},P_{2}(Rainy)A_{Rainy,Rainy} $中选取最大的作为t=2时刻的天气，而不应该是直接在 $P_{2}(Sunny),P_{2}(Cloudy),P_{2}(Rainy)​$ 中选最大的。个人感觉该博客在这地方有问题，最后我代码计算得到的序列是{Sunny,Cloudy,Rainy}，与原博客也不同。 上述是从吴军老师的书中摘取的，个人感觉已经非常明确简洁了，如果想知道实例怎么计算的，可以去上面我贴出的链接里面看，非常清楚。 贴一下我用python实现的上述例子吧：使用了numpy，向量化一些计算，省去了一些循环。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# coding=utf-8import numpy as npobserved_status = &#123;0:"Dry",1:"Dryish",2:"Damp",3:"Soggy"&#125;hidden_status = &#123;0:"Sunny",1:"Cloudy",2:"Rainy"&#125;sequence_observe = [0,2,3]#初始状态概率pi_0 = np.array([0.63,0.17,0.20])A_matrix = np.array([[0.5,0.375,0.125],[0.25,0.125,0.625],[0.25,0.375,0.375]])B_matrix = np.array([[0.6,0.2,0.15,0.05],[0.25,0.25,0.25,0.25],[0.05,0.10,0.35,0.5]])def viterbi(A_matrix,B_matrix,pi_0,sequence_observe): #序列长度 sequence_length = len(sequence_observe) state_length = len(hidden_status) #初始化一个矩阵存放每轮计算得到的prob curren_prob = np.zeros((sequence_length,state_length)) #存放结果隐状态序列index final_result_list = [] #用于回溯的存放前一个结点最大计算概率 prob_max = [] #起点到第一天 curren_prob[0,:] = np.multiply(pi_0,B_matrix[:,sequence_observe[0]]) #t=1 to T: for t in range(1,sequence_length): tmp_dict = &#123;&#125; for state_j in hidden_status.keys(): #计算前时刻状态到本时刻状态最大概率 curr_state_prob_max = np.max(np.multiply(curren_prob[t-1,:],A_matrix[:,state_j])) curr_state_max = np.argmax(np.multiply(curren_prob[t-1,:],A_matrix[:,state_j])) tmp_dict[state_j] = curr_state_max #计算本时刻转移到state_j状态的联合概率 curren_prob[t,state_j] = curr_state_prob_max*B_matrix[state_j,sequence_observe[t]] prob_max.append(tmp_dict) #backtrace final_pick_status = np.argmax(curren_prob[-1,:]) final_result_list.append(final_pick_status) curr_status = final_pick_status for t in range(sequence_length-1,0,-1): #根据已处理的最新结点，找到上一时刻概率最大的结点 previous = prob_max[t-1][curr_status] final_result_list.insert(0,previous) curr_status = previous final_result_str_list = [hidden_status[weather_index] for weather_index in final_result_list] return final_result_str_listprint(viterbi(A_matrix,B_matrix,pi_0,sequence_observe)) 终于把HMM的相关内容整理总结了一下，其实HMM的知识远不止我总结的这几篇文章，包括引入最大熵的MEMM模型、引入二阶马尔科夫性质的HMM等等，但个人不会短时间再深入了，因为作为一名做工程的，需要平衡工程和数学统计的研究时间，既不能完全忽略数学统计的作用，也不能跟专业学术界的大牛一样钻研其中，容易出不来，而且目前暂时的实际应用中，HMM算法的效果已经可以接受了，而且从HMM算法中，可以学到很多解决问题的思想和方法，包括最大似然估计，贝叶斯公式，拉格朗日乘子法，动态规划算法以及各种消元思想。后续有时间的话还可能对条件随机场进行总结，毕竟这两种模型代表了不同的建模方式（联合概率和条件概率），且条件随机场的效果在NLP里面通常都比HMM要好。 预告：下一个系列会对SVM支持向量机算法进行一个详细的总结，该算法本身的思想很简单，但是使用了大量的数学、凸优化知识来帮助构建该模型，值得好好剖析。 reference： 吴军 《数学之美》 wiki百科：Viterbi algorithm HMM模型和Viterbi算法 - Denise_hzf - 博客园]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>viterbi</tag>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔科夫模型学习总结之三]]></title>
    <url>%2F2019%2F04%2F03%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8B%E4%B8%89%2F</url>
    <content type="text"><![CDATA[本章节主要是总结一下HMM的模型训练问题。即只给定观测序列，如何训练HMM模型，得到模型的参数集 $\lambda=\left\{ \pi,A,B \right\} $,其中 $\pi$ 表示初始隐状态的概率，A表示状态概率转移矩阵，B表示发射概率矩阵。 怎么求这个参数集呢？一般求模型的参数集，是使用极大似然估计方法，求参数，使得给定参数的条件下，观测序列的概率最大化，即： \lambda_{MLE}=argmax_{\lambda}lnP(Y|\lambda)其中，一般求解极大似然估计函数，为了方便最优化，通常会在外层增加一个log函数，将连乘的形式转化为求和的形式，一来，log函数（其实是e为底的log函数）是单调增的，二来求和的形式相对于连乘的形式更容易最优化。求解这个问题，如果已知参数的概率分布的样式，只需求参数的概率分布参数，这个问题时很简单的，如已知参数的概率分布是高斯分布，则只需求解该高斯分布的均值和方差就可以了。但是在HMM的问题中，我们并不知道参数的概率分布是什么形式的。此时有一个方法可以解决该问题，即期望最大化方法（Expectation Maxmization）。 简单的说，该方法是一种以迭代形式求参数的方法，通过引入隐变量Z，然后假设已知迭代g的参数 $\theta^g​$ ,建立 $\theta^{g+1}​$ 与 $\theta^g​$ 的关系式，然后该试进行求期望将隐变量（E-Step），估计Z的概率分布，然后根据Z，间接求参数(M-Step)。E-Step和M-Step迭代进行，直到收敛。（关于EM算法的具体原理以及其收敛性证明以后有时间另开系列总结） 使用EM算法求解HMM的模型学习问题首先写出我们的问题求解，即极大似然估计概率式子： $\lambda_{MLE}=argmax_{\lambda}lnP(Y|\lambda) $ 在HMM中，隐状态Q就相当于是EM算法中引入的隐变量，因此可以写出EM算法的原始问题式，假设当前已求得迭代g的参数 $\lambda^g$ ,建立 $\lambda^{g+1}$ 和 $\lambda^g$ 的关系式如下： 关于该公式的说明： 在原问题式中增加了隐状态q，为了不改变原概率问题，通过积分的形式，将该变量的影响消除（之前有提过该方法），所以就形成了上述式子。 上述等号右边的第二个乘式可以进一步化简： 注意一点： $\lambda^g$ 在这个式子中是已知的常量。所以上式中 $P(Y|\lambda^g)$ 相当于一个常量，对优化求极值问题没有作用，因此可以删除，最后整个式子化简为： 后面为了描述方便，用 $Q(\lambda,\lambda^g) $表示这个式子， E-Step: 由于隐变量是一个离散型的变量，所以对其积分相当于是求和形式，故上式化为如下形式： 其中，外层有0-T个求和，每个求和有k项，而每项的式子对log函数展开成和的形式。而 $P(q,Y|\lambda^{(g)})​$ 则原封不动。 M-Step： 对该式子求极值，可以分别对参数求偏导，使其为0，求得参数的表达式。 首先求解参数 $\pi $，对 $\pi$ 求偏导，注意到只有第一个加项存在 $\pi $，因此其他项可以忽略。 上式中，不包含的初始状态 $q_{0} $的项可以忽略，因此可以化简得到等号右边的式子。 注意到 $\sum_{i=1}^{k}{\pi_{i}}=1$ ，面对这种带等式约束的多项式求极值的问题，自然就想到使用拉格朗日乘子法。 该多项式对 $\pi_{i} $求偏导，令偏导为0，得到： 1式 2式 该多项式对 $\tau​$ 求偏导，令偏导为0，得到： 将1式两边求和，得到： 得到 $\tau$ 的表达式，然后将该表达式带入2式，得到参数 $\pi_{i}$ 的表达式： $P(q_{0}=i,Y|\lambda^{g}) ​$怎么求？看过我之前文章的可以知道，我在第二章节最后讲到的用前向后向算法得到的三个推导式中，就有这个概率的表达式的推导，直接拿来替换得到参数 \pi_{i} 的最终表达式： 然后求解状态转移矩阵A的元素，即 $Q(\lambda,\lambda^g)​$ 对A作偏导，令偏导为0，发现只有第二个项包含A，所以只考虑第二项。 由于当前式子中的 $a_{q_{t-1},q_{t}} $表示只考虑相邻时刻的两个状态，因此整个式子可化简为： 另外，我们记得A矩阵的每一行的元素之和都应当是1，即 $\sum_{j}^{k}{a_{i,j}}=1 $，因此相当于有k个约束条件，也可以当做一个包含k个等式约束条件的多项式约束问题，可以使用拉格朗日乘子法来解决，只不过此时引入的拉格朗日乘子应当为k个。 同样，分别对 $a_{i,j} ​$和 $\tau_{i}​$ 求偏导，令偏导为0。 得到 $a_{i,j}$ 的表达式为： 式3 对式3等式两边同时求和，得到： 由于A矩阵中的每一行概率和都为1，所以上式最后为： 然后将 $-\sum_{i=1}^{k}{\tau_{i}} ​$的表达式替换掉，得到 a_{i,j} 同理，使用前一章最后总结的三个推导式，可以带入该表达式中，以高效率的方式求出A： 备注：等式右侧的A和B矩阵是前迭代算出来的当前的概率矩阵，右侧是本迭代需要计算的概率矩阵。 最后求解发射矩阵B。同理，也是 $Q(\lambda,\lambda^g)$ 对B元素求偏导，并令偏导为0，注意到Q只有第三个term包含B，所以偏导如下： 由于式中只考虑t时刻的b的发射矩阵，所以该项中的很多求和项可以忽略，最后化简为： 注意到发射矩阵B的每一行概率和也为1，所以该等式中只考虑一个等式约束：$\sum_{j}^{k}{b_{j}(y_{t})}=1​$ ，可以使用拉格朗日乘子法： 分别对 $b_{j}(y_{t}) ​$和 $\tau​$ 求偏导，得到： 4式 5式 由4式得到 $b_{j}(y_{t}) ​$的表达式： 6式 同时对6式等号两边求和： 因为5式，所以上式可进一步化简，得到： 然后将 $\tau$ 的表达式反带入 $b_{j}(y_{t})$ 的表达式，得到最后的发射矩阵的表达式： 注意到等号右边的分母中，可以将外层的求和消去，同时为了引入前向后向算法的结果，引入 $q_{t-1}=i$ ,同时防止其影响原式子的概率，故对其积分，消除影响，得到： 用前向后向算法替换，得到： 备注：由于HMM的参数学习问题函数是非凸的，所以存在很多局部极值，用EM算法，很容易就得到局部极值了。所以参数的初始化很重要。Andrew Ng的notes说到需要重复多次不同的试验，保证在一定程度上到达全局极值。另外，还要注意平滑问题，即所有最后得到的表达式的分母都要注意为0的问题，所以最好是分母都增加一个平滑算法（如拉普拉斯平滑等）。 结尾呼。。。终于把HMM中最难的一部分总结完了，总的来说还是很累人的，不同教程里面的符号还有说法有的都不一样，而且有的教程中推导一笔带过，完全看得云里雾里，不过好在还是打通了。HMM系列还有最后一部分——维特比算法解决标注问题，希望在下一篇章节中完美收尾。 总结：HMM模型训练学习中用到的知识点如下：EM算法，前向后向算法，拉格朗日乘子法、贝叶斯公式以及各种消元的小trick。 reference： cs229 Andrew ng的相关notes 徐亦达老师的notes]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔科夫模型学习总结之二]]></title>
    <url>%2F2019%2F04%2F02%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8B%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[前章节主要是简单总结了马尔科夫模型的一阶情况，包括其原理、可以解决的问题。最后引出了隐马尔科夫模型。本章节就对隐马尔科夫模型进行一个总结。 隐马尔科夫模型与马尔科夫模型的区别在于”隐“字。”隐“代表了什么呢？在现实生活中，马尔科夫模型的研究对象往往是不可观测到的（上一章节已举出了例子），但同时存在很多对象我们是可以观测的，且受那些抽象对象的影响，此时我们可以引入隐变量表示无法观测的对象，我们可借由对可观测对象进行建模，来达到间接研究隐变量的目的，这就是隐马尔科夫模型能够做到的事情。 仍然以天气为例（虽然天气是可以观测到的，但为了简化描述，先用这个例子吧），我们不需要直接去对天气建模，而是将它作为隐变量，同时我们观测一些状态属性，如每天的温度、湿度、冷饮消耗量，这些属性的状态或多或少都依赖于天气的状态。 假设当前T时刻内的观测序列为 $\left\{ y_{1},y_{2},…y_{T} \right\} $,每个时刻的观测状态来源于观测状态列表 $V=\left\{ v_{1},v_{2},…,v_{k} \right\} $,其中 $y_{t}\in V$ 。隐变量状态序列仍用 $Q = \left\{ q_{1},q_{2},…,q_{T} \right\} $，每个时刻的隐变量状态来源于隐状态列表 $S=\left\{ s_{1},s_{2},…,s_{n} \right\}$ ，其中 $q_{t}\in S$ 。 在隐马尔科夫模型（以下简称HMM）的问题中，存在两个条件独立。 在给定t时刻的隐状态的条件下，t时刻的观测状态与其他时刻的隐状态是独立的。即当前时刻的观测状态只与当前时刻的隐状态有关 $P(y_{t}=v_{i}|q_{t}=s_{j}) =P(y_{t}=v_{i}|y_{1},…,y_{T},q_{1},…,q_{T}) $ 在给定t时刻的隐状态的条件下，前一个t-1时刻的隐状态与后一个t+1时刻的隐状态时条件独立的，即一阶马尔科夫性质。$P(q_{t}|q_{t-1},q_{t-2},…,q_{1})=P(q_{t}|q_{t-1})$ 用图表展示如下： 根据上述性质，我们仍然有状态转移矩阵A表示隐状态之间的转移概率情况。但是在该情况下，我们并不能通过直接观测来构造这个A矩阵。除此之外，我们新增了一个矩阵B用于表示隐状态生成观测状态的概率， $P(y_{t}=v_{i}|q_{t}=s_{j}) =P(y_{t}=v_{i}|y_{1},…,y_{T},q_{1},…,q_{T})=B_{ji}​$ ，该矩阵又叫发射矩阵。 另外，与马尔科夫模型类似，HMM的初始隐状态也是需要学习的参数之一，通常用 $\pi $表示，因此HMM的参数集可以用 $\lambda=\left\{ A,B,\pi \right\}$ 表示。 相对于马尔科夫模型解决的两大问题，HMM模型能够解决三种问题： 预测T时刻内的观测序列的概率 预测T时刻内最大概率出现的隐状态 学习HMM模型的参数 $\lambda ​$ 下面分别总结这三个问题的解法。 预测一个观测序列的概率这个的问题的前提条件是，已知模型的参数集 $\lambda$ ,给定一个观测状态序列，求出该序列的概率是多少。 该问题的解法比较简单，就是计算给定状态概率转移矩阵A、发射矩阵B，求给定观测序列与所有隐状态的联合概率之和： 上述公式可以用之前提到的HMM的两个性质来化简，举例： 上式等号右边的第一个乘式使用的是一阶马尔科夫性质化简，第二个乘式可以看出是与等号左边的式子的递归形式，因此该式子最终可化简为： 可以看到上式最后其实是状态转移矩阵和发射矩阵、初始状态的连乘形式，扩展到我们一开始的问题公式，该公式最后可化简为： 其中： 直接计算上式虽然可行，但是上式相当于要做 $K^{T} $个项的计算，当时间序列很长的时候，是难以计算的。因此引入了前向算法、后向算法、以及前向后向结合的算法来计算该式子。 前向算法 该算法用到了动态规划的思想。首先，我们引入一个中间变量 $\alpha_{i}(t) $,它表示时刻t的隐状态 $q_{t}=i $的情况下， $y_{1}…y_{t} $与 $q_{t}$ 的联合分布。 我们把 $\alpha_{i}(t) ​$的表达式进行展开和化简： t=1时刻的 $\alpha_{i}(t) $很好求，使用贝叶斯公式就可以求出来，而t=2时刻的如何求呢？我们之所以要引入这个临时变量 $\alpha_{i}(t) $，是为了能够让整个序列概率的求和公式进行简化，最好是以递归的形式进行简化，因此t=2时刻的临时变量必须要与t=1时刻的临时变量建立一个递归关系，故我们在计算 $\alpha_{j}(2)$ 时，引入变量 $q_{1}$ ，但是为了不改变原来概率的计算，通过积分（在离散情况下，相当于求和）把这个引入变量消去。最后化简为： 至此，可以建立递归的算法，最后： 因为 所以得到最后的计算式子。上面的算法，在每个时刻只需要计算k个项的求和，最后一共需k*T个项的求和，比传统方法快多了。 后向算法 后向算法与前向算法的思想是相同的，不同之处在于前向算法是从前往后迭代，而后向算法是从后往前迭代,同时建立的是基于t时刻的隐状态的条件概率，定义个临时变量 $\beta_{i}(t) $,它表示时刻t的隐状态 $q_{t}=i $的情况下， $y_{t+1}…y_{T}$ 基于 $q_{t}$ 的条件概率。如图： 类似前向算法，我们后向算法是从t=T,到t=1进行迭代，同时注意到当t=T时，由于 $q_{T} ​$已经确定，故 $\beta_{i}(T)=1​$ 。 计算T-1时，同样，为了建立与T时刻的迭代关系，故引入 $q_{T}$ ,同时为了不改变原概率计算，对其进行积分，消除该变量的影响。 由于 $y_{T}$ 只与 $q_{T}$ 有关，因此上式继续化简为： 一直计算前一个的递归，直到t=1 要得到最终的 $P(Y|Q)$ ,还需要通过贝叶斯公式，考虑t=1时刻的 $P(y_{1}|q_{1})*P(q_{1})$ ，因此最后得到的计算式为： 前向后向结合 除了以一个方向来进行迭代算法外，还可以从两头向中间的方式进行前向和后向的结合，就不多赘述了。原理与上述类似。 至此，HMM的第一个问题就可以解决了。在讲下一章训练HMM的模型参数问题前，我要讲三个推导，与前面所讲的前向后向算法有关，且后面的模型训练时会用到这三个推导。 推导1： 由于在给定 $q_{t}=i$ 的条件下，t时刻前面的Y和t时刻后面的Y应当是条件独立的，因此上式可化为： 可以看到与之前的前向和后向算法联系起来了。 推导2： 该式子可以得到，给定观测序列和模型参数的条件下，t时刻的隐状态的概率为对前向后向算法结合后的一种归一化。 推导3： 与推导2类似，但是添加了一个项，求 $P(q_{t}=i,q_{t+1}=j,Y|\lambda)$ ,即给定模型参数的条件下，求某两个时刻隐状态为某组合值与观测序列的联合概率： 将上式第一个乘子式根据t和t+1时刻进行划分，利用前向后向算法的思想进行化简： 由于 $y_{t+1} ​$是依赖于 $q_{t+1}​$ 的，因此需要将 $y_{t+1}​$ 单独提出来，如下： 上述推导在HMM模型训练时，计算状态转移矩阵和发射矩阵时都会使用到该推导。 预告：下一章讲解的HMM模型训练，会使用EM算法，迭代得去训练得到最后的模型参数集 $\lambda=\left\{ A,B,\pi \right\} $ reference: cs229 徐亦达老师的notes]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔科夫模型学习总结之一]]></title>
    <url>%2F2019%2F04%2F02%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E4%B9%8B%E4%B8%80%2F</url>
    <content type="text"><![CDATA[最近学习了徐亦达老师关于隐马尔科夫模型的教程和cs229中关于HMM的notes，把所有分散的知识点进行总结，加深自己学习的印象。（会分几个章节写，因为内容太多了。） 首先说明两个备注： 目前很多博客文章中对HMM模型的介绍以及举例其实有限定，即模型中的观测变量的概率分布是离散型的，比如扔色子、预测天气等比较形象的实例中，观测状态都是以某几个取值分类来决定的。除了上述离散型的概率分布外，观测变量的概率分布也有连续型的，比如预测股票的价格中，每天的股价其实是连续的值。在现实生活中，连续型的数据分布应当是更加常见的，但碍于篇幅，这次我讨论离散型的数据分布，当学习了卡尔曼滤波以及高斯分布等相关知识后，再补充连续型的数据分布的HMM。 本文只讨论一阶马尔科夫的情况，即当前状态只与前一个状态有关。还有更复杂的二阶以及其他变式HMM模型，本文不做讨论。 HMM模型适用于时间序列特征的数据，在语音识别、中文分词、词性标注等任务中都有比较好的表现，且其模型结构和原理相较于CRF以及其他复杂模型来说较容易理解，因此在深入自然语言处理研究前，掌握该模型的原理和推导是很有必要的，下面将详细对HMM的模型进行解析和推导。 马尔科夫模型首先总结一下马尔科夫模型的知识点。何为马尔科夫模型？对于时间序列数据来说，每个时刻t都有其独有的状态，本文只讨论该状态为离散的情况。即给定一组状态列表$S=\left\{ s_{1},s_{2},…s_{|n|} \right\}$,我们可以再时间序列上观察到这个状态列表中的元素的序列组合。以天气预报问题为例，假设当前 $S=\left\{ sun,cloud,rain \right\} ,n=3,$我们观察到近5天里的一个天气序列： \left\{ z_{1}=s_{sun},z_{2}=s_{cloud},z_{3}=s_{cloud},z_{4}=s_{sun},z_{5}=s_{cloud} \right\}在没有任何假设的情况下，每一天的天气状态可能与任何一个变量都有关系，这样我们就不能用统计语言去为现实问题建模了。为了简化问题，我们引入了一阶马尔科夫假设，即时刻t的状态概率只与前一时刻t-1的状态有关，即 P(z_{t}|z_{t-1},z_{t-2},...,z_{1})=P(z_{t}|z_{t-1})满足该假设性质的概率模型，可以称之为一阶马尔科夫模型，或者叫一阶马尔科夫链。若某时刻t的状态与前一个时刻以及前两个时刻的状态有关，则为二阶马尔科夫链。以此类推。 同时上述公式无法表示第0时刻的状态概率，因此还需要引入一个初始状态 $z_{0} $表示第0时刻的状态。在HMM模型中，初始状态通常用 $\pi$ 表示。 根据对样本数据的状态转移统计，我们可以得到一个状态转移矩阵 $A\in\Re^{n*n} $，表示某个状态转移到另一个状态的概率，其中矩阵元素 $A_{ij}$ 表示任意时刻t状态i转移到状态j的概率。 由该矩阵可知，该矩阵每一行的概率和均为1，即 $\sum_{j}^{n}{A_{ij}}=1$ （该结论在后面推导时会有用。） 马尔科夫模型的两个问题结合上述我们队马尔科夫模型的描述，其实我们可以利用上述性质，解决时间序列特征的数据集的两个问题。 已知模型的状态转移矩阵，和初始状态 $\pi $，假定在时刻t内，一个状态序列 $z_{set}$ 出现的概率是多少？ 如何估计我们模型中的参数，即状态转移矩阵和初始状态，来最大化一个观察序列$z_{set}$的最大似然估计？ 问题一：计算某个状态序列的概率 可以通过链式法则来计算 $P(z_{set}) $ 根据我们讲过的一阶马尔科夫假设，当前时刻t的状态只与前一个时刻的状态有关，即 $P(z_{t}|z_{t-1},z_{t-2},…,z_{1})=P(z_{t}|z_{t-1}) $可以将上式进行化简，得到： 即一个状态序列的概率就是将每个时刻对应前一时刻状态的状态概率进行连乘得到。 问题二：极大似然参数估计 本问题实质上是求出模型的状态转移矩阵，使得观察序列 $z_{set} $似然估计最大。一般我们会对似然估计函数求对数，方便求极值。 首先这个似然函数是有约束条件的 每个概率值都应该是非负的，即 $A_{ij}\geq0 ​$ 矩阵A的每一行的概率和应当是1（用到前面的结论。） $\sum_{j}^{n}{A_{ij}}=1$ 面对这种带不等式和等式约束条件的优化问题，自然我们想到用拉格朗日乘子法进行优化。 注意到由于我们对A矩阵的每一行均有一个等式约束条件，因此一共有n个约束条件，所以拉格朗日乘子也有n个，用 $\alpha_{i}​$ 表示。 对参数 $A_{ij} $求偏导数，令偏导数为0。求得参数的值： 上式中， $1\left\{ z_{t-1}=s_{i}\wedge z_{t}=s_{j} \right\} ​$表示t-1时刻的状态为$s_i​$与t时刻的状态为$s_j​$同时发生时对该项取1，其他项取0。 得到状态转移矩阵元素值为： 由于表达式中还有拉格朗日乘子，故还需要对乘子进行求偏导数，得到乘子的表达式。 将之前得到的 $A_{ij} $的表达式代入上式中，即可得到 $\alpha_{i}$ 的表达式。 求得乘子后，再把乘子反代入到 $A_{ij} $的表达式中，就可以得到状态转移矩阵的元素。 根据求得的矩阵元素表达式可以看出，其实我们是根据我们样本数据中状态之间的真实转移频率来近似模型的状态转移概率的，这个是属于频率学派的思想，即对事物本身进行真理的探讨，直接对事物本身进行建模。与之相对的还有贝叶斯学派，他主张事物的不确定性，用概率分布去接近事物的本质。关于这两个学派的区别，感觉还是有点模糊，准备在将来单独开篇学习总结一下。 通过上述的描述，可以看到马尔科夫模型对时间序列数据能够有一个强力的抽象，方便我们使用统计方法对现实世界的数据进行建模分析。但是该模型具有一定的局限性，首先区别于天气，某些事物的状态我们是不能直接观测到，比如股市的熊市和牛市状态、语音识别中具体的语言语素、文本序列中的词性等等。此时直接用马尔科夫模型就不适用了，因此推出了隐马尔科夫模型，通过引入隐变量来表示无法观测的状态变量，来解决该问题。 关于HMM模型的具体总结，见下一章节。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>HMM</tag>
        <tag>Markov assumption</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拍拍贷文本相似度计算大赛总结]]></title>
    <url>%2F2019%2F04%2F01%2F%E6%8B%8D%E6%8B%8D%E8%B4%B7%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E5%A4%A7%E8%B5%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[18年6月下旬到7月上旬期间搞了个拍拍贷的NLP比赛，做的是文本相似度计算，本来是准备做蚂蚁金服的文本相似度比赛，不过它需要的开发环境比较麻烦，后来看拍拍贷的这个比赛只需要上传结果，而且词向量都帮你训练好了，于是就参加了拍拍贷的。最终成绩勉强进复赛，比不上排在前面的大佬。说是总结，其实没有什么很好的经验，毕竟名次很低，不过一来通过这次比赛，我熟悉了pytorch这个框架，趟了一些坑，二来也是对这个NLP任务有了一定的经验，所以在这边记录一下，如果对其他人有帮助，那就最好，如果我的总结有不对的地方，还请指正。 任务描述首先，本次NLP任务个人感觉应该是偏paragraph idenfication的，即两个文本表示虽然表示形式不同，但是表达内容是相同的。其实文本相似度计算是一个很大的分支，除了刚刚提到的之外，还有文本蕴含（Textual Entailment），即AB两种表达，通过A表达能否推断出B表达的真实性，举个例子就是：A：支付宝不能在超市付款，B：支付宝不能线下付款。这个B真实的例子，也有B是不真实或者B不确定是不是真实的情况。另外还有一种为relatedness，这个感觉与paragraph idenfication类似，但是更加广义，通常它是对两个文本评相似度的程度，并不是非黑即白。 关于文本相似度的相关知识我推荐一个文章，是我很佩服的一个大佬夕小瑶Elsa，感觉很有实际的借鉴意义：如何匹配两段文本的语义？ Traditional model这次比赛参加晚了，所以传统特征+xgboost虽然搞了几天，但是没有仔细搞，而且这个比赛的数据是脱敏的，所以一些文本预处理、语法、句法相关处理就没办法做了，所以主要是借鉴了以前参加quora的大神的一些特征提取方法和思路。大致分为以下几点： 1、从n-gram角度计算两个句子之间的各种相似度计算，包括jaccard，SimHash+海明距离等等，其中n-gram对词和字两个方向都进行计算，我取了1-4个gram。jaccard相似度计算很简单，公式如下： 其实就是计算两个句子之间的重复程度，A,B是两个集合，在这个任务中，可以将AB看做是两个用n-gram组成的词、词组集合。实际使用的时候，还会增加J分别被A和B长度归一化后的重复比率作为新特征。 而SimHash是一种将一段文本hash成固定长度，然后用于计算文本间相似度的方法，其原理简单说就是对文本中的每个特征，也就是词或者ngram词组，用传统hash方法分别生成一个唯一的签名如固定8位长度的二进制码10010000，其中1表示该特征的权重在该位上为正，0则为负，权重一般是某个词的次数，也可以是tfidf值，比如10010000对应词a权重为w，对应的表示为w-w-ww-w-w-w-w,然后将所有特征的权重按照签名后的表示按位相加，最后得到一个和向量x，该向量维数与hash的签名二进制码长度一致，假设某一维上的数值大于0，则置为1，否则置为0，最后得到的向量表示可以代表一段文本。如图： 其实很好理解，就是对一篇文档中的每个基本元素用一个唯一的随机hash字串表示，然后将这些字串通过字串的权重进行加权求和，就得到一段文本的表示。 之后使用海明距离计算，即两个码字的对应比特取值不同的比特数。这个是很简单的距离计算了。 2、一些NLP方面的传统特征，如wmd，欧式距离、余弦相似度、最长字串匹配、曼哈顿距离、皮尔逊相关系数，还有各种统计学上的相关性计算。这里其实重点在于如果将一个句子向量化。我用了三种方法：直接将句子的所有词向量做平均，得到句子的向量表示；通过计算每个词的tfidf值，然后将词向量做加权平均，权重为词对应的tfidf值；训练doc2vec,得到句子的向量表示。不过由于是脱敏数据，doc2vec是需要段落信息的，所以这个任务下训练doc2vec可能效果不明显。 3、fuzzywuzzy特征，这是一个开源的工具包，里面定义了对文本进行模糊匹配的一些api。 4、图特征，这类特征我研究时间最少，但是个人感觉应该是很重要的，因为句子的相似度计算，可以看成是两个句子作为图的节点，在领域上的接近程度。我主要是使用了pagerank算法，构建了句子的有向图，每个句子都有一个权重，表示这个句子的重要性程度，一般重要性程度越高，表示有越多的节点和它是相似的。最后将节点权重作为一个句子的一个特征。另外一个我没有做的就是，句子相似度的传递性，感觉这也是可以通过图来分析，即AB相似，BC相似，是否可以推出AC相似。而AB不相似，BC相似，是否可以推出AC不相似，等等。 5词共现矩阵。之前讲词向量和glove时，介绍过词共现矩阵也可以用于NLP任务。因此这边就使用了，主要是通过两个角度，一个是原始的词共现矩阵，每个句子都可以用词共现矩阵中的向量来表示；另外，由于原始词共现矩阵是一个稀疏矩阵，所以一般都会对该矩阵做SVD降维，然后再处理。 6、这个是我来不及做的，就是通过topic model，对文本建立主题模型或者隐语义分析。这个后续我会专门开辟一个坑来讲LDA。因为LDA个人感觉在传统方法中还是很好的，但是它的原理有点复杂，牵扯到各种统计分布、采样方法、EM等知识，所以对于不擅长统计概率学的我来说还是有点挑战性的。 综上，最后通过一些特征筛选方法，如皮尔逊相关系数、以及ensemble方法中的特征重要性计算，最后筛选出50多个特征，用xgboost训练，大概能得到0.25左右的分数。 这里吐槽一下，这次比赛基本没有用cv或者stacking，因为电脑硬件实在跟不上。跑一次简单的留一法，都要1-2个小时，唉，没有服务器搞比赛真是伤不起。 Deep model接下来就是时间花费最多的deep model。这次做比赛，我有一个目的就是借此机会学习pytorch这个框架。因为相对于tensorflow，感觉它更加方便，最重要的就是可以直接debug！！！而且python-style的风格实在是用的很爽。安利一波。不过，用的时候也有各种坑，好在它的社区好不错，上面回答问题的同学都很nice。 这次我主要是参考了COLING 2018的 Best Reproduction Paper，《Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering》，这篇文章可以说是将最近几年的一些state-of-art的model都罗列了出来，并做了实现和分析。因此我实现的模型就在这篇论文里了。 先简单介绍一下deep model在paraphrase identification中的大致分类。一般有两种实现方式。 1、一种是基于sentence encoding方式，即重点在于使用一些方法将句子进行编码，争取在编码后最大获取句子的各种信息，然后对两个待比较的句子编码做一些距离计算，如欧式距离、余弦相似度、曼哈顿距离，最后接几层全连接层，得到分类的输出，有的模型就是将两个句子编码直接接全连接层。这种方式虽然简单粗暴，但是效果还是不错的，我就是使用了这种方式，最好的有0.21的分数。这种方式的最大弊端在于训练速度实在是太慢了，因为它的参数量太多了，基本上一次训练都要2-3个小时。 2、另一种是基于交互的方式，即会对两个句子做一些词对齐的机制，比如用attention机制或者其他方法，然后将句子之间的交互信息拼接起来。这种方法的优势在于它的参数量实在是不多，对于我这种硬件不行的参赛者可以说是很友好了，基本上训练一次，只需要40-50分钟。效果也不错，大概在0.23左右。我觉得这种方式应该是潜力最大的，但是我后来实在是没时间调参了。希望有大佬能够给出一些参数建议，也让我体验一下单模型0.17+的快感。 基于交互的model 基于交互的model，重点在于如何将两个句子进行“交互”。我主要参考了DecAtt模型，即decomposable attention model，论文见：https://arxiv.org/abs/1606.01933 它的交互分为三个步骤，如图所示： attend：主要是对两个句子进行对齐处理。具体做法是先对两个句子分别使用一个全连接层+relu做线性和非线性变换，然后将变换后的两个向量做矩阵乘法（将一个向量转置后乘），得到一个n*m的矩阵（其中n和m分别为两个句子的长度）。这个矩阵就是一个未归一化的attention权重，然后分别对两个句子做权重的加权平均得到两个对齐后的句子表示。假设原句对为，得到的对齐表示为 compare：将上面得到的两个对齐句子a,b表示分别与bi,ai做比较，具体比较方式为接一个全连接层，最后得到句子的比较表示va,vb aggerate:对va和vb分别沿这时间维度求和，然后将两个句子表示做拼接，送到后面的分类层。 可以看到这个模型使用了attention机制，在参数量很小的情况下，就能获取两个句子之间的交互关系信息。可以说是把attention机制的优势发挥出来了。 具体实现时，有一个坑要重点说明，就是pytorch对padding后在句子的处理比较麻烦。即在NLP任务中，不同样本数据文本长度会不一样，而做minibatch训练时，需要一次送入多个样本数据，因此需要将一个batch中的所有文本先对齐到同一长度，因此会先做pad操作，即补0.但是在计算attention时，尤其是做attention加权平均时，补0的那部分信息不能作为平均的分母，即归一化分母应当只算原本句子的那些词。经过多方查阅，我最后实现了以下的方法，来解决这个问题： 1234567891011121314151617181920def attention_softmax3d(self, raw_attentions,lengths): """ :param raw_attentions: shape:batch_size,maxlen1,maxlen2 :param lengths:句子原始长度 len2 :return: """ # 待加权句子pad后的长度 max_len = raw_attentions.size(-1) # 创建一个1*maxlen的数列tensor，范围从0到maxlen-1 idxes = torch.arange(0,max_len,out=torch.LongTensor(max_len)).unsqueeze(0).cuda() # 创建mask，batch_size,1,maxlen,其中句子原始长度len以内的地方都置为1，其他位置都置为0 mask = Variable((idxes&lt;lengths.unsqueeze(1)).float()).unsqueeze(1) max_logits,_ = raw_attentions.max(dim=2, keepdim=True) # 算归一化权重时，乘以mask，使得pad为0的位置都不会被计算在内 masked_logits_exp = torch.exp(raw_attentions-max_logits) * mask logtis_sum = torch.sum(masked_logits_exp,dim=-1,keepdim=True) # out = nn.functional.softmax(raw_attentions, dim=1) out = masked_logits_exp/logtis_sum return out 尝试改进点：主要是对几个全连接层的单元数调参，还有就是在句子input representation层上，在embedding_lookup之后，接一个双向rnn层（BIlstm）。添加rnn层，虽然在一定程度上提高了local分数，但是在线上分数却降低了，推测应该是过拟合了，由于加了lstm之后训练速度明显变慢，时间不够，所以没有继续调下去。 基于encoding的model 基于encoding的model，核心在于如何使用一些方法捕捉每个句子的核心信息特征，然后将这些特征加到句子表示中，最后通过一些距离、相似度计算，来得到分类依据。 一开始，我参考的模型是SSE，即Shortcut-stacked BiLSTM，原始论文：[1708.02312v2] Shortcut-Stacked Sentence Encoders for Multi-Domain Inference 它的结构如下： 核心主要有三个： 1、使用了三层BiLstm，使用了残差网络的思想，会将之前原始的句子表示以及前面lstm层的输出做拼接，作为下一层lstm的输入。这样做，能够最大化三层lstm的学习能力，防止网络在层数增加到一定程度时，无法提升性能。 2、对lstm的最终输出做了over time max pooling，我试了average pooling，效果还是没有max pooling好。 3、最后得到的两个句子的encoding输出v1,v2，然后计算v1、v2的各种距离相似度，个人感觉|v1-v2|和v1*v2很有用，首先他们对于两个句子的比较与被比较关系是相同的，即AB和BA的结果是一样的，这也符合常理，因为在本任务中，AB相似度本身应该是没有方向性的。后面陆续试了余弦相似度，欧式距离，效果均没有明显提升。 具体实现情况是，我的电脑根本跑不动三层lstm！！！。一个小时连一个epoch都跑不完。最后无奈只能减小lstm的单元数，原本是[512,1024,2048],最后被我改到[64,128,256]，才勉强可以在我的耐心极限内跑完，当然效果不好，才0.24左右。 后来我看了Infersent这篇model，感觉一层lstm应该也会有很好的编码效果，所以试了一下，直接把一层的lstm单元数设为2048，最后效果居然达到了0.21。 嗯，后来经过分析，发现在机器性能有限的情况下，与其堆叠rnn层数，不如调整rnn单元数和最后全连接层的层数和单元数的比例来得实际。事实上，当lstm的输出维度达到一定的程度时，如果后面的全连接层层数或单元数达不到相应的量级，就无法完全适配前面lstm编码得到的信息。当我的lstm单元数为512时，后面全连接层设为三层，单元数设为3000-4000时，模型效果能达到0.21。当然这是我总结出来的经验，不知道有没有理论依据，如果有大佬了解相关理论，欢迎交流，我对理论知识的探讨是很渴求的，本身对这方面就一直在学习。 当然，关于pytorch实现还有一个坑需要说明一下，也是关于句子padding的问题，tensorflow对于rnn的padding处理有专门的api:dynamic_rnn。但是pytorch貌似没有，但是它有专门两个api处理padding的句子：pack_padded_sequence，pad_packed_sequence 一般rnn处理变长的经过padding的句子序列过程如下： 1、使用pack_padded_sequence将句子序列变为PackedSequence对象，这个对象封装有方法，能够让rnn对象识别哪些是pad的位置。但是，它需要输入的一个batch中的句子序列是按照长度降序排列的，原因在于它内部的实现机制，具体就不讲了，有兴趣的同学自己可以看看源码，略复杂。因此这一步要先将原始的batch中的句子重排序，有一点一定要记住，就是要保存原始的句子在batch中的id的位置。后续在rnn处理后，需要重置句子的顺序。 1234567891011121314151617181920def sort_lengths(data, length_list): """ 将句子按照长度进行降序排序，已满足pack_padded_sequence的需要 :param data: 待排序的句子序列 :param length_list:句子原始长度列表 :return: """ #将一个batch中的句子长度列表按照句子长度降序排序 sorted, idx_sort = torch.sort(length_list,dim=0,descending=True) #保存原始的未经过排序的id列表 _, idx_unsort = torch.sort(idx_sort, dim=0) sent_input_list = [] new_lengths_list = [] #根据排序后的id列表，存放一个batch中的句子 for i,index in enumerate(list(idx_sort)): sent_input_list.append(data[:,index,:].unsqueeze(1)) new_lengths_list.append(length_list[index]) sent_inputs = torch.cat(sent_input_list,dim=1) return sent_inputs,idx_unsort,new_lengths_list 排完序后，用pack_padded_sequence生成PackedSequence对象 1234#排序后的batch句子，原始的batch中句子id列表，排序后的句子长度列表sort_seq,orig_order_idx,sort_lengths_list = sort_lengths(seqs,length_list)#将batch的句子pack成专用对象packed_seq = nn.utils.rnn.pack_padded_sequence(sort_seq,sort_lengths_list) 2、使用RNN单元接受PackedSequence对象，它会像dynamic_rnn一样自动处理pad的位置，不算其梯度。得到output后，由于后续有其他操作，仍然需要将变长的序列对齐，因此调用pad_packed_sequence将句子补齐，这是pack_padded_sequence的反操作。 12output,(h_t,c_t) = lstm_unit(packed_seq,(h_0,c_0))output,_ = nn.utils.rnn.pad_packed_sequence(output) 3、但是由于之前对一个batch的句子做了重排序，然而我们的label并没有重排序，如果直接就送入后面的操作，计算loss时，得到的结果肯定是不对的。因此需要将句子按照原来的顺序排列，这时就要用到第一步排序时的副产物：原始句子id list。 123456#reordering，根据原始的存放顺序，将句子在batch中的顺序归位。reorder_output_list = []orig_order_idx = list(orig_order_idx)for index in orig_order_idx: reorder_output_list.append(output[:,index,:].unsqueeze(1))reorder_output = torch.cat(reorder_output_list,dim=1) 总之，绕了一个圈子，把相当于dynamic_rnn的代码实现了一下。如果有大佬有更好的实现凡是，欢迎给出建议。 deep model+传统特征后续我将筛选出的传统特征，加入到了deep model中，效果是有提升的，大概有5%左右的提升。不过由于时间关系，也没有尝试其他融合方式。 总结至此，我要总结的也差不多了。这次比赛，重在参与，虽然只是勉强进了复赛，但是还是学到了很多东西，尤其是熟悉了pytorch这个框架。然而也有很多想法没有去实践。 比如数据增强，这个脱敏数据如何做数据增强？可以通过图的建立，将间接相似的句子对找出来，也可以将间接不相似的句子对找出来，添加到样本中。 比如deep model，做深做大，或者尝试其他更复杂的模型，因为有大佬单模型能达到0.16左右，这样看来，我跟大佬的差距还真是蛮大的。。。 再比如stacking，这个是我一直想做的，但是一来是机器硬件跟不上，二来是单人参加比赛，实在是没时间，精力不够，只能在下班时间埋头苦干。下次比赛，争取找点队友，或者找个靠谱的机器。 最后说一句，希望有大佬能够开源，至少能够讲讲自己的模型思路，让我们这些菜鸟能够多多学习。]]></content>
      <categories>
        <category>比赛总结</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>attention</tag>
        <tag>paraphrase identification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于提升python程序效率的一些思考]]></title>
    <url>%2F2019%2F04%2F01%2F%E5%85%B3%E4%BA%8E%E6%8F%90%E5%8D%87python%E7%A8%8B%E5%BA%8F%E6%95%88%E7%8E%87%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[本文难得水一篇，讲一讲最近在工作中遇到的一些关于python编程的效率问题，希望对其他同学有一些启示，有些东西是网上其他博客就有的，感兴趣的同学可以自行搜索，深入学习。 问题起因是这样的：当前有一个pandas的dataframe结构A，A存储了一个数据序列中的各个字段的值，A的每一行表示时刻t的序列数据的具体内容。当前，需要对A进行归一化整合，使之转化为一段hash后的字符串，中间当然省略了一些很复杂的处理逻辑（包括各种字符串处理，列表查询、字典查询、hash转换等，注意加粗的两个操作，后面会提到）。一个dataframe里面的record数量从100+到5000+不等，DB中存有几十万的dataframe。 传统的单纯使用循环操作，逻辑代码如下： 123for dataframe in dataframe_list: for index, row in dataframe .iterrows(): do something complex........ 可想而知，稍微懂开发的人肯定会说这个效率太慢了。事实上，确实很慢，万级别的数据我运行了整整24小时才跑完。这对于任何一个开发人员来说都是不能忍受的，所以要想办法提升效率。因此我做了以下一些尝试，将运行时间从24小时，逐步地缩减至了1小时。可能还有一些提升空间，如果有同学对pandas或者python高性能编程有一些好的见解，欢迎留言，一起探讨，这方面我还是一个菜鸟。 1、使用python的multiprocessing，主要是使用python的多进程并行，至于python的多线程，熟悉python的都知道，几乎没有传统多线程并行的优势，它的运行机制本质还是单线程的。主要是使用multiprocessing.pool的map方法，但是我的机器目前的cpu核数并不多，才四核，最后试了下，提升效果并不明显，而且最后数据整合花费的时间还是不少的，性价比比较低。 2、然后研究了pandas的map，apply，其中，map的对象是series，即我不能传整个dataframe进去。实际情况是我循环内的操作是要用dataframe里的每个字段的数据的，因此map不适用。apply的话，可以适用，同时由于新的pandas版本中支持了apply的function适用多个参数，因此用起来也挺方便，内层的循环可以去掉，换为： 1dataframe.apply(function,args=(arg1,arg2,....),axis=1) 但是，实体测试之后，发现，这种方法几乎没有显著的提升，对于1000条数据的性能，只提升了几秒。后来，google查问题，发现pandas的apply方法对于rowwise的遍历性能是很差的，相反它对column_wise的列遍历倒是效果很好。因此和传统的for循环相比，apply on row几乎没有太大优势。 3、后来又想到了numpy的向量化操作，如果能将上述逻辑转化为向量单位进行操作，会不会有提升？因此就查到了numpy的vectorize方法： Define a vectorized function which takes a nested sequence of objects or numpy arrays as inputs and returns an single or tuple of numpy array as output. The vectorized function evaluates pyfunc over successive tuples of the input arrays like the python map function, except it uses the broadcasting rules of numpy. 简单说就是能将一个function改造成numpy支持的向量化操作。当然，这个function的参数是有要求的，dataframe或者series肯定是不行的，因此我将本来传入的参数dataframe按列拆开来（幸好列数不多），将每列以ndarray的形式作为参数传入function，然后再用numpy的vectorize改造： 12345def do_function(array_1,array_2,.....): do something return somethingvec_fun = np.vectorize(do_function)result = vec_fun(array_1,array_2,.....) 经过实际测试，性能确实有提升，对于1000条数据的性能，提升了30-40秒左右。 4、上述提升仍然是不够的，注意到我外层还有一个循环，能够对这个循环进行优化呢？答案是可以的。python3中，对于列表递推式的实现有优化，内部使用了generator迭代器来实现，比普通的for循环的性能更好，因此外层的循环用列表递推式来优化： 1[vec_fun(dataframe) for dataframe in dataframe_list] 经过实地测试，性能确实也有提升，对于1000条数据的性能，提升了20秒左右。 5、到了这个时候，一开始我也不知道该怎么优化下去了。对于循环来说，我能做的已经做了，那么只能去循环内部查看哪些代码块可以进行性能优化。经过漫长的分析，终于发现有一个地方优化后，对于性能会有一个惊人的提升。之前一开始我提到了我的内部循环逻辑中，有列表查询和字典查询，这个就是制约我提升性能的最大敌人！由于功能需要，我需要维护很多字典和列表供字符串处理时进行一些查找操作。这些字典列表规模都很大，基本都有十万-百万级。如果用传统的if somethin in list来进行查找，它的时间效率应该是O(n),随着列表规模的增大而增大的。通过google搜索，发现Set数据结构能够优化查找的效率，它首先会对列表进行去重，然后使用类似于红黑树的数据结构，这样进行查找的时候，它的效率应该是O(logn)。因此将所有list用set重新构建后，发现性能有惊人的提升，对于1000条数据的性能，提升了200秒左右！ 经过上述优化后，对于万级别的数据，本来需要24小时才能跑完，目前只需要1个小时的时间就能全部完成，优化效果显著。 当然，优化之路没有终点，我也只是对代码进行了初步的优化，可能还有一些奇技淫巧能够让python程序得到提升，希望对这方面有研究的同学来探讨。]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention机制简单总结]]></title>
    <url>%2F2019%2F03%2F31%2FAttention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本来是想接CMU的NLP公开课的坑的，但是发现讲attention这块内容的公开课程质量着实不高，教授说话声音模糊。（说实话这门公开课的质量都不太好，对萌新不太友好，不过它比斯坦福的cs224n内容多一点，建议学完cs224n再来看这门课会好一点。）再加上最近做一个headline generation的比赛，要用到的attention的地方很多，因此把之前比赛中以及课程中的attention知识简单总结一下，会参考到著名的论文：Attention is all you need。感兴趣的可以去读这篇论文，真的是一篇有划时代意义的论文。 备注：由于attention自被广泛运用以来，涌现出了很多attention的变体，各种奇技淫巧层出不穷，本文篇幅有限，只会讲几个最基本的attention方法。 What is attention？先简单描述一下attention机制是什么。相信做NLP的同学对这个机制不会很陌生，它在Attention is all you need可以说是大放异彩，在machine translation任务中，帮助深度模型在性能上有了很大的提升，输出了当时最好的state-of-art model。当然该模型除了attention机制外，还用了很多有用的trick，以帮助提升模型性能。但是不能否认的时，这个模型的核心就是attention。 attention机制：又称为注意力机制，顾名思义，是一种能让模型对重要信息重点关注并充分学习吸收的技术，它不算是一个完整的模型，应当是一种技术，能够作用于任何序列模型中。 Why attention？照例讲一下为什么要引入attention机制。在之前总结过的seq2seq模型以及之前做NLP的比赛中，对于一段文本序列，我们通常要使用某种机制对该序列进行编码，通过降维等方式将其encode成一个固定长度的向量，用于输入到后面的全连接层。 一般我们会使用CNN或者RNN（包括GRU或者LSTM）等模型来对序列数据进行编码，然后采用各种pooling或者对RNN直接取最后一个t时刻的hidden state作为句子的向量输出。这里会有一个问题： 常规的编码方法，无法体现对一个句子序列中不同语素的关注程度，在自然语言中，一个句子中的不同部分是有不同含义和重要性的，比如上面的例子中：I hate this movie.如果做情感分析，明显对hate这个词语应当关注更多。当然是用CNN和RNN能够编码这种信息。但是如果序列长度很长的情况下，这种方法会有一定的瓶颈。拿CNN举例，具体如下图：图来自“变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统 CNN的核心就是卷积核能够变相学习n-gram的信息，如果是用hierarchical的卷积核，那么越上层的卷积核越能编码原始距离较远的词组的信息。但是这种编码能力也是有上限的，对于较长的文本，模型效果不会再提升太多。RNN也是同理。 基于参加达观文本分类的经历，对于这种长文本处理，使用RNN+attention的效果比使用单纯的RNN+pooling的效果要好不少。 How to use attention?首先讲一下attention最基本最抽象的流程。以seq2seq模型为例。 基本流程：对于一个句子序列S，其由单词序列[w1,w2,w3,…,wn]构成。 1、应用某种方法S的每个单词 $w_i $编码为一个单独向量 $v_i$ 。 2、解码时，使用学习到的注意力权重 $a_i $对1中得到的所有单词向量做加权线性组合$\sum_{i}^{}{a_iv_i}$. 3、在decoder进行下一个词的预测时，使用2中得到的线性组合。 具体构成如下： 我们的最终目标是要能够帮助decoder在生成词语时，有一个不同词语的权重的参考。在训练时，对于decoder我们是有训练目标的，此时将decoder中的信息定义为一个Query。而encoder中包含了所有可能出现的词语，我们将其作为一个字典，该字典的key为所有encoder的序列信息。n个单词相当于当前字典中有n条记录，而字典的value通常也是所有encoder的序列信息。 上面对应于第一步，然后是第二部计算注意力权重，由于我们要让模型自己去学习该对哪些语素重点关注，因此要用我们的学习目标Query来参与这个过程，因此对于Query的每个向量，通过一个函数 $a_i = F(Q_i,K) $，计算预测i时刻词时，需要学习的注意力权重，由于包含n个单词，因此， $a_i$ 应当是一个n维的向量，为了后续计算方便，需要将该向量进行softmax归一化，让向量的每一维元素都是一个概率值。 最后对Value vectors进行加权线性组合，得到带权重参考的“字典”输出： 权重计算函数 眼尖的同学肯定发现这个attention机制比较核心的地方就是如何对Query和key计算注意力权重。下面简单总结几个常用的方法： 1、多层感知机方法 a(q,k) = w_2^Ttanh(W_1[q;k])主要是先将query和key进行拼接，然后接一个激活函数为tanh的全连接层，然后再与一个网络定义的权重矩阵做乘积。 这种方法据说对于大规模的数据特别有效。 2、Bilinear方法 a(q,k)=q^TWk通过一个权重矩阵直接建立q和k的关系映射，比较直接，且计算速度较快。 3、Dot Product a(q,k)=q^Tk这个方法更直接，连权重矩阵都省了，直接建立q和k的关系映射，优点是计算速度更快了，且不需要参数，降低了模型的复杂度。但是需要q和k的维度要相同。 4、scaled-dot Product 上面的点积方法有一个问题，就是随着向量维度的增加，最后得到的权重也会增加，为了提升计算效率，防止数据上溢，对其进行scaling。 a(q,k)=\frac{q^Tk}{\sqrt{|k|}}我个人通常会使用2和3，4。因为硬件机器性能的限制，1的方法计算比较复杂，训练成本比较高。 self-attention关于attention有很多应用，在非seq2seq任务中，比如文本分类，或者其他分类问题，会通过self attention来使用attention。这个方法思想很简单，而且计算成本相对来说不高，强烈推荐。具体来说就是： Query和Key，value都是相同的，即输入的句子序列信息（可以是词向量lookup后的序列信息，也可以先用cnn或者rnn进行一次序列编码后得到的处理过的序列信息。）后面的步骤与上述的都是一样的： 1、首先建立句子序列中的每个词 $q_i $与句子其他词k的注意力权重 $a_i$ 2、然后将注意力权重向量进行softmax归一化，并与句子序列的所有时刻的信息（词向量或者rnn hidden state）进行线性加权。 这种方法中，句子中的每个词都能与句子中任意距离的其他词建立一个敏感的关系，可以说在一定程度上提升了之前所说的CNN和RNN对于长距离语义依赖建模能力的上限。下图同样来自：“变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统 Multi-head attention下面介绍另一种很有效的attention使用方法，叫multi-head attention。这个是在Attention is all you need这篇论文中被使用。图例如下： 公式化表示如下： multihead(Q,K,V)=concat(head_1,head_2,...,head_h)W^o\\ head_i=attention(QW^Q,KW^K,VW^V)解释一下，与原来的self-attention的核心原理其实是差不多的，但是由于self attention只从一个角度去学习关注点，可能会有点偏颇。所以，设计h种不同的 $(W_i^Q,W_i^K,W_i^V) $权重矩阵对，然后做基本的attention操作前，将query，key和value分别用上述权重对做线性变换，然后再计算得到h个不同角度的attention权重 $head_i$ ，将这些 $head_i$ 按列拼接后，再与一个新的权重矩阵 $W^o$ 做线性变换，得到最终的attention输出。 这里要重点说一个问题，就是关于multihead的每个权重矩阵的维度。做self attention时，若使用Bilineard方式，也有一个权重矩阵W，这个权重矩阵维度一般是与Q、K、V的维度是一样的（通常做法，也有不一样的，但是维度不会低）。但是做multihead时，如果对每个权重矩阵的维度还是设为原始维度，那么计算的成本将会蹭蹭得往上涨。如果硬件性能不太行，极有可能会报OOM问题。（不要问我怎么知道的，惨痛的经历。。。）所以通常的做法是：设三种权重矩阵 $(W_i^Q,W_i^K,W_i^V)$的维度分别为 $(d^q,d^k,d^v)$ ,原始维度为d,那么 d^q=d^k=d^v=d/honly attention without RNN?看到有些文章或者博客对attention is all you need中的transformer模型的解读，可能不太完整，似乎只要用attention，就能秒杀其他任何模型。有些同学可能觉得以前的RNN，CNN都不需要了。其实个人感觉，将RNN与attention结合是能拿到好的结果的。为什么transformer没有使用RNN也能对序列数据有很好的建模效果？因为它用了很多其他的trick，首先就是使用了positional embedding，即将词语在句子中的位置关系也做了embedding，它的目的就是通过与原始词向量和attention结合，构建词序上的关系信息，这样就省去了rnn的网络结构，使得训练成本大大降低。但是它的embedding的初始化是一种带正弦函数和余弦函数的数学先验很强的一个方法，可能对其他任务不是太适用，且调这个参数也是比较难的。因此对于个人学习或者研究来说，可以尝试，但是使用RNN还是一个比较稳定的方法，虽然它的训练还是很比较慢的。。。 后记本文只是简单讲了几个最基础的attention方法，还有很多有效复杂的方法未涉及，比如global attention，local attention，hard attention等等，还有之前做文本分类时了解过的HAN（hierarchical attention network，是先对一个句子里面的词做attention，然后对文章中的句子做attention，相当于做了一个二层结果的层次attention）有兴趣的同学可以直接看论文了解。 引用： cmu nlp公开课 “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[seq2seq之tensorflow源码解析]]></title>
    <url>%2F2019%2F03%2F31%2Fseq2seq%E4%B9%8Btensorflow%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[很直接的说，这篇文章就是对tensorflow的seq2seq大礼包的源码做了一定程度的解析。个人一直觉得作为一个算法工程师，要经常学习好的开源框架里面的工程代码，这样不仅能够在实现自定义模型时好下手，也能提升自己的工程能力。本文用到的tensorflow框架版本为1.4。目前比较新的版本中的相关代码主体改动有一些，不是很大，所以可以适配着看。 之前的文章中，我已经对seq2seq+attention模型有过讲解，当时以为自己弄懂了。但是最近通过一些比赛，实际去实现这个模型的时候，才发觉自己并没有真正吃透这个模型，因此下面重新对这个带attention机制的seq2seq模型进行回顾，然后再从tensorflow的seq2seq大礼包源码入手，讲解其中的步骤。为了更好描述，这里以最近参加的标题生成的比赛作为例子。（成绩不太好，就不写总结了） 假设当前的输入数据，已经对out-of-vocab词用代替，且对decoder的输入数据增加了标记，对输出数据末尾增加了标记，最后已经padding成等长数据。模型的主要结构为encoder-decoder。规定输入和输出共享一个词典和embedding层。 Encoder首先是encoder。主要作用是读取并学习document text中的信息。常使用RNN或者带门机制的RNN（GRU或者LSTM）。一般会先对输入进行embedding_lookup，获取对应的embedding_input后，将其输入到RNN的中进行编码。一般的encoder会直接输出最后 $t_n $时刻的状态 $h_{t_n} $，作为decoder的初始 $t_0​$ 状态。这里有个改进点，可以提升整个模型的性能： 之所以可以将encoder的$h_{t_n}$作为decoder的初始状态，在于经过RNN的学习与传递后，最后一个时刻的状态h已经整合了当前网络对输入文本的分析和总结，因此直接将其作为decoder的初始状态，相对于随机初始化操作，可以加快网络的训练速度，提升模型的性能。那么，我们可以通过其他方式，更好得获得encoder的学习成果，比如将所有t时刻的状态 $h_{t_i} $进行一个average pooling,max pooling，甚至进行一次self attention，最后得到一个综合的输出，作为decoder的初始状态，这样应该能比baseline方法更好。在实践过程中，也确实是这样，其中average pooling的效果最好，而self attention之所以效果较差的原因可能是模型中后面decoder的时候也使用了attention机制，因此这里使用self attention机制对模型效果提升没有很大的效果，相反，还会导致过拟合。且average pooling的效率最高，因此最后我选用了这种方式，来输出decoder的初始状态。 当前encoder模块有两个比较重要的输出会被后续的decoder使用： 1、RNN中的每个时刻的状态$h_{t_i}​$ 2、RNN中的每个时刻的输出 $o_{t_i} ​$ 其中，状态$h_{t_i}​$会作为decoder的初始状态被使用到，而输出 o$_{t_i}​$则会在attention机制计算时使用到。这就是该模型的核心之处。下面用图表示，会比较清楚。 Decoder decoder的主体也是使用的RNN，原始输入经过embedding_lookup(和encoder共享的)后，输出到RNN中，每个t时刻的状态为 $s_t$ ,其中 $s_0$ 的初始化已经在前面讲过，用encoder的相关信息来初始化。然后就是模型的attention机制的计算和使用。 这里以Bahdanau Attention计算方式为例。这个attention具体的内容就不讲了，有关attention的内容之前文章有描述，这里直接放上公式： e^t_{i}=v^Ttanh(W_oo_i+W_ss_t+b_{attn})\\ a^t = softmax(e^t)其中， $v,W_o,W_s,b_{attn} ​$均为网络权重参数。 $o_i​$ 为参与attention计算的memory，即encoder的output，而 $s_t​$ 为decoder在decoder t时刻的状态。 这里注意，我使用的是Global attention，相对的还有Local attention，由于Local attention计算方面还要考虑上下文windows，为了便于讲解，使用了简单的Global attention机制，且效果也挺好的。 这个公式表明，我每次要计算输出当前时刻的decoder output logits时，需要将当前时刻的decoder 状态整合encoder的output，一起输入到attention公式中计算，最后得到alignment，这个alignment只与当前时刻t的计算有关，一般是不用保存的。当然后续应用coverage机制的时候，需要将这个alignment的历史信息都先保存下来。 得到alignment后，接下来就是应用 $a^t$ 到encoder的output中，进行加权求和，得到最终的attention context。这个是融合了encoder，decoder输入信息的attention学习成果。 o^*_t = \sum_ia^t_io_i这个学习成果需要和decoder的中间状态$s_t​$整合在一起（一般使用concat，也有其他整合方法），然后输入到全连接层中，并最终输出logits，或者输出一个softmax概率，即词库中每个词在当前位置的概率分布。最后计算当前时刻的 $loss_t ​$。 当然，所有时刻的 $loss_t$ 计算完后，需要将loss加起来，并做sequence_mask，将pad位置的loss过滤掉，然后求平均。 Tensorflow seq2seq上述过程，看着挺简单的，但是实现起来，其实还是挺麻烦的，因此tensorflow提供了一个方便的大礼包，包含了seq2seq的各个阶段的计算接口，只要按照规则正确调用，就能搭建出模型。但是这个大礼包，虽然很方便，但是要进行进一步的自定义修改，还是挺难的，需要对里面的代码进行长时间的阅读和理解，且tensorflow的文档可以说是比较少了，所以这里把我这段时间阅读代码的总结写下来，方便大家参考。 大礼包的主要核心部分在decoder侧，encoder侧，还是使用我们熟悉的RNN的接口，一开始我使用的是CudnnGRU，但是由于其不支持dynamic_rnn，无法对pad位置的信息特殊处理，因此实际使用效果虽然速度快，但是效果却没有原始的gru接口好，因此最后还是用了tf.contrib.rnn.GRUCell（貌似tf.contrib这个包会在后续的版本中逐渐舍弃，所以建议还是用另一个接口）。最后说明一下encoder的输出为两部分： 1、encoder_output,shape=[batch_size,time_steps,rnn_dims]，为rnn的输出 2、encoder_state,shape=[batch_size,dims]，如果为双向rnn，需要将两个方向的state先拼接，然后再对所有时刻的state沿time_steps做average pooling。最后可能还需要对encoder_state做一层全连接层，使得它与decoder的RNN的dimension一致。 decoder的编写流程 下面我先按照正常的流程使用大礼包编写decoder的流程。然后按照代码的执行顺序描述代码的运行机制。inference阶段我使用beamSearch来举例。 123456789101112attention_mechanism = tf.contrib.seq2seq.BahdanauAttention( decoder_dim, encoder_output, memory_sequence_length=sum_len, normalize=True)decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=atten_size)initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size)initial_state = initial_state.clone(cell_state=encoder_state)#trainhelper = tf.contrib.seq2seq.TrainingHelper(decoder_inputs_embedding,title_len, time_major=False)decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state,output_layer=projection_layer)outputs, self.train_dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=False)#logitsdecoder_output = outputs.rnn_output 上述是训练过程，下面我们自底向上的顺序来说明这个源码。 源码执行流程 由于下面内容比较繁琐，因此先po出流程图，不喜欢看文字的同志可以看这幅图了解一下大概调用流程。该图只是一个整体的流程，很多细节在后面文字描述。 tensorflow seq2seq大礼包模块简易流程 1、调用栈的最底层就是我们的tf.contrib.seq2seq.BahdanauAttention，它提供了具体的attention计算方法，源码位于tf.contrib.seq2seq.AttentionWrapper中。 可以看到它继承了一个基类，这个基类的主要作用是在_init_方法中对memory做了全连接层的计算（如果定义了dense layer的话），得到attention计算中的key和value。而子类则定义了query_layer和memory_layer,以及最后归一化的方法（一般是softmax） 接下来就是该类的核心方法： 可以看到该方法是一个call()方法，熟悉python的同志肯定知道，一个类实现了这个方法，可以让类变得可以向函数方法一样被直接调用。其中，主要核心就是调用了_bahdanau_score方法来进行attention计算。这个方法具体就不贴出来了，就是按照论文中的计算方法。 备注：这里有一个_probability_fn方法，需要输入历史的alignment，这个就是之前说的，有一些attention方法需要使用历史的alignment数据，因此所有attention方法都默认会调用这个_probability_fn方法，只不过在BahdanauAttention中它是不对历史的alignment做任何操作，直接对score做softmax。 2、往上面一层就是AttentionWrapper，这个类负责调用整个Attention机制的流程。首先看一下它的类定义： 可以看出来，它继承了RNNCell，说明这也是一个具有RNN特性的实现类，且在RNNCell上，封装了Attention的特性。另外，继承了RNNCell的所有子类，需要实现call()方法，使得在构建网络计算图（build())时，能够调用相关逻辑。 分析它的init方法，可以看到它接收decoder中的RNNCell，之前定义的Attention机制，另外关注一下alignment_history，这个属性就是能控制是否保存历史alignment信息的开关。Attention_layer_size属性则定义了Attention操作后是否需要连接一个全连接层到指定的dimension。 该类有两个重要的方法，zero_state和call。前者用于初始状态s的封装和生成，后者主要是用于attention计算的主流程。 下面看一下zero_state方法， 其实该方法最重要的就是返回一个封装过的可用于Attention计算的AttentionState。 下面看一下这个封装类的结构： 可以看到，其实该类就是一个namedtuple的数据结构封装。 cell_state存储的是AttentionWrapper包裹的RnnCell在t-1时刻的状态 attention存储的是t-1时刻输出的context time存储的当前时刻t alignments存储的是t-1时刻输出的alignment alignment_history存储的是所有时刻的历史alignment信息 AttentionWrapperState中还有一个clone方法，在我们的模型图中也有调用的地方： 1initial_state = initial_state.clone(cell_state=encoder_state) 其实就是对我们初始化的AttentionWrapperState对象，将cell_state的属性值对替换为从encoder输出的state（经过average pooling）。 下面是AttentionWrapper类的核心方法：call，该方法定义了attention操作的主流程。 该方法的参数为inputs：即decoder中的当前时刻t的输入，而state则是封装过的AttentionWrapperState。下面对关键代码进行注释： 上述代码的主要操作是将当前时刻input和前一时刻的context拼接后，输入到decoder中的RNN层中做处理。最后输出output，以及下一个时刻的中间状态。 主要核心是方法_compute_attention方法，该方法是attention计算的入口。 得到attention的context和alignment信息后，就是返回需要的信息，其中比较重要的是返回的当前时刻的中间状态为AttentionWrapperState，这个中间状态会被下一时刻t+1的计算使用。 需要注意的是最后返回的时候，有一个flag，_output_attention,这个是控制当前是否返回attention的信息还是rnn的output信息，对于BahdanauAttention style来说，是返回cell_output. 3、再往上一层，找到tf.contrib.seq2seq.BasicDecoder，这个类主要作用是将上述的所有操作流程按照decoder的序列长度依次按顺序执行。 看到它继承了Decoder这个类。它的核心方法为step方法，下面就basicDecoder的step方法具体描述： 它的参数为time：当前时刻，inputs:decoder的输入，state：前一时刻传递而来的状态。下面是具体的代码流程： 首先这里的_cell，是我们之前定义的AttentioWrapper，因为它是继承了RnnCell，因此具有RnnCell的特性。这里相当于是调用做了前两个大模块的操作。返回了当前时刻的output,state. 那么在训练阶段，模型是如何推动上面的计算步骤一步一步到最后的呢？下面要用到另一个有用的大礼包的类：tf.contrib.seq2seq.TrainingHelper 上图中的helper就是要用的帮助训练的类（顾名思义）。这里调用了两个方法，第一个是sample，即根据当前时刻的output，获取当前时刻词分布中概率最大的那个词id。 第二个是next_inputs方法，主要是根据当前处理的时刻t，读取下一个时刻的输入，用于下一时刻的计算，并返回序列处理结束标志。 step的最后一个步骤就是返回处理的结果，这里它又封装了一个特殊的数据类型：BasicDecoderOutput BasicDecoderOutput定义如下： 其实根AttentioWrapperState相似，也是一个封装了的namedtuple，主要存储的是rnn_output,以及最后得到的词的id。这样封装的好处是，rnn_output可以用于计算loss时直接使用，而sample_id则是在inference阶段可以用来输出结果。 4、最高一层是大礼包中最重要的一个部分，上述basicDecoder的step方法，如果没有任何上层接口驱动，也是无法完成。因此tf.contrib.seq2seq.dynamic_decode就是用于完成这项工作的。 与其他接口不同，这个是一个可直接调用的方法，其方法定义如下： 其中decoder就是之前定义的basicDecoder.impute_finished 属性表示模型在梯度传递的时候会忽略最后标记为finished的位置。这个一般设为True，能够保证梯度正确传递。而maximum_iterations为我们自定义的decoding最大长度，可以比设置的title_len大或者小，主要看调参。swap_memory表示在执行while循环是否启用GPU-CPU内存交换。 下面只列出该放里面的核心步骤： 首先这是tensorflow中的循环操作。它的循环条件condition为： 他会接受basicDecoder返回的finished标志，并判断当前是否已经处理结束。 然后是循环的body部分，也只放上核心部分： 即调用basicDecoder的step方法来执行decoding，这样就与之前讲的联系上了。 Inference阶段其实train阶段和inference的不同点很简单，在于inference阶段没有decoder的input，因此每个时刻的state计算都需要输入前一个时刻的计算结果。这里以BeamSearch举例。 12345678910111213141516171819202122tiled_encoder_output = tf.contrib.seq2seq.tile_batch(self.encoder_output,multiplier=self.cfg.beam_width))tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(encoder_state,multiplier=self.cfg.beam_width)tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.sum_len, multiplier=self.beam_width)attention_mechanism = tf.contrib.seq2seq.BahdanauAttention( self.cfg.lstm_units, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=self.cfg.lstm_units * 2)initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.cfg.beam_width)initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)decoder = tf.contrib.seq2seq.BeamSearchDecoder( cell=decoder_cell, embedding=self.embedding_init, start_tokens=tf.fill([self.batch_size], tf.constant(2)), end_token=tf.constant(3), initial_state=initial_state, beam_width=self.beam_width, output_layer=self.projection_layer )outputs,_,_ = outputs, _, _ = tf.contrib.seq2seq.dynamic_decode( decoder, output_time_major=false, maximum_iterations=self.decoder_max_iter, scope=decoder_scope)self.prediction = outputs.predicted_ids 整个inference流程与train类似，唯一不同的地方在于beamSearch算法本身，需要将所有的输入和中间状态复制beam_size份，用于beam的搜索。而主要的区分点在于使用的decoder不同，这里我就着重讲一下tf.contrib.seq2seq.BeamSearchDecoder。 主要还是贴出其step方法中的核心过程： beamSearch的方法中，会存在很多merge_beam,split_beam等改变tensor的shape操作，方便一些操作的计算，这里就不仔细讲了，只要记住merge一般和split应是成对出现。 首先当然是调用AttentionWrapper，来计算输出当前时刻的cell_output,以及下一个时刻的state。 然后就是另一个核心方法调用_beam_search_step： 这个方法主要是执行Beam搜索的流程，核心的模块流程如下： 先计算当前时刻为止的所有候选序列计算概率值之和。 然后计算每个beam的分数： 然后是根据指定的beam_size,使用top_k运算，得到最合适的beam_size候选。 最后就是返回一些封装的结果，就不具体列出了。 总结其实还有很多源码并没放到上面分析，鉴于篇幅问题，写的太多，可能看得也越困难。总体来说，通过这次的比赛实践，还是对seq2seq模型有一定的深入理解，无论是理论上还是工程实现上，tensorflow的大礼包的实现确实挺漂亮，一环扣一环，希望能吃透它的工程思维，融入到自己的实践中。]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[达观文本分类大赛17名思路总结]]></title>
    <url>%2F2019%2F03%2F29%2F%E8%BE%BE%E8%A7%82%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E5%A4%A7%E8%B5%9B17%E5%90%8D%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[最近solo了一个比赛，达观的文本分类比赛，大概搞了20来天，最后b榜成绩17名，比a榜下降了一名。虽然最后没有进前10，不过这次比赛相比之前参加kaggle的文本分类比赛，可以说是有了长足的进步。这里把这次比赛的思路总结一下，后续有时间的话会把代码开源，不过对于已成家的上班族来说，还是不要抱太大期望。。。 先简述一下赛题吧，任务很简单，是一个文本多分类比赛，这个比赛主要有两个难点： 1、文本长度分布不均，短的很短，长的很长，最长的貌似都上万个词了，最短的才几十个词。虽然训练数据只有十万多，但是对于缺少GPU、内存的同学来说，做这个比赛还是比较痛苦的，而且用深度学习时，对文本的截断和padding也是一个影响性能的因素。 2、文本是脱敏的，也就说不知道提供的数据里面的具体文本内容。这个也是现在国内文本比赛的趋势，脱敏的数据意味着很慢做一些传统NLP的数据预处理，比如停止词过滤、具体词的分析，同时也没办法使用已有的预训练好的词向量。 主办方提供了词分割的文本和字分割的文本，由于时间和硬件成本，我只使用了词分割的文本，其实如果将字文本也用上的话，进行模型融合，应该还可以让最后成绩提升，后面有时间再试了。 预处理首先是先看看所有分类标签的占比，有没有样本分类不均衡的现象。不知道是不是主办方处理过了，这次并没有严重的不均衡，一共19个类别，基本上每个类别样本量差距不大。另外，要想办法过滤一下停止词和标点符号。针对脱敏数据，我统计了每个词的idf值，将那些所有文章都出现的词筛选出来，大概筛出来10个左右的词。这些词即使不是标点符号，肯定也是一些对文章主题无用的词，可以过滤点。另外，经过分析，发现训练集样本中有文本重复的记录，应该是数据的噪声，有些重复样本的标签居然是不同的！这种噪声数据，我就全部过滤掉了，而标签相同的重复样本我就保留一条，其余的也删除。 （PS：删除过滤词之后，发现有的样本词全部删光了，这种文本估计也是噪声，重要的是测试集也有这种样本，总不能删除吧，只能使用-1这个UNK标记来填充，索性这种样本数量也不多，对总体的模型影响不是太大。） 词向量主要是自己从头开始训练了word2vec和glove两种词向量，维数是300维，最后两个词向量都使用了，做了concat拼接，当然这个维度直接使用会过拟合的，所以后面接了dropout层，使用0.5的概率。 模型本次比赛主要还是用的深度模型，最后会和xgboost结合，做stacking。 我用的方法其实也没有很复杂的地方，下面简单列出，同时会列出一些trick： 1、深度模型的话，我使用了双向LSTM+self-attention,这里要提一点，我的LSTM并没有使用传统的tf的rnncell，因为实在是太慢了！由于我使用的硬件平台关系，我的job最长时间不能超过12小时，否则我的当前进程会被强制下线。所以，一开始被这个训练成本搞得差点弃赛。 后来各种搜索高性能的rnn实现，发现tensorflow里面早有了cudnn的高性能实现，于是试了tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnnops的CudnnLSTM，效率瞬间飞起！本来我的一个epoch要10000多秒，最后降到了1500秒。但是要注意的是，这个CudnnLSTM不支持对padding的masking，即没有实现dynamic_rnn,所以对于一些padding的序列，它的梯度是会把padding的位置传递进去的。可能这也是我深度模型没有上太高分的一个原因吧。后面使用attention可以说对模型的提升程度也很高，其他的降维方法如max pooling，average pooling等都试过，但是还是attention效果最好，这个方法能够帮助模型对一些重要词赋予较高的权重，这个权重是通过模型动态学习到的。 到这里，我的分数提升路径大概是：bicudnnLSTM+maxpooling:0.777,bicudnnLSTM+attention:0.779，总算是赶上了baseline的水平了。 备注一下：基于性能和模型训练成本的平衡，最后我文本序列长度定为了2500。分析过文本长度的分布，大部分文本长度都在5000以下，但是1000-5000的文本数量也不少。 2、使用了伪标签。因为本次比赛的训练集为10万+，但是对于长本来说，这个量级还是比较少的，训练集不够，而且训练集和测试集的分布也可能会有一定的差异。因此借鉴了kaggle上的这个技巧，将当前预测最好效果的测试集+标签添加到训练集中，作为训练集进行训练，通过这种方式，我的线上成绩达到了0.78+。具体的使用方法就引用kaggle上的一个图例，将图中的评估标准换成比赛的F1_score就可以了。 3、使用了传统的NLP特征。这一步是我深度模型提升最大的。主要是对tfidf向量化后的term-doc矩阵，训练了lsi和lda两种特征，最后将两个特征的稀疏矩阵拼接起来放入我的深度模型中，大概如图所示： 使用这个模型，使用10cv+伪标签训练，最后模型的线上分数能达到0.79+ 4、我训练前期使用的是adam的优化方法，后期使用的momentum gradientDescent手调，由于时间因素，并没有调到很理想的性能，所以我尝试将10cv中，每个cv中所有训练数据（不包括测试集数据）在softmax层前得到的特征保存下来，并输入到xgboost进行训练： 这样，我一个参数组合的深度模型，最后能得到10个xgboost的训练结果，最后我调了两组参数，也就是得到了20个xgboost的训练结果，用于后面的stacking。 5、stacking是我之前比赛没有尝试过的，这次是第一次尝试，使用的是oof方法。我的基模型用的是步骤四得到的20个xgboost的结果，注意这里结果是19个分类的概率，即stacking后，应该是得到一个390个特征的数据。oof方法这个在网上很多都有，就不详细说明了，简单说就是用基模型验证集的结果拼成训练集，然后进行二次训练，而测试集的特征则是所有基模型的测试结果的平均。通过这种方法，我的线上分数升到了0.794+。（注意，stacking我并没有使用伪标签数据，一旦用了很容易过拟合，这个要注意。） 6、后续就是各种参数的调优和lgb、svm的尝试。基本上没有什么多说的。最后最好的A榜分数大概在0.7943，但是悲催的是我没有选择用于b榜评测的提交结果，幸好最近两次提交中有一次结果还可以，大概在0.7939左右。不过跟我的最好成绩也差不多，跟前面的差距也挺大的。。。 总得来说，这次比赛，将之前我参加比赛的未实现的想法都实现了，但是还是有一些遗憾的，比如没有使用字的文本，另外后来有想到将每个文本拆开来，单独训练出不同模型，最后再融合，这样可以解决文本过长的问题，不过时间不够就没有尝试。还有最后想做error analysis，发现对脱敏的数据基本无从下手，只是用了textrank提了关键词，发现一些互相分错混淆的类别，确实在关键词集上式相似的，估计是主题很接近的类别，这种类别要区分开来估计要更复杂的网络。希望在观看前10答辩时，能有一些启发。 最后，吐槽一下，一个人比赛实在是太累。。。由于工作关系，也不好与他人组队。后面选择比赛的时候，还是只能选一些数据相对较小的比，不然真的吃不消。不过做了那么多判别分类式的比赛，希望后面做一些生成式的比赛，比如机器阅读、自动摘要、QA等。最近有AIC和字节跳动的比赛，可能会选一个参加一下吧。欢迎有兴趣的同学私聊。]]></content>
      <categories>
        <category>比赛总结</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>attention</tag>
        <tag>text classificaton</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（基础算法复习）之最长回文子串的动态规划解法]]></title>
    <url>%2F2019%2F03%2F29%2F%EF%BC%88%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%EF%BC%89%E4%B9%8B%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E8%A7%A3%E6%B3%95%2F</url>
    <content type="text"><![CDATA[再开一个新坑，以后会不定期总结一下对于一些基本算法题的解法思路和实现。作为一个AI领域的从业者，不仅对于AI本身的技术要有掌握，对于计算机科学中的基本算法思想也要扎实的基础，这样才能做一个合格的算法工程，本人在这方面的能力还不够成熟，因此开这个坑，希望通过这种方式，真正去理解算法的本质。 本文要总结的题目是找最长回文子串，对应leetcode中的题目如下： Longest Palindromic Substring Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: Input: “babad” Output: “bab” Note: “aba” is also a valid answer. Example 2: Input: “cbbd” Output: “bb” 本文主要集中在如何一步一步使用动态规划的方法来解决这个问题并进行优化的过程，有其他更好的解决该问题的方法不在本文中讨论。 最简单思想首先，这个题目最简单的思想就是暴力遍历所有可能结果，假设当前字符串序列为 $s_1…s_n$ 具体做法是遍历所有可能长度的子串，判断其是不是回文子串，然后记录其长度，最后返回那个长度最长的子串即可。 暴力搜索的问题暴力搜索的时间效率肯定是最低的，那么低在哪里呢？我们以$s_1…s_n$为例，列举暴力搜索的前几个步骤，很明显就能看出来问题： 子串长度为1时，遍历所有单个字符 $s_1,s_2,…,s_n $，此时所有单个字符都是回文。 子串长度为2时，遍历所有二元字符子$s_1s_2,s_2s_3,s_3s_4,…,s_{n-1}s_n $，然后判断每个元素是否为回文，判断依据是 $s_i?=s_{i+1}$ 子串长度为3时，遍历所有三元字符串 $s_1s_2s_3,s_2s_3s_4,…,s_{i-1}s_is_{i+1},s_{n-2}s_{n-1}s_n$ ，这里判断每个元素是否为回文。 子串长度为4时，遍历所有四元字符串 $s_1s_2s_3s_4,s_2s_3s_4s_5,…,s_{i-2}s_{i-1}s_is_{i+1},s_{n-3}s_{n-2}s_{n-1}s_n $，然后判断每个元素是否为回文。 重复上述步骤，直到遍历到 $s_1…s_n​$ 这个长度为n的字符串为止。 进一步分析，其实步骤3时，其实会用到1的结果，因为每个元素中间项 $s_i $通过1已经知道是回文了，那么只要判断 $s_{i-1}==s_{i+1}$ 是否成立就行了。然而步骤3是重复处理了。 同样的，步骤4同样用到了步骤2的结果，对于 $s_{i-2}s_{i-1}s_is_{i+1}$ ，如果知道 $s_{i-1}s_i $是回文，那么只要分析 $s_{i-2}?=s_{i+1} $就可以了。因此步骤4也重复处理。 引入动态规划通过上面的分析可以知道，暴力搜索时，并不是每一个元素都需要去专门遍历并判断是否为回文，有些元素可以通过历史的结果来帮助判断一个元素是否为回文。同时我们也可以总结一些规律： 假设 $s_i,…,s_j $是我们要求解的最长的回文子串(其中j&gt;=i)，那么它必然符合两个条件： a. $s_i==s_j $ b. $s_{i+1},…,s_{j-1} （j&gt;=i+2）$必然也是一个回文子串，否则上述假设不成立。 基于上述规律，我们可以将这个问题进行拆分，分成最基本的优化目标问题单元：即只要到一个回文子串，然后以该子串为中心，左右两边的字符若相等，则找到一个更长的新的回文子串。即从一元字符开始，以该字符为中心向两边扩展的方式，逐渐找到更长的回文子串，有点像穿衣服一样，一层一层往上套。这个就是动态规划的思想，当然我只是用口语化的方式描述了一下，理论化的动态规划肯定更加严谨公式化，但是我就不在这里讨论了，有兴趣的可以自行查阅。 那么如何实现上述的思想呢？前面说了，我要知道$s_i,…,s_j$是否为回文，那么主要是需要知道$s_{i+1},…,s_{j-1}$是否为回文。这里可以将其视作一个填表问题。我们设计一个bool型的n*n的二维矩阵A，如果$s_i,…,s_j$是一个回文串，那么 $A_{ij} $位置就填上true，否则为false。填表的顺序则是从长度为1的字符串开始，逐级向长度更长的字符串填，间而言之就是一种周期性的斜向上方向，具体如图： 其实上图也说明了一个事情，就是我们并不需要把所有表格都填上，而是只需要填满上三角的表格就能得到最终结果。因此，在代码实现的时候，我们可以初始化一个上三角形状的二维数组，这样可以节省一半的空间。（代码实现的时候，我还是初始化了整矩阵，为了与后续优化区分）。 我们要做的，就是当遇到回文串时，不仅在上述表格相应位置填上值，而且需要记录当前子串的起始位置和长度，用于后续的返回结果。 代码实现1下面给出我写的初步动态规划解法，只列出关键逻辑，完整代码后续会放到github上，到时会贴出来。代码不是很优美，求大佬轻喷。 12int s_size = s.size();vector&lt;vector&lt;bool&gt; &gt; result(s_size, vector&lt;bool&gt; (s_size,false)); 初始化一个n*n的bool型矩阵，n为字符串长度。 12345678910111213for(int i=0;i&lt;s_size;i++)&#123; result[i][i] = true; if(i &lt; s_size-1) &#123; if (s[i]==s[i+1]) &#123; result[i][i+1] = true; cur_max = 2; start = i; &#125; &#125;&#125; 这里是先填充对角线，以及 $(i,i+1)$ 上斜线的位置，对角线对应一元字符，这个很好理解，都是回文，而上斜线的位置也很好处理，只要判断$s_i?=s_{i+1} $就可以了。同时要记录当前最长回文的起始位置start以及长度cur_max。 1234567891011121314151617for (int distance = 2;distance &lt; s_size;distance++) &#123; // cout&lt;&lt;distance; for (int i = 0;i+distance&lt;s_size;i++) &#123; result[i][i+distance] = (result[i+1][i+distance-1]&amp;&amp;(s[i]==s[i+distance])); if(result[i][i+distance]) &#123; if(distance+1&gt;cur_max) &#123; cur_max = distance+1; start = i; end = i+distance; &#125; &#125; &#125; &#125; 这里最外层的循环表示字符串的长度遍历，里层遍历则是对字符串起始位置的遍历。循环内部的逻辑在上面章节已经介绍过，即根据$s_{i+1},…,s_{j-1}$是否为回文以及$s_i?=s_j$来判断$s_i,…,s_j$是否为回文。同时记录回文子串的起始位置和长度。 进一步优化上述代码在leetcode运行后的时间和空间成本如下： 可以看到无论是时间成本还是空间成本都是很高的，优化的空间很大呀！ 回头看我们的上述解法过程，我们将一个矩阵中一个斜线上的所有位置填表称为一个epoch操作。每轮epoch其实都隐含得使用了前一个epoch的信息，更加具体的说是前一个epoch的某一个位置上的值。我们是否需要一整个二维矩阵表格来填某个epoch的信息呢？答案是不需要！具体原因看下图图示： 具体来说，每次填表格，其实只需要其右下角的表格的值就可以了，而初始的对角线和上斜线位置的判断也是很简单就能实现也不用记录，因此实际上我们不用一个二维矩阵，只需要几个临时变量记录右下角的表格值就可以了。 另外，我们需要变更填表的顺序，之前我们的顺序都是每个epoch斜上方向45度填。这里我们改换成如下的填表顺序： 其中箭头上标的数字代表循环的epoch轮数。即每次我向左斜向上方填表，填到边界时，跳到下一轮的其实位置，接着填。 优化代码实现代码的话其实注意几个点就行，一个是每一轮遍历的顺序是往左斜向上的，另一个就是每一轮要记录当前轮起始的位置，因为下一轮的起始位置是与前一轮的起始位置和当前轮数相关的，如图： 主要代码逻辑如下： 123bool pre_flag = false;int epoch_start_i = 0;int epoch_start_j = 0; 这里直接初始化一个临时变量存放右下角的值，然后初始化两个位置变量来存每一个epoch的起始位置。 对于对角线和上斜线的处理就不贴出来了，这里贴一下其他位置的填法： 12345678910111213141516171819202122232425262728293031//根据右下角的表格填当前表格pre_flag = (pre_flag &amp;&amp; s[i]==s[j]);//如果是回文，且长度最长，则记录起始位置if(pre_flag &amp;&amp; j-i+1&gt;cur_max)&#123; cur_max = j-i+1; start = i;&#125;//到边界位置，需要换到下一轮的起始位置if(i-1&lt;0 || j+1 &gt;=s_size)&#123; //本轮起始位置为对角线上 if (epoch_start_i == epoch_start_j) &#123; i =epoch_start_i; j = epoch_start_j+1; &#125;else &#123; //本轮起始位置在上斜线上 i =epoch_start_i+1; j = epoch_start_j; &#125; continue; &#125;else&#123; //向左斜上方向处理 i--; j++; continue;&#125; 最后，上述代码在leetcode上运行的花费如下： 可以看到，无论是时间复杂度，还是空间复杂度，都有了很大的优化。]]></content>
      <categories>
        <category>基础算法</category>
      </categories>
      <tags>
        <tag>dynamic programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow模型线上部署]]></title>
    <url>%2F2019%2F03%2F24%2Ftensorflow%E6%A8%A1%E5%9E%8B%E7%BA%BF%E4%B8%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[这次写一个比较短的工程部署总结，说是tensorflow线上部署，并没有使用目前推荐的tensorflow serving。一则是该功能只是整个项目中的一个很小的功能点，如果单独为该功能部署tensorflow serving，成本和时间上会比较超标；二则由于公司内部网络环境限制，无法完整顺利获取tensorflow serving需要的依赖包（在没有网络环境下去装tensorflow serving环境的同学都懂的）；三则，项目并没有对模型在线学习的需求，只需要模型离线训练后，部署到线上即可。基于上述三点条件，选择了使用tensorflow python开发并训练模型，最后将模型进行序列化。然后在java端（项目主开发语言）调用模型进行在线预测。 前置条件tensorflow 1.3 所需的tensorflow java jar包：libtensorflow-1.4.1.jar 所需的tensorflow jni 库文件：libtensorflow_jni.so libtensorflow_framework.so 模型序列化一开始，由于未考虑模型如何移植到java平台的原因，模型训练和保存的时候，还是按照传统的方式，即通过tf.train.Saver()方法得到saver对象，然后调用saver.save方法保存模型，得到模型的checkpoint,meta,data,index四个文件。 上述文件中保存了模型的中间参数，模型当前训练的状态等信息，在某种意义上是动态的，只能通过tensorflow本身的python接口来调用该保存好的模型，并不能跨平台直接使用。其实模型的本质还是一堆权重数据和具体的权重计算流程，因此需要某种机制能固定住模型的权重数据和计算流程，即freeze模型。 1234567891011saver_predict = tf.train.import_meta_graph(model_config.ckpt_path + model_name)with tf.Session() as sess: if os.path.exists(model_config.ckpt_path): print("Restoring Variables from Checkpoint") saver_predict.restore(sess, tf.train.latest_checkpoint(model_config.ckpt_path)) else: print("Can't find the checkpoint.going to stop") exit() output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node] frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names) tf.train.write_graph(frozen_graph_def, model_config.ckpt_path, target_model_path, as_text=False) 上述代码的作用是对使用import_meta_graph读取训练好的模型,然后获取当前计算图中的所有节点，并将这些节点中的所有权重参数转化为常量，最后将这些常量保存到一个pb文件中，pb文件即protobuf，是 Google 推出的一种二进制数据交换格式。能够用于跨平台间的数据交换。上述代码实际使用时，会有问题，在java侧使用java的api调用模型时，会出现如下错误： 1Invalid argument: Input 0 of node XXXXXXXXXXX/BatchNorm/cond/AssignMovingAvg_1/Switch was passed float from XXXXXXXXXXX/BatchNorm/moving_variance:0 incompatible with expected float_ref. 经过一番google，找到了这个github的issue，貌似并没有得到很好的解决，推测问题原因是在freeze模型的权重参数时，对一些tensor的data type处理有问题。问题链接如下，有兴趣的同学可以看看，我最后换了另外一种方式来做。 []: https://github.com/davidsandberg/facenet/issues/161 最终，我使用的是另外一种方法，即在模型训练完时，直接对模型的权重参数序列化，保存为pb文件。代码如下： 1234# 将模型序列化保存`builder = tf.saved_model.builder.SavedModelBuilder(model_config.pb_path.format(epoch))builder.add_meta_graph_and_variables(sess, ["XXX"])builder.save() 调用SavedModelBuilder得到builder对象，然后将session中的所有图结构和权重参数存入到builder中，最后保存为pb文件。 Java调用模型在线预测这里主要使用的是tensorflow的Java版本api，说是Java版本，其实功能很局限，并没有模型训练方面的功能，所幸它有读取模型和数据，然后在线预测的功能，因此可以适用于当前场景。在部署时，有几点需要注意一下： 1.最基础的事情，当然是记得将tensorflow jar包加入到build-path中。 2.在linux上进行部署时，需要将两个so文件部署到项目工程的java build path中，因为我们的工程中的path包含/usr/lib/，因此可以将两个so文件放到这个路径下。两个so文件主要是用于tensorflow 上层的api与底层操作系统native library进行通信的接口。 下面主要介绍一下如何使用java来调用模型预测。 首先列出用到 两个主要的操作对象： 12SavedModelBundle tensorflowModelBundleSession tensorflowSession SavedModelBundle为java侧与pb文件接口的对象，能够读取pb文件。而Session对应的是tensorflow中的会话对象，java中，tensorflow的预测操作也是需要在一个会话中进行的。 12tensorflowModelBundle = SavedModelBundle.load(tensorflowModelPath, "XXXX");tensorflowSession = tensorflowModelBundle.session(); 然后就是构造输入模型的数据了。同python中的情况类似，java侧的模型接收的数据类型必须为tensor类型，因此需要将数据转化为tensor。因此要用到Tensor对象的create方法来生成Tensor，假设当前我们处理好后的数值型输入数据为一个二维数组testvec： 1Tensor input = Tensor.create(testvec); 当然如果有其他输入的话，也要都转化为Tensor，简单说就是原来模型中feed_dict中的所有输入都要转化为Tensor对象。 然后就是调用session，输入需要的数据，然后调用具体的计算节点输出结果： 1Tensor output = tensorflowSession.runner().feed("input",input).feed(XX,XX).fetch("computation node_name").run().get(0); 这行代码有几个注意点： 1、feed方法返回的仍然是Runner对象，这个机制使得我们可以链式调用feed方法，将所有需要喂入模型的数据装载。 2、Runner对象的fetch方法是定位到具体的计算图中的计算节点（tensor），这个与python中调用模型预测的方法类似，需要在构造计算图的时候，对模型输出样本预测概率（或者logits）的tensor指定名称。 3、最后的get()方法则是获取返回的结果，这里我输入了参数0，是因为run()方法默认返回的是一个List，因为有可能有的需求会调用多个计算节点，因此会返回多个tensor，但是此时我只需要得到一个tensor结果，因此获取List中的第一个元素。 上述方法返回的是一个Tensor对象，为了输出结果，需要将它转化为原始的二维数据格式： 1float[][] resultValues = (float[][]) out.copyTo(new float[1][1]); 调用Tensor的copyTo方法，能够将Tensor转化为指定数据格式的数组。之所以是二维数组，是因为我们的输入数据是二维数组，虽然一次一般是预测一个样本，但为了开发的普适性，统一处理为二维数组，数组存储的就是该样本的预测概率。 最后有一点需要注意一下，在使用完模型后，需要将所有创建的Tensor关闭，销毁资源，当然这个是开发的一个好习惯，能够避免资源的泄露和低效利用。 12out.close();input.close();]]></content>
      <categories>
        <category>工程经验</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记系列（一）SGM for multi-label classification]]></title>
    <url>%2F2019%2F03%2F24%2F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89SGM-for-multi-label-classification%2F</url>
    <content type="text"><![CDATA[阅读论文，不能读完就过去了，要思考和记录论文的创新点和有用的思想。这个系列就是以尽可能的简单，尽可能少的文字去将一些核心的东西提取出来，方便自己以后查阅。 论文题目：SGM:Sequence generation model for Multi-label classification 论文target：文本分类，多标签分类，即一个文本样本分类标签会有多个。 论文intuition：多标签分类问题中，不同标签之间往往存在着一定的相关关系，使得每个标签并不是独立的。而传统的多标签分类中，通常是在最后一层对每个分类标签使用sigmoid_crossentropy_loss来计算损失，然后将损失求和，同时计算每个分类标签的概率，忽略了标签之间的关联性。比如kaggle上的toxic-classification比赛中，不同类别toxic标签之间的关联性是很大的，比如性别歧视和种族歧视等。 论文主要关注点：1、使用seq2seq的方式来解决多标签分类问题。该网络架构本身就很新颖。通过这种方式，能够在一定程度上建模标签之间的关联信息。结构如下： 其实，网络结构主体与经典的带attention的seq2seq是非常相似的。encoder并没有变化，主要改进点在decoder上。由于并没有target文本，我们需要建模的是不同标签之间的关系，因此将所有标签类别作为一个序列，假设有n中标签，那么decoder就有n个时刻。其中，每个时刻t的解码器（假设是一个rnn），一共接受三个不同来源的信息：前一个时刻t-1的rnn hidden state；encoder的attention上下文信息（attention的输入为encoder的隐层信息和decoder当前时刻t的隐层信息）；分类标签的global embedding信息。 备注： 论文在此处有一个不太对应的地方，在上图中，画出的是每个时刻t，decoder的rnn接收的是当前时刻的 $c_t$ ，但是按照attention本身的使用方法，以及论文中的公式： s_t = LSTM(s_{t-1},[g(y_{t-1});c_{t-1}])应当是先将前一时刻的attention信息和前一时刻的标签global embedding先拼接，然后在输入到rnn中。这与图上的画法不符，只能认为是作者的图画错了。另外，在作者给出的源码中，并没有将c信息与global embedding拼接操作，而是直接将global embedding输入到了rnn中。这个在github上有人提出了issue，但至今没有回应，不知道是什么原因。 2、masked softmax以及sorted label。这个是本文另一个重要的点。decoder中，每个时刻计算得到$s_t$后，如何得到标签呢？需要进行两步操作，第一步原图中并没有详细画出，我就补了一条线，即会在后面接一个类似于全连接层的计算，其接收的输入为当前decoder的$s_t$以及当前时刻计算得到的attention context $c_t$ : o_t=W_of(W_ss_t+W_cc_t)其中 $W_o,W_s,W_c$均为需要学习的权重参数矩阵。而f表示一个non-linear函数。 第二步为masked softmax，即上图中的MS。 y_t=softmax(o_t+I_t)使用mask的方式就是对上一步的输出 $o_t$添加一个mask vector $I_t$ ，这个向量的值可是有一定说法的，如果当前时刻输出的标签结果在前面t-1时刻中有出现过，则赋予 $I_t$ 一个极小值，否则则赋予零向量。至于这个极小值，官方源码中是赋予了-9999999999。 为什么要使用mask vector？论文说明是为了防止重复预测相同的标签，有时候预测错了标签会因此导致错误一直传递下去，而模型也不希望预测出重复的标签。 至于这样mask有用吗？虽然论文中做了相关的w/o实验，但是由于实验数据本身标签的量级不大，所以个人感觉并不能看出这个做法的有效性。这个还有待后续的验证。 另外还有一个注意点就是sorted label，即训练时，将每个样本的标签按照频率来排序，即出现较多的标签排在序列的较早时刻。这么做的目的在于，我们训练的loss还是使用交叉熵loss，每个时刻上的标签y都需要给定，因此其顺序也要给定。将频率较高的标签放在开头来训练，能够让模型提早学习对数据整体而言最有用的信息，如果一个标签出现次数很少，模型在一开始花了大力气学习它的性价比就不高，容易拖模型的后腿。 3、global embedding。这个可以说是该论文中第二重要的一个点。它的最大作用就是能让每个时刻都能编码记录前面t-1时刻的标签预测进程。传统的做法是直接使用上一时刻预测出来的结果 $y_{t-1}$但是这个结果是通过取所有标签概率分布中最大的那个结果，不一定就是正确的，如果错误，那么就会使这个错误传递。因此论文借鉴了LSTM中的关于门机制的思想，通过门的机制，将前面t-1时刻的所有预测出来的y信息都编码整合。具体做法： a. 为每个标签初始化一个embedding$e_i$ b. 根据decoder计算得到的标签的概率分布，以概率为权重，计算所有标签的加权和$\bar{e}$ ： \bar{e}=\sum_{i}^{l}{P(y_i)e_i}c.构建门H: H=\sigma(W_1e+W_2\bar{e})可以看到这个门是由embedding和加权和embedding共同控制的。 再次吐槽一下，这里论文并没有sigmoid函数的计算，个人认为应当是漏掉了，因为门机制本身就是控制0-1之间的通路的作用，且源码中，是有这一步的操作的。 d.使用门，计算得到最后的global embedding： g(y_{t-1})=(1-H)\odot e+H\odot\bar{e}注意，这里是向量element-wise乘。 通过上述方式，根据加权和$\bar{e}$ 以及门的控制，能够学习所有标签label的综合信息，防止模型在错误的道路上”一往无前“。 后记虽然， 这篇论文写得有一些问题，但是其中有一些思想是可以借鉴的。 1、使用seq2seq来建模多标签之间的相关关系。 2、使用门机制的思想，来综合考虑所有标签的信息，防止对某个标签预测错误的短视。]]></content>
      <categories>
        <category>论文总结</category>
      </categories>
      <tags>
        <tag>multi-label text classification</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN循环神经网络—梯度爆炸和消失的简单解析]]></title>
    <url>%2F2019%2F03%2F23%2FRNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%B6%88%E5%A4%B1%E7%9A%84%E7%AE%80%E5%8D%95%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[通过cs224n关于循环神经网络一章的内容，以及自己对其他相关论文和博客的研读后，发现对循环神经网络中BPTT（即backpropagation through time）中，关于产生梯度爆炸和梯度消失的数学推导不是太理解，因此自己翻阅了线性代数中关于雅克比矩阵的相关内容，经过一番推导，终于搞懂其中的原委。故总结一下，若有错误，请各位大牛指正。 雅克比矩阵先百科一下什么是雅克比矩阵。引用维基百科上的定义，the matrix) of all first-order partial derivatives of a vector-valued function。首先，他是一个矩阵，其次矩阵的元素是一个一阶函数的偏导数，最后这个一阶函数的偏导数的对象是一个向量函数。 举个例子，大家应该就能明白这个定义意思： 假设有如下函数排列： 可以将$y_1…y_n$组成一个向量Y，其shape为$n_1$,同理$x_1…x_n$也组成一个向量X，其shape也为,可以$n_1$看到该平方函数就是一个对向量X的一个向量函数，而其雅克比矩阵可以写成如下形式：（公式比较难看，望包涵） 可以看到除了对角线上的所有矩阵元素值都是0，而对角线上的矩阵元素值就是对应y与x的一阶偏导数值。 RNN中的梯度消失和梯度爆炸问题RNN的BPTT，很多博客包括中都详细推过了，这里我不做重复说明，我只聚焦到其中的一点，即梯度消失和爆炸的问题探讨。 首先，我使用一下cs224n中，对该问题的所有notation。 该公式表示了RNN的隐藏层的计算过程，其中，h表示隐藏层的输出，W（hh）表示隐藏层到隐藏层的权重矩阵。（是个方阵）在梯度反向传播过程中，需要计算损失函数对W权重矩阵的梯度，可以得到如下公式： t表示时刻，E表示损失函数，该公式表示将所有时刻t的权重偏导数求和，得到这个序列的最终权重偏导，继续使用链式推导，可以得到： 其中，损失函数E对y的偏导数、y对ht的偏导数，以及h对W的偏导数都能很容易的求得，除了中间的ht对hk的偏导数。我们下面就重点关注这一项的求解。该式子最终可写成： 可得知，我们最终需要求的是某时刻的隐藏层值h对上一时刻隐藏层值h的偏导数在所有时刻上的和。 将这个式子再用链式法则拆开一下，令 得到： 其实，从上面的式子可以得知，等式右边的两个乘子项分别是两个不同的雅克比矩阵，因为$h_{t-1}$本身是一个向量函数，而$z_t$是对$h_{t-1}$的向量函数，因此分别将两个乘子式化为雅克比矩阵： 这个矩阵怎么求？回想一下我在上一章介绍雅克比矩阵时举的例子，其实可以类比到这个矩阵，其中$h_t$是一个$n_1$维的向量，而$z_t$也是一个$n_1$维的向量，写成函数排列可得： 简单的说，就是该雅克比矩阵中，某一行或某一列上，只有一个偏导数是非0的元素，即该矩阵除了对角线元素，其他元素都为0，因此可写成： 该矩阵是一个对角矩阵，用diag来表示，其中对角相当于一个向量，每个元素是h对z的偏导数，h与z的关系映射函数是我们网络中的激活函数，这里用 $f’(z_t)$表示h对z的偏导，则上述可写成：$diag(f’(z_t))$ 现在关注一下另一个雅克比矩阵： 同样写成函数排列形式： 看到这个函数排列式，是不是对上面的雅克比矩阵求解比较清晰了呢？是的，该雅克比矩阵的每个元素都是对应W权重矩阵的对应元素，即 最终，可得到： 可以得知，当序列长度越长，对一个序列进行BPTT，每次时刻多有对 的连乘操作，即相当于W权重矩阵的连乘，刚开始的几层可能影响不大，因为连乘较少，但是当越往前传播，W连乘的级数是急剧上升的，若W矩阵初始化不当，小于1或大于1，都将会使得梯度计算朝着接近于0或者无限大的趋势发展。而我们的激活函数sigmoid或tanh，在梯度很大或很小时的曲线都是很平滑的，很容易导致越往后训练，梯度几乎不变。因此产生了梯度消失和梯度爆炸问题。 reference： 1、cs224n公开课 2、Jacobian matrix and determinant]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
</search>
